
\section{Simulations}\label{sec-sim}


To assess the finite sample performance of our methods, we conduct a number of simulations. In Sections \ref{subsec-sim-1} and \ref{subsec-sim-2}, we investigate the performance of our multiscale test and compare it to the SiZer methods for time series developed in \cite{Rondonotti2004} and \cite{Rondonotti2007}. In Section \ref{subsec-sim-3}, we analyse the finite sample properties of our long-run variance estimator from Section \ref{subsec-error-var-ar} and compare it to the estimator of \cite{Hall2003}. 


\subsection{Size and power properties of the multiscale test}\label{subsec-sim-1} 


We consider a simulation design which mimics the situation in the application of Section \ref{sec-data}. We generate data from the model $Y_{t,T} = m(t/T) + \varepsilon_t$ for different time series lengths $T$. The error process $\{ \varepsilon_t\}$ is assumed to have the AR($1$) structure $\varepsilon_t = a_1 \varepsilon_{t-1} + \eta_t$, where $a_1 \in \{-0.5,-0.25,0.25,0.5\}$ and $\eta_t$ are i.i.d.\ standard normal with zero mean and $\ex[\eta_t^2] = 1$. In addition, we consider the AR($2$) specification $\varepsilon_t = a_1 \varepsilon_{t-1} + a_2 \varepsilon_{t-2} + \eta_t$, where $\eta_t$ are normally distributed with $\ex[\eta_t] = 0$ and $\ex[\eta_t^2] = \nu^2$. We set $a_1 = 0.164$, $a_2 = 0.181$ and $\nu^2 = 0.321$, thus matching the estimated values obtained in the application of Section \ref{sec-data}. To simulate data under the null hypothesis, we let $m$ be a constant function. In particular, we set $m = 0$ without loss of generality. To generate data under the alternative, we consider the trend functions $m(u) = \beta \cdot (u - 0.6) \cdot \ind(0.6 \le u \le 1)$ with $\beta = 1.25, 1.875, 2.5$. These functions are broken lines with a kink at $u = 0.6$ and different slopes $\beta$. The slope parameter $\beta$ corresponds to a trend with the value $m(1) = 0.4 \beta$ at the right endpoint $u = 1$. We thus consider broken lines with the values $m(1) = 0.5, 0.75, 1.0$. Inspecting the local linear trend estimates in the real-data example of Section \ref{sec-data} which are plotted in the middle panel of  Figure \ref{plot-results-app1}, the broken line with the slope $\beta = 2.5$ can be seen to resemble the plotted trend estimates the most (where we neglect the nonlinearities of the local linear fits at the beginning of the observation period). The broken lines with the smaller slopes $\beta = 1.25$ and $\beta = 1.875$ are closer to the null making it harder for our test to detect these alternatives.\footnote{The broken lines $m$ are obviously non-differentiable at the kink point. We could replace them by slightly smoothed versions to satisfy the differentiability assumption that is imposed in the theoretical part of the paper. However, as this leaves the simulation results essentially unchanged but only creates additional notation, we stick to the broken lines.}


\begin{sidewaystable}
\centering
\footnotesize{
\caption{Size of our multiscale test for different AR parameters $a_1$ and $a_2$, sample sizes $T$ and nominal sizes $\alpha$.}\label{tab:size_test}
\newcolumntype{C}[1]{>{\hsize=#1\hsize\centering\arraybackslash}X}
\newcolumntype{Z}{>{\centering\arraybackslash}X}
\begin{tabularx}{\textwidth}{C{2} C{0.1} ZZZ C{0.1} ZZZ C{0.1} ZZZ C{0.1} ZZZ C{0.1} ZZZ} 
\toprule
 & &  \multicolumn{3}{c}{$a_1 = -0.5$} & &  \multicolumn{3}{c}{$a_1 = -0.25$} & &  \multicolumn{3}{c}{$a_1 = 0.25$} & &  \multicolumn{3}{c}{$a_1 = 0.5$} & &  \multicolumn{3}{c}{$(a_1,a_2) = (??,??)$} \\
\cmidrule[0.4pt]{3-5} \cmidrule[0.4pt]{7-9} \cmidrule[0.4pt]{11-13} \cmidrule[0.4pt]{15-17} \cmidrule[0.4pt]{19-21}
 $T$& &  \multicolumn{3}{c}{nominal size $\alpha$} & &  \multicolumn{3}{c}{nominal size $\alpha$} & &  \multicolumn{3}{c}{nominal size $\alpha$} & &  \multicolumn{3}{c}{nominal size $\alpha$} & &  \multicolumn{3}{c}{nominal size $\alpha$} \\
 & &  0.01 & 0.05  & 0.1 & &  0.01 & 0.05  & 0.1  & &  0.01 & 0.05  & 0.1  & &  0.01 & 0.05  & 0.1  & &  0.01 & 0.05  & 0.1   \\
\cmidrule[0.4pt]{1-21}
250 &  & 0.004 & 0.068 & 0.129 &  & 0.006 & 0.057 & 0.117 &  & 0.008 & 0.049 & 0.112 &  & 0.008 & 0.047 & 0.103 &  & 0.009 & 0.060 & 0.104 \\ 
  350 &  & 0.026 & 0.063 & 0.099 &  & 0.021 & 0.066 & 0.119 &  & 0.022 & 0.058 & 0.099 &  & 0.022 & 0.067 & 0.099 &  & 0.026 & 0.063 & 0.107 \\ 
  500 &  & 0.007 & 0.068 & 0.118 &  & 0.007 & 0.058 & 0.129 &  & 0.010 & 0.053 & 0.136 &  & 0.011 & 0.061 & 0.124 &  & 0.015 & 0.067 & 0.123 \\ 
\bottomrule
\end{tabularx}
\vspace{0.5cm}

\caption{Power of our multiscale test for different AR parameters $a_1$ and $a_2$, sample sizes $T$ and nominal sizes $\alpha$. The three panels (a)--(c) corresponds to different slope parameters $\beta$ of the broken line $m$.}\label{tab:power_test}

\begin{tabularx}{\textwidth}{C{2} C{0.1} ZZZ C{0.1} ZZZ C{0.1} ZZZ C{0.1} ZZZ C{0.1} ZZZ} 
\multicolumn{21}{c}{(a) $\beta = 1.25$} \\[0.2cm]
\toprule
 & &  \multicolumn{3}{c}{$a_1 = -0.5$} & &  \multicolumn{3}{c}{$a_1 = -0.25$} & &  \multicolumn{3}{c}{$a_1 = 0.25$} & &  \multicolumn{3}{c}{$a_1 = 0.5$} & &  \multicolumn{3}{c}{$(a_1,a_2) = (??,??)$} \\
\cmidrule[0.4pt]{3-5} \cmidrule[0.4pt]{7-9} \cmidrule[0.4pt]{11-13} \cmidrule[0.4pt]{15-17} \cmidrule[0.4pt]{19-21}
  $T$& &  \multicolumn{3}{c}{nominal size $\alpha$} & &  \multicolumn{3}{c}{nominal size $\alpha$} & &  \multicolumn{3}{c}{nominal size $\alpha$} & &  \multicolumn{3}{c}{nominal size $\alpha$} & &  \multicolumn{3}{c}{nominal size $\alpha$} \\
        & &  0.01 & 0.05  & 0.1   & &  0.01 & 0.05  & 0.1   & &  0.01 & 0.05  & 0.1    & &  0.01 & 0.05  & 0.1    & &  0.01 & 0.05  & 0.1   \\
\cmidrule[0.4pt]{1-21}
250 &  & 0.188 & 0.431 & 0.542 &  & 0.079 & 0.300 & 0.411 &  & 0.025 & 0.143 & 0.222 &  & 0.016 & 0.076 & 0.144 &  & 0.091 & 0.243 & 0.383 \\ 
  350 &  & 0.577 & 0.832 & 0.893 &  & 0.358 & 0.677 & 0.754 &  & 0.077 & 0.259 & 0.332 &  & 0.029 & 0.122 & 0.213 &  & 0.300 & 0.570 & 0.677 \\ 
  500 &  & 0.925 & 0.988 & 0.993 &  & 0.724 & 0.907 & 0.954 &  & 0.179 & 0.430 & 0.552 &  & 0.066 & 0.219 & 0.276 &  & 0.593 & 0.819 & 0.882 \\ 
\bottomrule
\end{tabularx}
\vspace{0.25cm}

\begin{tabularx}{\textwidth}{C{2} C{0.1} ZZZ C{0.1} ZZZ C{0.1} ZZZ C{0.1} ZZZ C{0.1} ZZZ} 
\multicolumn{21}{c}{(b) $\beta = 1.875$} \\[0.2cm]
\toprule
 & &  \multicolumn{3}{c}{$a_1 = -0.5$} & &  \multicolumn{3}{c}{$a_1 = -0.25$} & &  \multicolumn{3}{c}{$a_1 = 0.25$} & &  \multicolumn{3}{c}{$a_1 = 0.5$} & &  \multicolumn{3}{c}{$(a_1,a_2) = (??,??)$} \\
\cmidrule[0.4pt]{3-5} \cmidrule[0.4pt]{7-9} \cmidrule[0.4pt]{11-13} \cmidrule[0.4pt]{15-17} \cmidrule[0.4pt]{19-21}
 $T$ & &  \multicolumn{3}{c}{nominal size $\alpha$} & &  \multicolumn{3}{c}{nominal size $\alpha$} & &  \multicolumn{3}{c}{nominal size $\alpha$} & &  \multicolumn{3}{c}{nominal size $\alpha$} & &  \multicolumn{3}{c}{nominal size $\alpha$} \\
        & &  0.01 & 0.05  & 0.1   & &  0.01 & 0.05  & 0.1   & &  0.01 & 0.05  & 0.1    & &  0.01 & 0.05  & 0.1    & &  0.01 & 0.05  & 0.1   \\
\cmidrule[0.4pt]{1-21}
250 &  & 0.380 & 0.578 & 0.716 &  & 0.243 & 0.410 & 0.528 &  & 0.076 & 0.165 & 0.246 &  & 0.036 & 0.085 & 0.160 &  & 0.207 & 0.349 & 0.445 \\ 
  350 &  & 0.844 & 0.946 & 0.964 &  & 0.669 & 0.831 & 0.888 &  & 0.199 & 0.331 & 0.439 &  & 0.069 & 0.162 & 0.223 &  & 0.568 & 0.699 & 0.801 \\ 
  500 &  & 0.996 & 0.999 & 1.000 &  & 0.944 & 0.989 & 0.993 &  & 0.394 & 0.558 & 0.700 &  & 0.143 & 0.299 & 0.346 &  & 0.881 & 0.938 & 0.972 \\ 
\bottomrule
\end{tabularx}
\vspace{0.25cm}

\begin{tabularx}{\textwidth}{C{2} C{0.1} ZZZ C{0.1} ZZZ C{0.1} ZZZ C{0.1} ZZZ C{0.1} ZZZ} 
\multicolumn{21}{c}{(c) $\beta = 2.5$} \\[0.2cm]
\toprule
 & &  \multicolumn{3}{c}{$a_1 = -0.5$} & &  \multicolumn{3}{c}{$a_1 = -0.25$} & &  \multicolumn{3}{c}{$a_1 = 0.25$} & &  \multicolumn{3}{c}{$a_1 = 0.5$} & &  \multicolumn{3}{c}{$(a_1,a_2) = (??,??)$} \\
\cmidrule[0.4pt]{3-5} \cmidrule[0.4pt]{7-9} \cmidrule[0.4pt]{11-13} \cmidrule[0.4pt]{15-17} \cmidrule[0.4pt]{19-21}
  $T$& &  \multicolumn{3}{c}{nominal size $\alpha$} & &  \multicolumn{3}{c}{nominal size $\alpha$} & &  \multicolumn{3}{c}{nominal size $\alpha$} & &  \multicolumn{3}{c}{nominal size $\alpha$} & &  \multicolumn{3}{c}{nominal size $\alpha$} \\
        & &  0.01 & 0.05  & 0.1   & &  0.01 & 0.05  & 0.1   & &  0.01 & 0.05  & 0.1    & &  0.01 & 0.05  & 0.1    & &  0.01 & 0.05  & 0.1   \\
\cmidrule[0.4pt]{1-21}
250 &  & 0.515 & 0.781 & 0.872 &  & 0.323 & 0.585 & 0.722 &  & 0.055 & 0.214 & 0.301 &  & 0.034 & 0.126 & 0.205 &  & 0.255 & 0.502 & 0.632 \\ 
  350 &  & 0.967 & 0.995 & 1.000 &  & 0.827 & 0.956 & 0.986 &  & 0.214 & 0.482 & 0.583 &  & 0.068 & 0.216 & 0.306 &  & 0.720 & 0.893 & 0.938 \\ 
  500 &  & 1.000 & 1.000 & 1.000 &  & 0.994 & 1.000 & 0.999 &  & 0.544 & 0.784 & 0.872 &  & 0.161 & 0.392 & 0.489 &  & 0.957 & 0.996 & 0.998 \\ 
\bottomrule
\end{tabularx}
}
\end{sidewaystable}


To implement our test, we choose $K$ to be an Epanechnikov kernel and define the set $\mathcal{G}_T$ of location-scale points $(u,h)$ as
\begin{align}
\mathcal{G}_T = \big\{ (u, h): & \, \, u = 5k/T \text{ for some } 1 \le k \le T/5 \text{ and } \nonumber \\ & \, \, h = (3+5\ell)/T \text{ for some } 0 \le \ell \le T/20 \big\}. \label{grid-sim-app}
\end{align}
We thus take into account all rescaled time points $u \in [0,1]$ on an equidistant grid with step length $5/T$. For the bandwidth $h = (3 + 5\ell)/T$ and any $u \in [h,1-h]$, the local linear weights $w_{t,T}(u,h)$ are non-zero for exactly $5 + 10 \ell$ observations. Hence, the bandwidths $h$ in $\mathcal{G}_T$ correspond to effective sample sizes of $5, 15, 25, \ldots$ up to approximately $T/4$ data points. As a robustness check, we have re-run the simulations for a number of other grids. As the results are very similar, we do however not report them here. The long-run error variance $\sigma^2$ is estimated by the procedures from Section \ref{subsec-error-var-ar}. Specifically, we first compute the pilot estimator $\widetilde{\boldsymbol{a}}_q$ with $q = 20$ and then produce the second-step estimator $\widehat{\boldsymbol{a}}$ with $\overline{r} = 10$. Based on $\widehat{\boldsymbol{a}}$, we finally compute the estimator $\widehat{\sigma}^2$ of the long-run error variance $\sigma^2$. As a further robustness check, we have re-run the simulations for other choices of the tuning parameters $q$ and $\overline{r}$, which yields very similar results. The dependence of the estimators $\widetilde{\boldsymbol{a}}_q$, $\widehat{\boldsymbol{a}}$ and $\widehat{\sigma}^2$ on the tuning parameters $q$ and $\overline{r}$ is further explored in Section \ref{subsec-sim-3}. To compute the critical values of the multiscale test, we simulate $1000$ values of the statistic $\Phi_T$ defined in Section \ref{subsec-method-test} and compute their empirical $(1-\alpha)$ quantile $q_T(\alpha)$. 


Tables \ref{tab:size_test} and \ref{tab:power_test} report the simulation results for the sample sizes $T=250,350,500$ and the significance levels $\alpha = 0.01, 0.05, 0.10$. The sample size $T = 350$ is approximately equal to the time series length $359$ in the real-data example of Section \ref{sec-data}. To produce our simulation results, we generate $S=1000$ samples for each model specification and carry out the multiscale test for each simulated sample. The entries of Tables \ref{tab:size_test} and \ref{tab:power_test} are computed as the number of simulations in which the test rejects divided by the total number of simulations. As can be seen from Table \ref{tab:size_test}, the actual size of the test is fairly close to the nominal target $\alpha$ even for small values of $T$. Hence, the test has approximately the correct size. Inspecting Table \ref{tab:power_test}, one can further see that the test has reasonable power properties. For the smallest value $\beta = 1.25$, the deviation from the null is quite small, making it hard for the test to detect the alternative. As a consequence, the power is only moderate for $T=250$ and $T=350$. When we move further away from the null by increasing the slope parameter $\beta$, the power of the test quickly increases. It can also be seen to rapidly get larger as the sample size grows. For the slope $\beta =2.5$, the sample size $T=350$ and the AR(2) error process, which is the model specification that resembles the real-life data in Section \ref{sec-data} the most, the power of the test is above $??\%$ for all significance levels $\alpha$ considered and thus comes quite close to $1$. 


%We next explore in more detail how the multiscale test is influenced by different specifications of the error process. To do so, we simulate data from the model $Y_{t,T} = m(t/T) + \varepsilon_t$ with different AR(1) processes $\{ \varepsilon_t \}$. Specifically, we let $\varepsilon_t = a \varepsilon_{t-1} + \eta_t$, where $a \in \{-0.5,-0.25,0.25,0.5\}$ and $\eta_t$ are i.i.d.\ standard normal. To explore the size properties of the test, we generate data under the null by setting $m = 0$ as before. We do not further investigate the power properties since additional simulation exercises on the power of the test are included in Section \ref{subsec-sim-2}, where we compare it with SiZer for time series. The test is implemented exactly as described above. The simulation results are reported in Table ??.  


\subsection{Comparison with SiZer}\label{subsec-sim-2}


% However, their analysis is mainly methodlogical and the developed tools have an explorative character. Our multiscale method in ciontrast is a statistical rigorous test which is back up by a complete asymptotic theory. 


We now compare our multiscale test to SiZer for times series as described in \cite{Rondonotti2007}. Roughly speaking, the SiZer method proceeds as follows: For each location $u$ and bandwidth $h$ in a pre-specified set, SiZer computes an estimator $\widehat{m}_h^\prime(u)$ of the derivative $m^\prime(u)$ and a corresponding confidence interval. For each $(u,h)$, it then checks whether the confidence interval includes the value $0$. The set $\Pi_T^{\text{SiZer}}$ of points $(u,h)$ for which the confidence interval does not include $0$ corresponds to the set of intervals $\Pi_T^\pm$ for which our multiscale test rejects the null hypothesis $H_0$ that $m$ is constant on all intervals $[u-h,u+h]$ under consideration. %finds an increase/decrease in $m$. 
In order to explore how our test performs in comparison to SiZer, we compare the two sets $\Pi_T^\pm$ and $\Pi_T^{\text{SiZer}}$ in different ways to each other in what follows. 


The SiZer method is implemented as described in Section ?? of the Supplementary Material. To simplify its implementation, we assume that the autocovariance function $\gamma_\varepsilon(\cdot)$ of the error process and thus the long-run error variance $\sigma^2$ is known. Our multiscale test is implemented in the same way as in the previous section. To keep the comparison fair, we of course treat $\sigma^2$ as known also when implementing our method. Moreover, we use the same grid $\mathcal{G}_T$ of points $(u,h)$ for both methods. To achieve this, we start off with the grid $\mathcal{G}_T$ from \eqref{grid-sim-app} and then restrict attention to those points $(u,h) \in \mathcal{G}_T$ where the number of ``independent blocks'', as argued in \cite{Rondonotti2007}, is not too sparse for reasonable statistical inference. In particular, we calculate the effective sample size $\text{ESS}^*$ for correlated data and consider the grid $\mathcal{G}_T^* = \{ (u, h) \in \mathcal{G}_T \, | \, \text{ESS}^*(u, h) \geq 5 \}$. A detailed discussion of ``independent blocks'' and $\text{ESS}^*$ can be found in \cite{ChaudhuriMarron1999} and \cite{Rondonotti2007}. 


\begin{table}[t!]
\centering
\footnotesize{
\caption{Size of our multiscale test (MT) and SiZer for different model specifications.}\label{tab:size_comparison}
\newcolumntype{C}[1]{>{\hsize=#1\hsize\centering\arraybackslash}X}
\newcolumntype{Z}{>{\centering\arraybackslash}X}
\begin{tabularx}{\textwidth}{C{2} C{0.0001} ZZZZZZ C{0.0001} ZZZZZZ} 
\toprule
        & & \multicolumn{6}{c}{$a_1 = -0.5$} & & \multicolumn{6}{c}{$a_1 = 0.5$} \\ 
\cmidrule[0.4pt]{3-8} \cmidrule[0.4pt]{10-15}
        & & \multicolumn{2}{c}{$\alpha=0.01$} & \multicolumn{2}{c}{$\alpha=0.05$}  & \multicolumn{2}{c}{$\alpha=0.1$} 
        & & \multicolumn{2}{c}{$\alpha=0.01$} & \multicolumn{2}{c}{$\alpha=0.05$}  & \multicolumn{2}{c}{$\alpha=0.1$} \\[0.1cm]
        & & MT & SiZer & MT & SiZer & MT & SiZer & & MT & SiZer & MT & SiZer & MT & SiZer \\
\cmidrule[0.4pt]{1-15}
$T=250$ & & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 \\
$T=350$ & & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 \\ 
$T=500$ & & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 \\  
\bottomrule
\end{tabularx}
\vspace{0.5cm}

\caption{Power of our multiscale test (MT) and SiZer for different model specifications. The three panels (a)--(c) corresponds to different slope parameters $\beta$ of the linear tend $m$.}\label{tab:power_comparison}
\newcolumntype{C}[1]{>{\hsize=#1\hsize\centering\arraybackslash}X}
\newcolumntype{Z}{>{\centering\arraybackslash}X}
\begin{tabularx}{\textwidth}{C{2} C{0.0001} ZZZZZZ C{0.0001} ZZZZZZ} 
\multicolumn{15}{c}{(a) $\beta = 3.5$} \\[0.2cm]
\toprule
        & & \multicolumn{6}{c}{$a_1 = -0.5$} & & \multicolumn{6}{c}{$a_1 = 0.5$} \\ 
\cmidrule[0.4pt]{3-8} \cmidrule[0.4pt]{10-15}
        & & \multicolumn{2}{c}{$\alpha=0.01$} & \multicolumn{2}{c}{$\alpha=0.05$}  & \multicolumn{2}{c}{$\alpha=0.1$} 
        & & \multicolumn{2}{c}{$\alpha=0.01$} & \multicolumn{2}{c}{$\alpha=0.05$}  & \multicolumn{2}{c}{$\alpha=0.1$} \\[0.1cm]
        & & MT & SiZer & MT & SiZer & MT & SiZer & & MT & SiZer & MT & SiZer & MT & SiZer \\
\cmidrule[0.4pt]{1-15}
$T=250$ & & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 \\
$T=350$ & & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 \\ 
$T=500$ & & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 \\  
\bottomrule
\end{tabularx}
\vspace{0.25cm}

\begin{tabularx}{\textwidth}{C{2} C{0.0001} ZZZZZZ C{0.0001} ZZZZZZ} 
\multicolumn{15}{c}{(b) $\beta = 4.25$} \\[0.2cm]
\toprule
        & & \multicolumn{6}{c}{$a_1 = -0.5$} & & \multicolumn{6}{c}{$a_1 = 0.5$} \\ 
\cmidrule[0.4pt]{3-8} \cmidrule[0.4pt]{10-15}
        & & \multicolumn{2}{c}{$\alpha=0.01$} & \multicolumn{2}{c}{$\alpha=0.05$}  & \multicolumn{2}{c}{$\alpha=0.1$} 
        & & \multicolumn{2}{c}{$\alpha=0.01$} & \multicolumn{2}{c}{$\alpha=0.05$}  & \multicolumn{2}{c}{$\alpha=0.1$} \\[0.1cm]
        & & MT & SiZer & MT & SiZer & MT & SiZer & & MT & SiZer & MT & SiZer & MT & SiZer \\
\cmidrule[0.4pt]{1-15}
$T=250$ & & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 \\
$T=350$ & & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 \\ 
$T=500$ & & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 \\  
\bottomrule
\end{tabularx}
\vspace{0.25cm}

\begin{tabularx}{\textwidth}{C{2} C{0.0001} ZZZZZZ C{0.0001} ZZZZZZ} 
\multicolumn{15}{c}{(c) $\beta = 5$} \\[0.2cm]
\toprule
        & & \multicolumn{6}{c}{$a_1 = -0.5$} & & \multicolumn{6}{c}{$a_1 = 0.5$} \\ 
\cmidrule[0.4pt]{3-8} \cmidrule[0.4pt]{10-15}
        & & \multicolumn{2}{c}{$\alpha=0.01$} & \multicolumn{2}{c}{$\alpha=0.05$}  & \multicolumn{2}{c}{$\alpha=0.1$} 
        & & \multicolumn{2}{c}{$\alpha=0.01$} & \multicolumn{2}{c}{$\alpha=0.05$}  & \multicolumn{2}{c}{$\alpha=0.1$} \\[0.1cm]
        & & MT & SiZer & MT & SiZer & MT & SiZer & & MT & SiZer & MT & SiZer & MT & SiZer \\
\cmidrule[0.4pt]{1-15}
$T=250$ & & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 \\
$T=350$ & & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 \\ 
$T=500$ & & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 \\  
\bottomrule
\end{tabularx}
}
\end{table}


In the first part of the comparison study, we analyse the size and power of the two methods. To do so, we treat SiZer as a rigorous multiple testing method which rejects the null hypothesis $H_0$ if the set $\Pi_T^{\text{SiZer}}$ is empty, that is, if the value $0$ is not included in the confidence interval for at least one point $(u,h) \in \mathcal{G}_T^*$. We simulate data from the model $Y_{t,T} = m(t/T) + \varepsilon_t$ with different AR($1$) error processes and different trends $m$. In particular, we let $\{\varepsilon_t\}$ be an AR($1$) process of the form $\varepsilon_t = a_1 \varepsilon_{t-1} + \eta_t$ with $a_1 \in \{ -0.5,0.5\}$ and i.i.d.\ standard normal innovations $\eta_t$. To simulate data under the null, we set $m = 0$ as in the previous section. To generate data under the alternative, we consider the linear trends $m(u) = \beta (u - 0.5)$ with $\beta \in \{ 3.5, 4.25, 5.0 \}$. Further model specifications with nonlinear trends are considered in the second part of the comparison study. To produce our simulation results, we generate $S=1000$ samples for each model specification and carry out the two methods for each sample. 


The simulation results are reported in Tables \ref{tab:size_comparison} and \ref{tab:power_comparison}. Both for our multiscale test and SiZer, the entries in the tables are computed as the number of simulations in which the respective method rejects the null hypothesis $H_0$ divided by the total number of simulations. As can be seen from Table \ref{tab:size_comparison}, our test has approximately correct size in all of the considered settings, whereas SiZer is very liberal and rejects the null way too often. Examining Table \ref{tab:power_comparison}, one can further see that our procedure has reasonable power against the considered alternatives. The power of the SiZer method is somewhat higher, which is a trivial consequence of it being extremely liberal and should thus be treated with caution. All in all, the simulations suggest that SiZer can hardly be regarded as a rigorous statistical test of the null hypothesis $H_0$ that $m$ is constant on all intervals $[u-h,u+h]$ with $(u,h) \in \mathcal{G}_T^*$. This is not very surprising as SiZer is not designed to be such a test but to produce informative SiZer maps. In what follows, we thus attempt to compare the two methods in a different way which goes beyond mere size and power comparisons. 


Both our method and SiZer can be regarded as statistical tools to identify time regions where the curve $m$ is increasing/decreasing.\footnote{More precisely speaking, SiZer is usually interpreted as investigating the curve $m$, viewed at different levels of resolution, rather than the curve $m$ itself. Put differently, the underlying object of interest is a family of smoothed versions of $m$ rather than $m$ itself.} Suppose that $m$ is increasing/decreasing in the time region $\mathcal{R} \subset [0,1]$ but constant otherwise, that is, $m^\prime(u) \ne 0$ for all $u \in \mathcal{R}$ and $m^\prime(u) = 0$ for all $u \notin \mathcal{R}$. A natural question is the following: How well can the two methods identify the time region $\mathcal{R}$? In our framework, information on the region $\mathcal{R}$ is contained in the minimal intervals of the set $\Pi_T^\pm$. In particular, the union $\mathcal{R}_T^\pm$ of the minimal intervals in $\Pi_T^\pm$ can be regarded as an estimate of $\mathcal{R}$. This is a direct consequence of the results in Propositions \ref{prop-test-2} and \ref{prop-test-3}. Let $\mathcal{R}_T^{\text{SiZer}}$ be the union of the minimal intervals in $\Pi_T^{\text{SiZer}}$. In what follows, we compare $\mathcal{R}_T^\pm$ and $\mathcal{R}_T^{\text{SiZer}}$ to the region $\mathcal{R}$. This gives us information on how well the two methods approximate the true region where $m$ is increasing/decreasing.\footnote{The same exercise could of course also be carried out separately for the time region $[0.4,0.5)$ where the trend $m$ increases and the region $(0.5,0.6]$ where it decreases.} 


\begin{figure}[t]
\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=.9\linewidth]{Plots/min_int_with_T_500_a1_-50.pdf}
\caption{$a_1 = -0.5$}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=.9\linewidth]{Plots/min_int_with_T_500_a1_50.pdf}
\caption{$a_1 = 0.5$}
\end{subfigure}
\caption{Comparison of the regions $\mathcal{R}_T^\pm$ and $\mathcal{R}_T^{\text{SiZer}}$. Subfigure (a) corresponds to the model setting with the AR parameter $a_1 = -0.5$, subfigure (b) to the setting with $a_1 = 0.5$. The upper panel of each subfigure shows a simulated time series path together with the underlying trend function $m$. The middle panel depicts the regions $\mathcal{R}_T^\pm$ produced by our multiscale test for $100$ simulation runs. The lower panel presents the regions $\mathcal{R}_T^{\text{SiZer}}$ produced by SiZer.}  
\label{fig:comparison_SiZer}
\end{figure}


We consider the same simulation setup as in the first part of the study, only the trend function $m$ is different. We let $m$ be defined as $m(u) = 2 \cdot \ind(u \in [0.4,0.6]) \cdot (1 - 100 \{u-0.5\}^2)^2$, which implies that $\mathcal{R} = [0.4,0.5) \cup (0.5,0.6]$. The function $m$ is plotted in the two upper panels of Figure \ref{fig:comparison_SiZer}. We set the significance level to $\alpha= 0.05$ and the sample size to $T=500$. For each AR parameter $a_1 \in \{ -0.5,0.5 \}$, we simulate $S=100$ samples and compute $\mathcal{R}_T^\pm$ and $\mathcal{R}_T^{\text{SiZer}}$ for each sample. The simulation results are depicted in Figure \ref{fig:comparison_SiZer}, the two subfigures (a) and (b) corresponding to different AR parameters. The upper panel of each subfigure displays the time series path of a representative simulation together with the trend function $m$. The middle panel shows the regions $\mathcal{R}_T^\pm$ produced by our multiscale approach for the $100$ simulation runs: On the $y$-axis, the simulation runs $i$ are enumerated for $1 \le i \le 100$, and the black line at $y$-level $i$ represents $\mathcal{R}_T^\pm$ for the $i$-th simulation. Finally, the lower panel of each subfigure depicts the regions $\mathcal{R}_T^{\text{SiZer}}$ in an analogous way. 


Inspecting Figure \ref{fig:comparison_SiZer}, our multiscale method can be seen to approximate the region $\mathcal{R}$ fairly well in both simulation scenarios under consideration. Especially for the negative AR parameter $a_1 = -0.5$, it performs substantially better than SiZer: For most simulations, $\mathcal{R}_T^\pm$ gives a good approximation to the region $\mathcal{R}$, whereas SiZer identifies regions of decrease/increase all over the unit interval $[0, 1]$. SiZer thus frequently mistakes fluctuations in the time series which are due to the dependence in the error terms for increases/decreases in $m$. For the positive AR parameter $a_1 = 0.5$, the difference in performance is not so pronounced. Nevertheless, also here, SiZer appears to spuriously detect increases/decreases in $m$ outside $\mathcal{R}$ more often than our method. 


To sum up, our multiscale test exhibits good size and power properties in the simulations, and the minimal intervals produced by it identify the time regions where $m$ increases/decreases in a quite reliable way. SiZer performs considerably worse in these respects. Nevertheless, it may still produce informative SiZer plots. % (which is what it is designed for anyway). 
All in all, we would like to regard the two methods as complementary rather than direct competitors. SiZer is an explorative tool which aims to give an overview of the increases/decreases in $m$ by means of a SiZer plot. Our method, in contrast, is tailored to be a rigorous statistical test of the hypothesis $H_0$. In particular, it allows to make rigorous confidence statements about whether and where the trend $m$ increases/decreases. Such statements are not possible with SiZer. 


\subsection{Small sample properties of the long-run variance estimator}\label{subsec-sim-3}


In the final part of the simulation study, we examine the estimators of the AR parameters and the long-run error variance from Section \ref{subsec-error-var-ar}. We simulate data from the model $Y_{t,T} = m(t/T) + \varepsilon_t$, where $\{ \varepsilon_t\}$ is an AR($1$) process of the form $\varepsilon_t = a_1 \varepsilon_{t-1} + \eta_t$. We consider the AR parameters $a_1 \in \{-0.95,-0.75,-0.5,-0.25,0.25,0.5,0.75,0.95\}$ and let $\eta_t$ be i.i.d.\ standard normal innovation terms. For simplicity, $m$ is chosen to be a linear function of the form $m(u) = \beta u$ with $\beta \in \{1,10\}$. The slope parameter $\beta = 1$ corresponds to a moderate trend in the data, whereas the trend becomes quite pronounced for $\beta = 10$. 


\begin{table}[t!]
\centering
\footnotesize{
\caption{Empirical mean and standard deviation of the AR parameter estimators.}\label{tab:AR_parameters}
\newcolumntype{L}{>{\raggedright\arraybackslash}X}
\newcolumntype{C}[1]{>{\hsize=#1\hsize\centering\arraybackslash}X}
\newcolumntype{Z}{>{\centering\arraybackslash}X}
\begin{tabularx}{0.925\textwidth}{LZZZZZ} 
\multicolumn{6}{c}{(a) $\beta = 1$} \\[0.2cm]
\toprule
 & & $\widetilde{a}$ & $\widehat{a}$ & $\widehat{a}_{\text{HvK}}$ & $\widehat{a}_{\text{oracle}}$ \\
\cmidrule[0.4pt]{1-6}
$a = -0.95$ & $T=250$ & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) \\
            & $T=500$ & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) \\[0.1cm]
$a = -0.75$ & $T=250$ & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) \\
            & $T=500$ & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) \\[0.1cm]
$a = -0.5$  & $T=250$ & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) \\
            & $T=500$ & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) \\[0.1cm]
$a = -0.25$ & $T=250$ & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) \\
            & $T=500$ & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) \\[0.1cm]
$a = 0.25$  & $T=250$ & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) \\
            & $T=500$ & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) \\[0.1cm]
$a = 0.5$   & $T=250$ & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) \\
            & $T=500$ & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) \\[0.1cm]
$a = 0.75$  & $T=250$ & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) \\
            & $T=500$ & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) \\[0.1cm]
$a = 0.95$  & $T=250$ & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) \\
            & $T=500$ & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) \\
\bottomrule
\end{tabularx}
\vspace{0.25cm}

\begin{tabularx}{0.925\textwidth}{LZZZZZ} 
\multicolumn{6}{c}{(b) $\beta = 10$} \\[0.2cm]
\toprule
 & & $\widetilde{a}$ & $\widehat{a}$ & $\widehat{a}_{\text{HvK}}$ & $\widehat{a}_{\text{oracle}}$ \\
\cmidrule[0.4pt]{1-6}
$a = -0.95$ & $T=250$ & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) \\
            & $T=500$ & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) \\[0.1cm]
$a = -0.75$ & $T=250$ & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) \\
            & $T=500$ & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) \\[0.1cm]
$a = -0.5$  & $T=250$ & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) \\
            & $T=500$ & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) \\[0.1cm]
$a = -0.25$ & $T=250$ & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) \\
            & $T=500$ & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) \\[0.1cm]
$a = 0.25$  & $T=250$ & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) \\
            & $T=500$ & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) \\[0.1cm]
$a = 0.5$   & $T=250$ & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) \\
            & $T=500$ & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) \\[0.1cm]
$a = 0.75$  & $T=250$ & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) \\
            & $T=500$ & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) \\[0.1cm]
$a = 0.95$  & $T=250$ & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) \\
            & $T=500$ & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) \\
\bottomrule
\end{tabularx}}
\end{table}


\begin{table}[t!]
\centering
\footnotesize{
\caption{Empirical mean and standard deviation of the long-run variance estimators.}\label{tab:lrv}
\newcolumntype{L}{>{\raggedright\arraybackslash}X}
\newcolumntype{C}[1]{>{\hsize=#1\hsize\centering\arraybackslash}X}
\newcolumntype{Z}{>{\centering\arraybackslash}X}
\begin{tabularx}{0.925\textwidth}{LZZZZZ} 
\multicolumn{6}{c}{(a) $\beta = 1$} \\[0.2cm]
\toprule
 & & $\widetilde{\sigma}^2$ & $\widehat{\sigma}^2$ & $\widehat{\sigma}^2_{\text{HvK}}$ & $\widehat{\sigma}^2_{\text{oracle}}$ \\
\cmidrule[0.4pt]{1-6}
$a = -0.95$ & $T=250$ & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) \\
            & $T=500$ & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) \\[0.1cm]
$a = -0.75$ & $T=250$ & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) \\
            & $T=500$ & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) \\[0.1cm]
$a = -0.5$  & $T=250$ & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) \\
            & $T=500$ & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) \\[0.1cm]
$a = -0.25$ & $T=250$ & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) \\
            & $T=500$ & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) \\[0.1cm]
$a = 0.25$  & $T=250$ & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) \\
            & $T=500$ & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) \\[0.1cm]
$a = 0.5$   & $T=250$ & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) \\
            & $T=500$ & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) \\[0.1cm]
$a = 0.75$  & $T=250$ & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) \\
            & $T=500$ & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) \\[0.1cm]
$a = 0.95$  & $T=250$ & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) \\
            & $T=500$ & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) \\
\bottomrule
\end{tabularx}
\vspace{0.25cm}

\begin{tabularx}{0.925\textwidth}{LZZZZZ} 
\multicolumn{6}{c}{(b) $\beta = 10$} \\[0.2cm]
\toprule
 & & $\widetilde{\sigma}^2$ & $\widehat{\sigma}^2$ & $\widehat{\sigma}^2_{\text{HvK}}$ & $\widehat{\sigma}^2_{\text{oracle}}$ \\
\cmidrule[0.4pt]{1-6}
$a = -0.95$ & $T=250$ & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) \\
            & $T=500$ & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) \\[0.1cm]
$a = -0.75$ & $T=250$ & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) \\
            & $T=500$ & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) \\[0.1cm]
$a = -0.5$  & $T=250$ & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) \\
            & $T=500$ & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) \\[0.1cm]
$a = -0.25$ & $T=250$ & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) \\
            & $T=500$ & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) \\[0.1cm]
$a = 0.25$  & $T=250$ & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) \\
            & $T=500$ & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) \\[0.1cm]
$a = 0.5$   & $T=250$ & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) \\
            & $T=500$ & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) \\[0.1cm]
$a = 0.75$  & $T=250$ & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) \\
            & $T=500$ & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) \\[0.1cm]
$a = 0.95$  & $T=250$ & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) \\
            & $T=500$ & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) & 0.000 (0.000) \\
\bottomrule
\end{tabularx}}
\end{table}


For each model specification, we generate $S=1000$ data samples and compute the following quantities for each simulated sample: 
\begin{enumerate}[label=(\roman*),leftmargin=0.9cm]
\item the first step-estimator $\widetilde{a}$ from \eqref{est-AR-FS-av} with the tuning parameters $\underline{q}$ and $\overline{q}$, which nests the estimator $\widetilde{a}_q$ from \eqref{est-AR-FS} as a special case upon setting $\underline{q} = \overline{q} = q$, and the corresponding estimator $\widetilde{\sigma}^2$ of the long-run error variance. 
\item the second-step estimator $\widehat{a}$ from \eqref{est-AR-SS-av} with the tuning parameter $\overline{r}$ and the corresponding long-run variance estimator $\widehat{\sigma}^2$. 
\item the estimators of $a$ and $\sigma^2$ from \cite{Hall2003}, which are denoted by $\widehat{a}_{\text{HvK}}$ and $\widehat{\sigma}^2_{\text{HvK}}$ for ease of reference. The estimator $\widehat{a}_{\text{HvK}}$ is computed as described in Section 2.2 of \cite{Hall2003} and $\widehat{\sigma}^2_{\text{HvK}}$ as defined at the bottom of p.447 in Section 2.3. The estimator $\widehat{a}_{\text{HvK}}$ depends on two tuning parameters which are labelled as $m_1$ and $m_2$ in \cite{Hall2003}. As these are the exact counterparts of $\underline{q}$ and $\overline{q}$, we set $m_1 = \underline{q}$ and $m_2 = \overline{q}$. 
\item oracle estimators $\widehat{a}_{\text{oracle}}$ and $\widehat{\sigma}^2_{\text{oracle}}$ of $a_1$ and $\sigma^2$, which are constructed under the assumption that the error process $\{\varepsilon_t\}$ is observed. For each simulation run, we compute $\widehat{a}_{\text{oracle}}$ as the maximum likelihood estimator of $a_1$ from the time series of simulated error terms $\varepsilon_1,\ldots,\varepsilon_T$. We then calculate the residuals $r_t = \varepsilon_t - \widehat{a}_{\text{oracle}} \, \varepsilon_{t-1}$ and estimate the innovation variance $\nu^2 = \ex[\eta_t^2]$ by $\widehat{\nu}_{\text{oracle}}^2 = (T-1)^{-1} \sum_{t=2}^T r_t^2$. Finally, we set $\widehat{\sigma}^2_{\text{oracle}} = \widehat{\nu}_{\text{oracle}}^2 / (1 - \widehat{a}_{\text{oracle}})^2$. 
\end{enumerate}
Throughout the section, we set $\underline{q}=20$, $\overline{q}=30$ and $\overline{r} = 10$. In order to assess how sensitive our estimators are to the choice of the tuning parameters $\underline{q}$, $\overline{q}$ and $\overline{r}$, we carry out a number of robustness checks, considering a range of different values for $\underline{q}$, $\overline{q}$ and $\overline{r}$. The results are reported in Section ?? in the Supplementary Material. Inspecting the robustness checks, our estimators can be seen to be rather insensitive to the choice of tuning parameters. In particular, we obtain essentially the same results when repeating the simulation exercises of this section for the values of $\underline{q}$, $\overline{q}$ and $\overline{r}$ considered in the Supplementary Material.


\begin{figure}[t!]
\centering
\includegraphics[width=\linewidth]{Plots/variance_histogram_1.pdf}
\caption{Histograms of the simulated values of the estimators $\widetilde{a}$, $\widehat{a}$, $\widehat{a}_{\text{HvK}}$, $\widehat{a}_{\text{oracle}}$ in the simulation scenario with $T=500$, $a_1 = 0.25$ and $\beta = 10$.}\label{fig:hist_AR_scenario1} 
\vspace{0.25cm}

\includegraphics[width=\linewidth]{Plots/variance_histogram_2.pdf}
\caption{Histograms of the simulated values of the estimators $\widetilde{a}$, $\widehat{a}$, $\widehat{a}_{\text{HvK}}$, $\widehat{a}_{\text{oracle}}$ in the simulation scenario with $T=500$, $a_1 = -0.95$ and $\beta = 1$.}\label{fig:hist_AR_scenario2}
\end{figure}


For each estimator $\widetilde{a}$, $\widehat{a}$, $\widehat{a}_{\text{HvK}}$, $\widehat{a}_{\text{oracle}}$ and $\widetilde{\sigma}^2$, $\widehat{\sigma}^2$, $\widehat{\sigma}^2_{\text{HvK}}$, $\widehat{\sigma}^2_{\text{oracle}}$ and for each model specification, the simulation output consists in a vector of length $S=1000$ which contains the $1000$ simulated values of the respective estimator. 
%We denote these vectors by $v(\widetilde{a}_{\text{AV}})$, $v(\widehat{a}_{\text{AV}})$, etc.\ in what follows. 
Tables \ref{tab:AR_parameters} and \ref{tab:lrv} report the empirical mean and standard deviation of these simulated values for each estimator. 
%the vectors $v(\widetilde{a}_{\text{AV}})$, $v(\widehat{a}_{\text{AV}})$, etc.\ 
Moreover, Figures \ref{fig:hist_AR_scenario1} and \ref{fig:hist_AR_scenario2} present histograms of these values for two specific simulation scenarios. The main findings can be summarized as follows: 
\begin{enumerate}[label=(\roman*),leftmargin=0.9cm]

\item The performance of our second-step estimators $\widehat{a}$ and $\widehat{\sigma}^2$ is fairly close to that of the oracle estimators $\widehat{a}_{\text{oracle}}$ and $\widehat{\sigma}^2_{\text{oracle}}$ in all of the considered simulation scenarios. In particular, the mean values and standard deviations in Tables \ref{tab:AR_parameters} and \ref{tab:lrv} are quite similar for our estimators and the corresponding oracles. 

\item In the simulation scenarios with a moderate trend ($\beta = 1$), the first-step estimators $\widetilde{a}$ and $\widetilde{\sigma}^2$ produce results very similar to those of the second-step estimators $\widehat{a}$ and $\widehat{\sigma}^2$. However, in the scenarios with a pronounced trend ($\beta = 10$), the first-step estimators are strongly biased. The reason is that the trend is not eliminated appropriately by them. The second-step estimators substantially reduce this bias. This point is nicely illustrated by Figure \ref{fig:hist_AR_scenario1} which shows histograms of the simulated values of the estimators $\widetilde{a}$, $\widehat{a}$, $\widehat{a}_{\text{HvK}}$, $\widehat{a}_{\text{oracle}}$ in the scenario with $T=500$, $a_1=0.25$ and $\beta = 10$. As one can see, the histogram produced by our second-step estimator $\widehat{a}$ is nicely centred around the true value $a_1 = 0.25$, whereas those of the estimators $\widetilde{a}$ and $\widehat{a}_{\text{HvK}}$ are strongly biased upwards. 

\item The estimators $\widehat{a}_{\text{HvK}}$ and $\widehat{\sigma}^2_{\text{HvK}}$ of \cite{Hall2003} exhibit a similar performance as our first-step estimators $\widetilde{a}$ and $\widetilde{\sigma}^2$ as long as the AR parameter $a_1$ is not too close to $-1$. For strongly negative values of $a_1$ (in particular for $a_1 = -0.75$ and $a_1 = -0.95$), the estimators perform much worse than ours. This is already indicated by the extremely large variances of the estimators $\widehat{a}_{\text{HvK}}$ and $\widehat{\sigma}^2_{\text{HvK}}$ in Tables \ref{tab:AR_parameters} and \ref{tab:lrv} in the scenarios with $a_1 \in \{-0.75,-0.95\}$. Figure \ref{fig:hist_AR_scenario1} gives some further insights into what is happening here. It presents the histograms of the simulated values of $\widetilde{a}$, $\widehat{a}$, $\widehat{a}_{\text{HvK}}$, $\widehat{a}_{\text{oracle}}$ in the scenario with $T=500$, $a_1=-0.95$ and $\beta = 1$. As can be seen, the estimator $\widehat{a}_{\text{HvK}}$ does not obey the causality restriction $|a_1| \le 1$ but frequently takes values smaller than $-1$. This results in a very large spread of the histogram and thus in a disastrous performance of the estimator. Our estimators $\widetilde{a}$ and $\widehat{a}$, in contrast, exhibit a stable behaviour in this case.
\end{enumerate}



\section{Application}\label{sec-data}


The analysis of time trends in long temperature records is an important task in climatology. Information on the shape of the trend is needed in order to better understand long-term climate variability. The Central England temperature record is the longest instrumental temperature time series in the world. It is a valuable asset for analysing climate variability over the last few hundred years. The data is publicly available on the webpage of the UK Met Office. A detailed description of the data can be found in \cite{Parker1992}. For our analysis, we use the dataset of yearly mean temperatures which consists of $T=359$ observations covering the years from $1659$ to $2017$. We assume that the data follow the nonparametric trend model $Y_{t,T} = m(t/T) + \varepsilon_t$, where $m$ is the unknown time trend of interest. The error process $\{ \varepsilon_t \}$ is supposed to have the AR($p$) structure $\varepsilon_t = \sum_{j=1}^p a_j \varepsilon_{t-j} + \eta_t$, where $\eta_t$ are i.i.d.\ innovations with mean $0$ and variance $\nu^2$. As pointed out in \cite{Mudelsee2010} among others, this is the most widely used error model for discrete climate time series. We select the AR order $p$ by minimizing an FPE criterion, which yields the AR $p=2$. We then estimate the parameters $a_1$, $a_2$ and $\nu^2$ as described in Section \ref{subsec-error-var-ar} which yields the estimates $\widehat{a}_1 \approx ??$, $\widehat{a}_2 \approx ??$ and $\widehat{\nu}^2 \approx ??$.


\begin{figure}[t]
\centering
\includegraphics[width=0.8\textwidth]{Plots/temperature_L1_20_L2_30.pdf}
\caption{Summary of the application results for Section \ref{sec-data}. The upper panel shows the Central England mean temperature time series. The middle panel depicts local linear kernel estimates of the time trend for a number of different bandwidths $h$. The lower panel presents the minimal intervals in the set $\Pi_T^+$ produced by the multiscale test. These are $[??,??]$, $[??,??]$, $[??,??]$ and $[??,??]$.}\label{plot-results-app1}
\end{figure}


With the help of our multiscale method from Section \ref{sec-method}, we test the null hypothesis $H_0$ that $m$ is constant on all intervals $[u-h,u+h]$ with $(u,h) \in \mathcal{G}_T$, where we use the grid $\mathcal{G}_T$ defined in \eqref{grid-sim-app}. To do so, we set the significance level to $\alpha = 0.05$ and implement the test in exactly the same way as in the simulations of Section \ref{sec-sim}. The results are presented in Figure \ref{plot-results-app1}. The upper panel shows the raw temperature time series, whereas the middle panel depicts local linear kernel estimates of the trend $m$ for different bandwidths $h$. As one can see, the shape of the estimated time trend strongly differs with the chosen bandwidth. When the bandwidth is small, there are many local increases and decreases in the estimated trend. When the bandwidth is large, most of these local variations get smoothed out. Hence, by themselves, the nonparametric fits do not give much information on whether the trend $m$ is increasing or decreasing in certain time regions. 


Our multiscale test provides this kind of information, which is summarized in the lower panel of Figure \ref{plot-results-app1}. The plot depicts the minimal intervals contained in the set $\Pi_T^+$ which is defined in Section \ref{subsec-method-theo}. The set of intervals $\Pi_T^-$ is empty in the present case. The height at which a minimal interval $I_{u,h} = [u-h,u+h] \in \Pi_t^+$ is plotted indicates the value of the corresponding (additively corrected) test statistic $\widehat{\psi}_T(u,h) / \widehat{\sigma} - \lambda(h)$. The dashed line specifies the critical value $q_T(\alpha)$, where $\alpha = 0.05$ as already mentioned above. According to Proposition \ref{prop-test-3}, we can make the following simultaneous confidence statement about the collection of minimal intervals in $\Pi_T^+$. We can claim, with confidence of about $95\%$, that the trend function $m$ has some increase on each minimal interval. More specifically, we can claim with this confidence that there has been some upward movement in the trend both in the period from around $1680$ to $1740$ and in the period from about $1880$ onwards. Hence, our test in particular provides evidence that there has been some warming trend in the period over approximately the last $140$ years. On the other hand, as the set $\Pi_T^-$ is empty, there is no evidence of any downward movement of the trend.  

