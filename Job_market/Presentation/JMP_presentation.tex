\documentclass[10pt]{beamer}

\usetheme[progressbar=frametitle]{metropolis}

\usepackage{appendixnumberbeamer}

\usepackage{booktabs}
\usepackage{blkarray}
%\usepackage{ccicons}
\usepackage{graphicx}
\usepackage{color}

\definecolor{UniBlue}{RGB}{7,82,154}
\definecolor{UniYellow}{RGB}{234,185,12}

%\setbeamercolor{title}{fg=UniBlue, bg = UniYellow}
%\setbeamercolor{frametitle}{fg=UniBlue, bg= UniYellow}
%\setbeamercolor{structure}{fg=UniBlue, bg= UniYellow}
%\setbeamercolor{progress bar}{fg=UniBlue, bg= UniYellow}
\usepackage{xspace}

\title{Simultaneous statistical inference for epidemic trends: the case of COVID-19}
\date{01/10/2020}
\author{Marina Khismatullina \and Michael Vogt}
\setbeamertemplate{frame footer}{Simultaneous statistical inference for epidemic trends: the case of COVID-19}
\metroset{block=fill}
% \titlegraphic{\hfill\includegraphics[height=1.5cm]{logo.pdf}}

\newcommand{\Prob}{\mathrm{P}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Corr}{\mathrm{Corr}}
\newcommand{\sgn}{\text{sgn}}
\newtheorem{prop}{Proposition}
\newcommand{\ind}{\boldsymbol{1}\Big( \frac{t}{T} \in \mathcal{I}_k \Big)} % indicator function
\newcommand{\indsmall}{\boldsymbol{1}\big( \frac{t}{T} \in \mathcal{I}_k \big)} % indicator function

\begin{document}

\maketitle

\begin{frame}{Table of contents}
  \setbeamertemplate{section in toc}[sections numbered]
  \tableofcontents[hideallsubsections]
\end{frame}

\section{Introduction}


\begin{frame}{Motivation}
	\textbf{Research question:}
	
	How do outbreak patterns of COVID-19 compare across countries?
	\begin{figure}
    		\centering
    		\includegraphics[height=0.45\textheight]{plots/Germany_and_Italy.pdf}
    		%\caption{Observed new cases per day in Germany and Italy}
    		\label{fig:DEUvsITA}
  	\end{figure}\pause
	\vspace{-3mm}
	\begin{block}{Aim of the paper}
	To develop new inference methods that allow to \textit{identify} and \textit{locate} differences between time trends.
	\end{block}
\end{frame}

\section{Model}
\begin{frame}{Model}
We observe $n$ time series $\mathcal{X}_i = \{X_{it}: 1 \le t \le T \}$ of length $T$:
\begin{equation*}
X_{it} = \lambda_i \Big( \frac{t}{T} \Big) + \sigma\sqrt{\lambda_i \Big( \frac{t}{T} \Big)} \eta_{it},
\end{equation*}\pause
\vspace{-3mm}
where
\begin{itemize}
\item $\lambda_i$ are unknown trend functions on $[0,1]$;
\item $\sigma$ is the overdispersion parameter;
\item $\eta_{it}$ are error terms that are independent across $i$ and $t$ and have zero mean and unit variance.
\end{itemize}
\end{frame}

\begin{frame}{Literature}
	Comparison of deterministic trends:
	\begin{itemize}
		\item Park et al. (2009), Degras et al. (2012), Zhang et al. (2012), Hidalgo and Lee (2014), Chen and Wu (2019).
	\end{itemize}\pause
	Studies of COVID-19:
	\begin{itemize}
		\item SEIR models: Yang et al. (2020), Wu et al. (2020), De Brouwer et al. (2020).
		\item Time series analysis: Gu et al. (2020), Li and Linton (2020).
		\item Dong et al. (2020).
	\end{itemize}
\end{frame}

\section{Testing}
\begin{frame}{Testing problem}

Let $\mathcal{F} =\{ \mathcal{I}_k \subseteq [0, 1]: 1 \le k \le K\}$ be a family of intervals on $[0, 1]$, and for a given interval $\mathcal{I}_k$ we want to test whether the functions $\lambda_i$ and $\lambda_j$ are the same on this interval. Formally, the testing problem is
\begin{align*}
H_0^{(ijk)}:\quad  \lambda_i(w) = \lambda_j(w) \text{ for all } w\in \mathcal{I}_k.
\end{align*}\pause
We want to test these hypothesis $H_0^{(ijk)}$ simultaneously for all pairs of countries $i$ and $j$ and all intervals $\mathcal{I}_k$ in the family $\mathcal{F}$.
\end{frame} 


\begin{frame}{Test statistic}
For the given interval $\mathcal{I}_k$ and a pair of time series $i$ and $j$ we calculate
\begin{equation*}
\hat{s}_{ijk,T} = \frac{1}{T h_k} \sum\limits_{t=1}^T \ind (X_{it} -X_{jt}), 
\end{equation*}
%\vspace{-3mm}
where $h_k$ is the length of $\mathcal{I}_k$. \pause The statistic $\hat{s}_{ijk,T}$ estimates the average distance between $\lambda_i$ and $\lambda_j$ on $\mathcal{I}_k$. \pause Under certain assumptions, 
\begin{align*}
\Var(\hat{s}_{ijk,T})  & = \frac{\sigma^2}{T^2 h_k^2} \sum\limits_{t=1}^T \ind \Big\{ \lambda_i\Big(\frac{t}{T}\Big) + \lambda_j\Big(\frac{t}{T}\Big) \Big\}. 
\end{align*}\pause
In order to normalize the variance of the statistic $\hat{s}_{ijk,T}$, we scale it by an estimator of its variance:
\[ \widehat{\Var(\hat{s}_{ijk,T})} = \frac{\hat{\sigma}^2}{T^2 h_k^2} \sum\limits_{t=1}^T \ind (X_{it} + X_{jt} ), \]
with $\hat{\sigma}^2 = n^{-1} \sum_{i = 1}^n \hat{\sigma}_i^2$ and $\hat{\sigma}_i^2 = \frac{\sum_{t=2}^T (X_{it}-X_{it-1})^2}{2 \sum_{t=1}^T X_{it}}$.\hyperlink{frame_sigma}{\beamerbutton{Idea}}
\end{frame}


\begin{frame}[label = frame_teststatistic]{Test statistic}
Test statistic for the hypothesis $H_0^{(ijk)}$ is defined as
\begin{equation*}
\widehat{\psi}_{ijk, T} = \frac{\sum\nolimits_{t=1}^T \indsmall (X_{it} -X_{jt})}{\hat{\sigma} \big\{ \sum\nolimits_{t=1}^T \indsmall  (X_{it} + X_{jt} )\big\}^{1/2}}. 
\end{equation*} \pause
Under certain conditions and under the null, $\widehat{\psi}_{ijk, T}$ can be approximated by the Gaussian version of the test statistic:
\begin{align*}
\phi_{ijk,T} = \frac{1}{\sqrt{2 T h_k}} \sum\limits_{t=1}^T \ind (Z_{it} - Z_{jt}), 
\end{align*}
where $Z_{it}$ are independent standard normal random variables.
\end{frame}


\begin{frame}[label = frame_test]{Test procedure}
\begin{align*}
H_0^{(ijk)}: \quad \lambda_i(w) = \lambda_j(w) \text{ for all } w \in \mathcal{I}_k.
\end{align*} \pause
How do determine critical values?\pause
\vspace{-2mm}
\begin{enumerate}
	\item Consider the Gaussian test statistic $ \Phi_T = \max_{(i,j,k)} a_k \big( |\phi_{ijk,T}| - b_k \big) $, where $a_k = \{\log(e/h_k)\}^{1/2} / \log \log(e^e / h_k)$ and $b_k = \sqrt{2 \log(1/h_k)}$ are scale-dependent constants.\pause
	\item Denote by $q_{T, \text{Gauss}} (\alpha)$ a $(1-\alpha)$-quantile of $\Phi_T$, which can be computed by Monte Carlo simulations.\pause
	\item Adjust the quantile $q_{T, \text{Gauss}} (\alpha)$ by the scale-dependent constants: $c_{T,\text{Gauss}}(\alpha,h_k) = b_k + q_{T,\text{Gauss}}(\alpha)/a_k$. \hyperlink{frame_constants}{\beamerbutton{Idea}} \pause
\end{enumerate}
\begin{block}{Test procedure}
For the given significance level $\alpha \in (0,1)$ and for each $(i,j,k)$, reject $H_0^{(ijk)}$ if $|\widehat{\psi}_{ijk,T}| > c_{T,\text{Gauss}}(\alpha,h_k)$.
\end{block}
\end{frame}

\section{Theoretical properties}
\begin{frame}{Assumptions}
\begin{itemize}
\onslide<1->\item[$\mathcal{C}1$] \label{C1} The functions $\lambda_i$ are uniformly Lipschitz continuous: $|\lambda_i(u) - \lambda_i(v)| \le L |u-v|$ for all $u, v \in [0,1]$.
\onslide<2->\item[$\mathcal{C}2$] \label{C2} $0 < \lambda_{\min} \le \lambda_i(w) \le \lambda_{\max} < \infty$ for all $w \in [0, 1]$ and all $i$. 
\onslide<3->\item[$\mathcal{C}3$] \label{C3} $\eta_{it}$ are independent both across $i$ and $t$.
\onslide<4->\item[$\mathcal{C}4$] \label{C4} $\E[\eta_{it}] = 0$, $\E[\eta_{it}^2] = 1$ and $\E[|\eta_{it}|^\theta] \le C_\theta < \infty$ for some $\theta > 4$. 
\onslide<5->\item[$\mathcal{C}5$] \label{C5} $h_{\max} = o(1/\log T)$ and $h_{\min} \ge CT^{-b}$ for some $b \in (0,1)$.
\onslide<6>\item[$\mathcal{C}6$] \label{C6} $p := \{ \# (i, j, k) \} = O(T^{(\theta/2)(1-b)-(1+\delta)})$ for some small $\delta > 0$.
\end{itemize}
\end{frame}


\begin{frame}{Theoretical properties}
\begin{prop}\label{prop1}
Denote $\mathcal{M}_0$ the set of triplets $(i, j, k)$ where $H_0^{(ijk)}$ holds true. Then under $\mathcal{C}1 - \mathcal{C}6$, it holds that 
\vspace{-2mm}
\begin{align*}
 \Prob\Big( \forall (i,j,k) \in \mathcal{M}_0: |\hat{\psi}_{ijk,T}| \le c_{T,\textnormal{Gauss}}(\alpha,h_k) \Big) \ge 1 - \alpha + o(1)
\end{align*}
\end{prop}\pause
\begin{prop}\label{prop2}
Consider any sequence of functions $\lambda_{i} = \lambda_{i,T}$, $\lambda_{j} = \lambda_{j, T}$ with the following property: There exists an interval $\mathcal{I}_{k}$ such that $\lambda_{i, T}(w) - \lambda_{j, T}(w) \ge c_T \sqrt{T \log T / h_{k}}$ for all $w \in \mathcal{I}_{k}$, and $c_T \rightarrow \infty$. Then under $\mathcal{C}1 - \mathcal{C}6$, it holds that
\[ \Prob\big(|\hat{\psi}_{ijk,T}| \le c_{T,\textnormal{Gauss}}(\alpha,h_k) \big) = o(1). \]
\end{prop}
\end{frame}

\begin{frame}{Notation}
\begin{center}
In order to proceed with the proof, we will need the following notation:
\end{center}
\vspace{-2mm}
\begin{align*}
\widehat{\psi}_{ijk, T} &= \frac{\sum\nolimits_{t=1}^T \indsmall (X_{it} -X_{jt})}{\hat{\sigma} \big\{ \sum\nolimits_{t=1}^T \indsmall  (X_{it} + X_{jt} )\big\}^{1/2}}, &&\\
\hat{\psi}_{ijk,T}^0 &= \frac{\sum\nolimits_{t=1}^T \indsmall \, \sigma \overline{\lambda}_{ij}^{1/2}(\frac{t}{T}) (\eta_{it} - \eta_{jt})}{ \hat{\sigma} \{ \sum\nolimits_{t=1}^T \indsmall (X_{it} + X_{jt}) \}^{1/2}} &&\hat{\Psi}_T^0 = \max_{(i,j,k)} a_k (|\hat{\psi}_{ijk,T}^0| - b_k),\\
\psi_{ijk,T}^0 &= \frac{1}{\sqrt{2Th_k}} \sum\limits_{t=1}^T \ind (\eta_{it} - \eta_{jt}) &&\Psi_T = \max_{(i,j,k)} a_k (|\psi_{ijk,T}^0| - b_k),\\
\phi_{ijk,T} &= \frac{1}{\sqrt{2 T h_k}} \sum\limits_{t=1}^T \ind (Z_{it} - Z_{jt}) &&\Phi_T = \max_{(i,j,k)} a_k (|\phi_{ijk,T}| - b_k).
\end{align*}
\end{frame}

\begin{frame}{Strategy of the proof}
\begin{enumerate}
\item We prove that  $\big| \hat{\Psi}_T^0 - \Psi_T \big| = o_p(r_T)$, where $\{r_T\}$ is some null sequence.

\item With the help of results from Chernozhukov et al. (2017), we prove
\begin{equation*}\label{eq:kolmogorov-distance}
\sup_{q \in \mathbf{R}} \Big| \Prob\big( \Psi_T \le q \big) - \Prob\big( \Phi_T \le q \big) \Big| = o(1).
\end{equation*}
\item By using these two results, we now show that 
\begin{equation}\label{eq:kolmogorov-distance-hat}
\sup_{q \in \mathbb{R}} \Big| \Prob\big( \hat{\Psi}_T^0 \le q \big) - \Prob\big( \Phi_T \le q \big) \Big| = o(1).
\end{equation}
\item $\Prob (\Phi_T \le q_{T,\text{Gauss}}(\alpha)) = 1-\alpha$ by definition of the quantile $q_{T,\text{Gauss}}(\alpha)$. From this and \eqref{eq:kolmogorov-distance-hat}, it immediately follows that  
\begin{equation*}
\Prob\big( \hat{\Psi}_T^0 \le q_{T,\text{Gauss}}(\alpha) \big) = 1 - \alpha + o(1), 
\end{equation*}
which in turn implies the desired statement.
\end{enumerate}
\end{frame}

\begin{frame}{Graphical representation}
\begin{block}{Minimal intervals}
An interval $\mathcal{I}_k \in \mathcal{F}_{\text{reject}}(i, j)$ is called \textbf{minimal} if there is no other interval $\mathcal{I}_{k^\prime} \in \mathcal{F}_{\text{reject}}(i, j)$ with $\mathcal{I}_{k^\prime} \subset \mathcal{I}_k$. The set of minimal intervals is denoted $\mathcal{F}_{\text{reject}}^{\min} (i, j)$.
\end{block}\pause
We can make very similar confidence statement about the set of minimal intervals as well:
\begin{align*}
 \Prob\Big( \forall (i,j,k) \in \mathcal{M}_0: \mathcal{I}_k \notin \mathcal{F}_{\text{reject}}^{\min} (i, j) \Big) \ge 1 - \alpha + o(1).
\end{align*}
\end{frame}

\section{Application}
\begin{frame}{Application results}
	\begin{figure}
		\includegraphics[width=0.49\textwidth]{plots/DEU_vs_ITA_presentation}
   		%\caption{Observed new cases per day in Germany and Italy}    			\label{fig:DEUvsITA}
		\hfill
		\includegraphics[width=0.49\textwidth]{plots/DEU_vs_ESP_presentation}
	\end{figure}
\end{frame}

\begin{frame}{Application results, part 2}
	\begin{figure}
		\includegraphics[width=0.49\textwidth]{plots/DEU_vs_GBR_presentation}
   		%\caption{Observed new cases per day in Germany and Italy}    			\label{fig:DEUvsITA}
		\hfill
		\includegraphics[width=0.49\textwidth]{plots/DEU_vs_FRA_presentation}
	\end{figure}
\end{frame}


\begin{frame}{Discussion}
We can claim, with confidence of about $95\%$, that we are able to detect all of the differences between the trend functions in different countries.\pause

However, we can not say anything about the causes of such differences. This question requires further (probably not purely statistical) analysis.\pause

Further possible extensions:
\vspace{-2mm}
\begin{itemize}
	\item introduce scaling factor in the trend function, that allow for adjusting for the size of the country (population, density, testing regimes, etc.);\pause
	\item connect with data-driven techniques such as machine learning;\pause
	\item cluster the countries based on the trends they exhibit; \pause
	\item build in policy changes.
\end{itemize}
\end{frame}

\begin{frame}[standout]
  Thank you!
\end{frame}


\appendix

\begin{frame}{Simulation results for the size of the test}
\begin{figure}[t!]
	\includegraphics[height = 0.4\textheight]{plots/lambda_fct}
\end{figure}
\vspace{-2mm}
\scriptsize{\begin{table}[t]
\begin{center}
\caption{Size of the multiscale test}
\label{tab:size_shape}
\input{plots/size_overdispersion_15}
\end{center}
\end{table}}
\end{frame}

\begin{frame}{Simulation results for the power of the test}
\begin{figure}[t!]
	\onslide<1->\includegraphics[width = 0.49\textwidth, height = 0.4\textheight]{plots/lambda_fcts_height}
	\onslide<2->\includegraphics[width = 0.49\textwidth, height = 0.4\textheight]{plots/lambda_fcts_shift}	
\end{figure}\pause
\vspace{-5mm}
{\onslide<1>\scriptsize{\begin{table}[t]
\begin{center}
\caption{Power of the multiscale test for scenario A}
\label{tab:size_shape}
\input{plots/power_sigma_15_higher_peak}
\end{center}
\end{table}}}
{\onslide<2>
\vspace{-39.5mm}
\scriptsize{\begin{table}[t]
\begin{center}
\caption{Power of the multiscale test for scenario B}
\label{tab:size_shape}
\input{plots/power_sigma_15_shifted_peak}
\end{center}
\end{table}}}
\end{frame}

\begin{frame}[label = frame_sigma]{Idea behind $\hat{\sigma}$}
We assume that $\lambda_i$ is Lipschitz continuous. Then
\[ X_{it} - X_{it-1} = \sigma \sqrt{\lambda_i\Big(\frac{t}{T}\Big)} (\eta_{it} - \eta_{it-1}) + r_{it}, \]
where $|r_{it}| \le C(1+|\eta_{it-1}|)/T$ with a sufficiently large $C$.\pause \, Hence,
\[ \frac{1}{T} \sum_{t=2}^T (X_{it} - X_{it-1})^2 = 2 \sigma^2 \Big\{ \frac{1}{T} \sum_{t=2}^T \lambda_i(t/T) \Big\} + o_p(1).\] \pause
Together with \[ \frac{1}{T} \sum_{t=1}^T X_{it} = \frac{1}{T} \sum_{t=1}^T \lambda_i(t/T) + o_p(1), \] we get that $\hat{\sigma}_i^2 = \sigma^2 + o_p(1)$ for any $i$ and thus $\hat{\sigma}^2 = \sigma^2 + o_p(1)$.

\hyperlink{frame_teststatistic}{\beamerbutton{Go back}}
\end{frame}

\begin{frame}[label = frame_constants]{Idea behind $a_k$ and $b_k$}
How to construct critical values $c_{ijk,T}(\alpha)$? \pause
\begin{itemize}
\item Traditional approach: $c_T(\alpha) = c_{ijk,T}(\alpha)$ for all $(i,j,k)$. \pause
\item A more modern approach: $c_{ijk,T}(\alpha)$ depend on the length $h_k$ of the time interval (D{\"u}mbgen and Spokoiny (2001)). In our context:
\begin{equation*}
c_{ijk,T}(\alpha) = c_T(\alpha,h_k) := b_k + q_T(\alpha)/a_k, 
\end{equation*}
where $a_k = \{\log(e/h_k)\}^{1/2} / \log \log(e^e / h_k)$ and $b_k = \sqrt{2 \log(1/h_k)}$ are scale-dependent constants and $q_T(\alpha)$ is the $(1-\alpha)$-quantile of the statistic 
\[ \hat{\Psi}_T = \max_{(i,j,k)} a_k \big( |\hat{\psi}_{ijk,T}^0| - b_k \big) \]
in order to ensure control of the FWER at level $\alpha$.
\end{itemize}
\hyperlink{frame_test<4>}{\beamerbutton{Go back}}
\end{frame}

\begin{frame}{Idea behind the additive correction}
Consider the uncorrected Gaussian statistic
\begin{align*}
\Phi_{T}^{\text{uncor}} = \max_{(i,j,k)} |\phi_{ijk, T}|
\end{align*}\pause
and let the family of intervals be \[\mathcal{F} = \big\{[(m-1) h_l, m h_l] \text{ for } 1\le m \le 1/h_l, 1 \le l \le L\big\}.\]\pause
Then we can rewrite the uncorrected test statistic as
\begin{align*}
\Phi_{T}^{\text{uncor}} = \max_{i, j} \max_{\substack{1 \le l \le L, \\ 1\le m \le 1/h_l}} \Big|\frac{1}{\sqrt{2 T h_l}} \sum\limits_{t=1}^T 1 \Big( \frac{t}{T} \in [(m-1) h_l, m h_l] \Big) (Z_{it} - Z_{jt})\Big|
\end{align*}\pause
$\Rightarrow \quad \max_m \ldots =\sqrt{2\log(1/h_l)} + o_P(1) \to \infty$ as $h \to 0$ and the stochastic behavior of $\Phi_{T}^{\text{uncor}}$ is dominated by the elements with small bandwidths $h_l$. \hyperlink{frame_test<4>}{\beamerbutton{Go back}}
\end{frame}


\end{document}
