\newpage
\appendix



\section{Appendix}\label{appendix}


In what follows, we provide detailed proofs of the theoretical results from Section \ref{sec:theo}. We use the following notation: The symbol $C$ denotes a universal real constant which may take a different value on each occurrence. For $a,b \in \reals$, we write $a \vee b = \max\{a,b\}$. For $x \in \reals_{\geq 0}$, we let $\lfloor x \rfloor$ denote the integer value of $x$ and $\lceil x \rceil$ the smallest integer greater than or equal to $x$. For any set $A$, the symbol $|A|$ denotes the cardinality of $A$. The expression $X \stackrel{\mathcal{D}}{=} Y$ means that the two random variables $X$ and $Y$ have the same distribution. Finally, we sometimes use the notation $a_T \ll b_T$ to express that $a_T = o(b_T)$. 



\subsection*{Auxiliary results}\label{subsec:appendix:aux}


Let $\{Z_t\}_{t=-\infty}^\infty$ be a stationary time series process with $Z_t \in \mathcal{L}^q$ for some $q > 2$ and $\ex[Z_t] = 0$. Assume that $Z_t$ can be represented as $Z_t = g(\ldots, \eta_{t-1}, \eta_t)$, where $\eta_t$ are i.i.d.\ variables and $g: \reals^\infty \to \reals$ is a measurable function. We first state a Nagaev-type inequality from \cite{Wu2016}. 


\begin{definitionA}\label{defA-DAN} 
Let $q > 0$ and $\alpha > 0$. The dependence adjusted norm of the process $Z. = \{Z_t\}_{t=-\infty}^\infty$ is given by 
$\|Z.\|_{q, \alpha} = \sup_{t\geq 0} (t+1)^{\alpha} \sum_{s=t}^{\infty} \delta_{q}(g,s)$.
\end{definitionA}


\begin{propA}[\cite{Wu2016}, Theorem 2]\label{theo-wu2016}
Assume that $\|Z.\|_{q, \alpha} < \infty$ with $q > 2$ and $\alpha > 1/2 - 1/q$. Let $S_T = a_1 Z_1 + \ldots + a_T Z_T$, where $a_1,\ldots,a_T$ are real numbers with $\sum_{t=1}^T a_t^2 = T$. Then for any $w>0$,
\[ \pr(|S_T| \geq w) \leq C_1 \frac{|a|_{q}^{q}\|Z.\|^{q}_{q, \alpha}}{w^{q}} + C_2 \exp \left( - \frac{C_3 w^2} {T \|Z.\|^2_{2, \alpha}}\right), \]
where $C_1, C_2, C_3$ are constants that only depend on $q$ and $\alpha$.
\end{propA}


The following lemma is a simple consequence of the above inequality. 


\begin{lemmaA}\label{lemma-wlln}
Let $\sum_{s=t}^\infty \delta_{q}(g,s) = O(t^{-\alpha})$ for some $q > 2$ and $\alpha > 1/2 - 1/q$. Then 
\[ \frac{1}{\sqrt{T}} \sum_{t=1}^T Z_t = O_p(1). \]
\end{lemmaA} 


\begin{proof}[\textnormal{\textbf{Proof of Lemma \ref{lemma-wlln}.}}]
Let $\eta > 0$ be a fixed number. We apply Proposition \ref{theo-wu2016} to the sum $S_T = \sum_{t=1}^T a_t Z_t$ with $a_t = 1$ for all $t$. The assumption $\sum_{s=t}^\infty \delta_{q}(g,s) = O(t^{-\alpha})$ implies that $\|Z.\|_{2, \alpha} \le \|Z.\|_{q, \alpha} \le C_Z < \infty$. Hence, for $w$ chosen sufficiently large, we obtain that
\begin{align*}
\pr\Big( \Big| \sum_{t=1}^T Z_t \Big| \geq \sqrt{T} w\Big) 
 & \leq C_1 \frac{T C_Z^q}{T^{q/2} w^q} + C_2 \exp \left( - \frac{C_3 T w^2} {T C_Z^2}\right) \\
 & = \frac{\{C_1 C_Z^q\} T^{1-q/2}}{w^q} + C_2 \exp \left( - \frac{C_3 w^2} {C_Z^2}\right) \le \eta
\end{align*}
for all $T$. This means that $\sum_{t=1}^T Z_t / \sqrt{T} = O_p(1)$. 
\end{proof}


Let $\Delta \varepsilon_{it} = \varepsilon_{it} - \varepsilon_{it-1}$ and $\Delta \X_{it} = \X_{it} - \X_{it-1}$. By Assumptions \ref{C-err1} and \ref{C-reg1}, $\Delta \varepsilon_{it} = \Delta g_i(\mathcal{F}_{it})$ and $\Delta \X_{it} = \Delta \boldsymbol{h}_{i}(\mathcal{G}_{it})$. We further define
\begin{align*}
\boldsymbol{a}_{i}(\mathcal{H}_{it}) & := \Delta \boldsymbol{h}_{i}(\mathcal{G}_{it}) \Delta g_i(\mathcal{F}_{it})\phantom{^\top} = \Delta \X_{it} \Delta \varepsilon_{it} \\
\boldsymbol{b}_{i}(\mathcal{G}_{it}) & := \Delta \boldsymbol{h}_{i}(\mathcal{G}_{it}) \Delta \boldsymbol{h}_{i}(\mathcal{G}_{it})^\top = \Delta \X_{it} \Delta \X_{it}^\top,
\end{align*}
where $\boldsymbol{a}_i = (a_{ij})_{j=1}^d$, $\boldsymbol{b}_i = (b_{ikl})_{k,l=1}^d$ and $\mathcal{H}_{it} = (\mathcal{H}_{it,1},\ldots,\mathcal{H}_{it,d})^\top$ with $\mathcal{H}_{it,j} = (\ldots, \nu_{it-1,j},\linebreak \nu_{it,j})$ and $\nu_{it,j} = (\eta_{it,j},\xi_{it,j})$. The next result gives bounds on the physical dependence measures of the processes $\{ \boldsymbol{a}_{i}(\mathcal{H}_{it}) \}_{t=-\infty}^\infty$ and $\{ \boldsymbol{b}_{i}(\mathcal{G}_{it}) \}_{t=-\infty}^\infty$. 


\begin{lemmaA}\label{lemma-bounds-dep-measure}
Let Assumptions \ref{C-err1}, \ref{C-err3}, \ref{C-reg1} and \ref{C-reg3} be satisfied. Then for each $i$, $j$, $k$ and $l$, it holds that
\begin{align*}
 & \sum_{s=t}^\infty \delta_p(a_{ij}, s) = O(t^{-\alpha}) \, \qquad \text{for } p = \min\{q,q^\prime\}/2 \text{ and some } \alpha > 1/2 - 1/p \\
 & \sum_{s=t}^\infty \delta_p(b_{ikl}, s) = O(t^{-\alpha}) \qquad \text{for } p = q^\prime/2 \text{ and some } \alpha > 1/2 - 1/p.
\end{align*}
\end{lemmaA}


\begin{proof}[\textnormal{\textbf{Proof of Lemma \ref{lemma-bounds-dep-measure}.}}]
We only prove the first statement. The second one follows by analogous arguments. By the definition of the physical dependence measure and the Cauchy-Schwarz inequality, we have with $p = \min\{q,q^\prime\}/2$ that
\begin{align*}
\delta_p(a_{ij}, s) 
 & = \| a_{ij}(\mathcal{H}_{it,j}) - a_{ij}(\mathcal{H}_{it,j}^\prime) \| \\
 &= \| \Delta h_{ij}(\mathcal{G}_{it}) \Delta g_i(\mathcal{F}_{it}) -  \Delta h_{ij}(\mathcal{G}_{it}^\prime) \Delta g_i(\mathcal{F}_{it}^\prime) \| \\
 &\leq \| h_{ij}(\mathcal{G}_{it}) g_i(\mathcal{F}_{it}) - h_{ij}(\mathcal{G}_{it}^\prime) g_i(\mathcal{F}_{it}^\prime) \| \\
 &\quad + \| h_{ij}(\mathcal{G}_{it-1}) g_i(\mathcal{F}_{it-1}) - h_{ij}(\mathcal{G}_{it-1}^\prime) g_i(\mathcal{F}_{it-1}^\prime) \| \\
 &\quad + \| h_{ij}(\mathcal{G}_{it-1}) g_i(\mathcal{F}_{it}) - h_{ij}(\mathcal{G}_{it-1}^\prime) g_i(\mathcal{F}_{it}^\prime) \| \\
 &\quad + \| h_{ij}(\mathcal{G}_{it}) g_i(\mathcal{F}_{it-1}) - h_{ij}(\mathcal{G}_{it}^\prime) g_i(\mathcal{F}_{it-1}^\prime) \| \\
 & = \| \{ h_{ij}(\mathcal{G}_{it}) - h_{ij}(\mathcal{G}_{it}^\prime)\} g_i(\mathcal{F}_{it}) + h_{ij}(\mathcal{G}_{it}^\prime) \{g_i(\mathcal{F}_{it}) - g_i(\mathcal{F}_{it}^\prime)\} \| \\
 & \quad + \| \{ h_{ij}(\mathcal{G}_{it-1}) - h_{ij}(\mathcal{G}_{it-1}^\prime)\} g_i(\mathcal{F}_{it-1}) + h_{ij}(\mathcal{G}_{it-1}^\prime) \{g_i(\mathcal{F}_{it-1}) - g_i(\mathcal{F}_{it-1}^\prime)\} \| \\
 &\quad + \| \{ h_{ij}(\mathcal{G}_{it-1}) - h_{ij}(\mathcal{G}_{it-1}^\prime)\} g_i(\mathcal{F}_{it}) + h_{ij}(\mathcal{G}_{it-1}^\prime) \{g_i(\mathcal{F}_{it}) - g_i(\mathcal{F}_{it}^\prime)\} \| \\
 &\quad + \| \{ h_{ij}(\mathcal{G}_{it}) - h_{ij}(\mathcal{G}_{it}^\prime)\} g_i(\mathcal{F}_{it-1}) + h_{ij}(\mathcal{G}_{it}^\prime) \{g_i(\mathcal{F}_{it-1}) - g_i(\mathcal{F}_{it-1}^\prime)\} \| \\
 &\leq \delta_{2p}(h_{ij}, t) \| g_i(\mathcal{F}_t) \|_{2p} + \delta_{2p} (g_i, t) \| h_{ij}(\mathcal{G}_{it}^\prime) \|_{2p} \\
&\quad + \delta_{2p}(h_{ij}, t-1) \| g_i(\mathcal{F}_{t-1}) \|_{2p} + \delta_{2p} (g_i, t-1) \| h_{ij}(\mathcal{G}_{it-1}^\prime) \|_{2p} \\
&\quad + \delta_{2p}(h_{ij}, t-1) \| g_i(\mathcal{F}_t) \|_{2p} + \delta_{2p} (g_i, t) \| h_{ij}(\mathcal{G}_{it-1}^\prime) \|_{2p} \\
&\quad + \delta_{2p}(h_{ij}, t) \| g_i(\mathcal{F}_{t-1}) \|_{2p} + \delta_{2p} (g_i, t-1) \| h_{ij}(\mathcal{G}_{it}^\prime) \|_{2p}, 
\end{align*}
where $\mathcal{H}_{it,j}^\prime  = (\ldots, \nu_{i(-1),j}, \nu^\prime_{i0,j}, \nu_{i1,j}, \ldots, \nu_{it-1,j}, \nu_{it,j})$, $\mathcal{G}_{it,j}^\prime  = (\ldots, \xi_{i(-1),j}, \xi^\prime_{i0,j}, \xi_{i1,j}, \ldots, \linebreak \xi_{it-1,j}, \xi_{it,j})$ and $\mathcal{F}_{it}^\prime  = (\ldots, \eta_{i(-1)}, \eta^\prime_{i0}, \eta_{i1}, \ldots, \eta_{it-1}, \eta_{it})$ are coupled processes with $\nu_{i0,j}^\prime$, $\xi_{i0,j}^\prime$ and $\eta_{i0,j}^\prime$ being i.i.d.\ copies of $\nu_{i0,j}$, $\xi_{i0,j}$ and $\eta_{i0}$. From this and Assumptions \ref{C-err1}, \ref{C-err3}, \ref{C-reg1} and \ref{C-reg3}, it immediately follows that $\sum_{s=t}^\infty \delta_p(a_{ij}, s) = O(t^{-\alpha})$.
\end{proof}


We now show that the estimator $\widehat{\bfbeta}_i$ is $\sqrt{T}$-consistent for each $i$ under our conditions.


\begin{lemmaA}\label{lemma-beta-rate}
Let Assumptions \ref{C-err1}, \ref{C-err3} and \ref{C-reg1}--\ref{C-reg-err} be satisfied. Then for each $i$, it holds that
\[ \widehat{\bfbeta}_i - \bfbeta_i = O_p\Big(\frac{1}{\sqrt{T}}\Big). \]
\end{lemmaA}


\begin{proof}[\textnormal{\textbf{Proof of Lemma \ref{lemma-beta-rate}.}}]
The estimator $\widehat{\bfbeta}_i$ can be written as
\begin{align*}
\widehat{\bfbeta}_i &= \Big( \sum_{t=2}^T \Delta \X_{it} \Delta \X_{it}^\top \Big)^{-1} \sum_{t=2}^T \Delta \X_{it} \Delta Y_{it} \\
& =  \Big( \sum_{t=2}^T \Delta \X_{it} \Delta \X_{it}^\top \Big)^{-1} \sum_{t=2}^T \Delta \X_{it} \bigg(\Delta \X_{it}^\top \bfbeta_i +  \Delta m_{it}+ \Delta \varepsilon_{it} \bigg) \\
&= \bfbeta_i + \Big( \sum_{t=2}^T \Delta \X_{it} \Delta \X_{it}^\top \Big)^{-1} \sum_{t=2}^T \Delta \X_{it} \Delta m_{it} +  \Big( \sum_{t=2}^T \Delta \X_{it} \Delta \X_{it}^\top \Big)^{-1} \sum_{t=2}^T \Delta \X_{it} \Delta \varepsilon_{it}, 
\end{align*}
where $\Delta \X_{it} = \X_{it} - \X_{it-1}$, $\Delta \varepsilon_{it} = \varepsilon_{it} - \varepsilon_{it-1}$ and $\Delta m_{it} = m_i (\frac{t}{T}) - m_i(\frac{t-1}{T})$. Hence, 
\begin{align}
 \sqrt{T}( \widehat{\bfbeta}_i - \bfbeta_i) = &\Big( \frac{1}{T}\sum_{t=2}^T \Delta \X_{it} \Delta \X_{it}^\top \Big)^{-1} \frac{1}{\sqrt{T}}\sum_{t=2}^T \Delta \X_{it} \Delta m_{it} \nonumber \\
&\quad+  \Big(\frac{1}{T} \sum_{t=2}^T \Delta \X_{it} \Delta \X_{it}^\top \Big)^{-1}\frac{1}{\sqrt{T}} \sum_{t=2}^T \Delta \X_{it} \Delta \varepsilon_{it}. \label{theo:beta:proof1}
\end{align}
In what follows, we show that 
\begin{align}
\frac{1}{\sqrt{T}} \sum_{t=2}^T \Delta \X_{it} \Delta \varepsilon_{it} & = O_p(1) \label{lemma-beta-rate-claim1} \\
\Big( \frac{1}{T}\sum_{t=2}^T \Delta \X_{it} \Delta \X_{it}^\top \Big)^{-1} & = O_p(1) \label{lemma-beta-rate-claim2} \\
\frac{1}{\sqrt{T}} \sum_{t=2}^T \Delta \X_{it} \Delta m_{it} & = O_p\Big(\frac{1}{\sqrt{T}}\Big). \label{lemma-beta-rate-claim3}
\end{align} 
Lemma \ref{lemma-beta-rate} follows from applying these three statements together with standard arguments to formula \eqref{theo:beta:proof1}.


Since $\ex[\Delta \X_{it} \Delta \varepsilon_{it}] = 0$ by \ref{C-reg-err} and $\sum_{s=t}^\infty \delta_p(a_{ij}, s) = O(t^{-\alpha})$ for some $p > 2$ and all $j$ by Lemma \ref{lemma-bounds-dep-measure}, the claim \eqref{lemma-beta-rate-claim1} follows upon applying Lemma \ref{lemma-wlln}. Another application of Lemma \ref{lemma-wlln} yields that 
\[ \frac{1}{T}\sum_{t=2}^T \Big\{ \Delta \X_{it} \Delta \X_{it}^\top - \ex[\Delta \X_{it} \Delta \X_{it}^\top] \Big\} = O_p\Big(\frac{1}{\sqrt{T}}\Big). \]
As $\ex[\Delta \X_{it} \Delta \X_{it}^\top]$ is invertible, we can invoke Slutsky's lemma to obtain \eqref{lemma-beta-rate-claim2}. By assumption, $m_i$ is Lipschitz continuous, which implies that $|\Delta m_{it}| = |m_i (\frac{t}{T}) - m_i (\frac{t-1}{T}) | \leq C/T$ for all $t \in \{1, \ldots, T\}$ and some constant $C > 0$. Hence, 
\begin{align*}
\Big| \frac{1}{\sqrt{T}}\sum_{t=2}^T \Delta X_{it,j} \Delta m_{it}\Big| &\leq \frac{1}{\sqrt{T}}\sum_{t=2}^T \big|\Delta X_{it,j} \big| \cdot \big| \Delta m_{it} \big| \\
	& \leq \frac{C}{\sqrt{T}} \cdot \frac{1}{T} \sum_{t=2}^T \left|\Delta X_{it,j} \right| = O_p\Big(\frac{1}{\sqrt{T}}\Big),
\end{align*}
where we have used that $T^{-1} \sum_{t=2}^T |\Delta \X_{it,j}| = O_p(1)$ by Markov's inequality. This yields \eqref{lemma-beta-rate-claim3}.
\end{proof}


%\begin{propA}[\cite{Wu2007}, Corollary 2]\label{theo-wu-2}
%Let $\{\epsilon_t\}_{t\in\integers}$ be i.i.d.\ random variables, $\xi_t = (\ldots, \epsilon_{t-1}, \epsilon_t)$ and $g(\cdot)$ a measurable function such that $g(\xi_t) \in \mathcal{L}^q$ for some $q > 2$ and $\ex[g(\xi_t)] = 0$. Moreover, let $\mathit{l}$ be a positive, nondecreasing function which is slowly varying, that is, $\lim_{x \to \infty} \mathit{l}(\lambda x) / \mathit{l}(x) = 1$ for any $\lambda > 0$. Assume that 
%\[ \sum_{k = m}^\infty \Big\| \ex [g(\xi_k)| \xi_0] - \ex [g(\xi_k)| \xi_{-1}]\Big\|_q =O\Big([\log m]^{-\beta}\Big) \] 
%with $0 \leq \beta < 1/q$ and 
%$\sum_{k=1}^\infty \{ k^{-\beta q} / [l(2^k)]^q \} < \infty$.	
%Then 
%\[ S_m = g(\xi_1) + \ldots + g(\xi_m) = o_{a.s.}\big(\sqrt{m}l(m)\big). \]
%\end{propA}


%\begin{propA}[\cite{Wu2007}, Proposition 3]\label{prop-wu}
%Let $\{\epsilon_t\}_{t\in\integers}$ be i.i.d.\ random variables, $\xi_t = (\ldots, \epsilon_{t-1}, \epsilon_t)$ and $g(\cdot)$ a measurable function such that $g(\xi_t) \in \mathcal{L}^q$ for some $q > 1$ and $\ex[g(\xi_t)] = 0$. For $k \geq 0$, let $\xi_k^\prime = (\ldots, \epsilon_{-1}, \epsilon_0^\prime, \epsilon_1, \ldots, \epsilon_{k-1}, \epsilon_k)$, where $\epsilon_0^\prime$ is an i.i.d.\ copy of $\epsilon_0$. For any $t \geq 1$, it holds that
%\[ \big\| \ex [g(\xi_t)| \xi_0] - \ex [g(\xi_t)| \xi_{-1}]\big\|_q \leq 2 \big\| g(\xi_t) - g(\xi_t^\prime)\big\|_q. \]
%\end{propA}


\begin{lemmaA}\label{lemmaA:lrv}
Let $s_T \asymp T^{1/3}$. Under Assumptions \ref{C-err1}--\ref{C-reg-err}, for each $i \in \{1, \ldots, n\}$ we have
$$\widehat{\sigma}_i^2 = \sigma_i^2 + O_p(T^{-1/3}).$$
where $\widehat{\sigma}_i^2$ is the subseries variance estimate of $\sigma_i^2$ introduced by \eqref{eq:lrv}.
\end{lemmaA}


\begin{proof}[\textnormal{\textbf{Proof of Lemma \ref{lemmaA:lrv}}}]
\textcolor{blue}{(Proof not double-checked yet.)}
For notational convenience, we let $Y_{it}^* = Y_{it} - \bfbeta_i^\top \X_{it}$. Note that 
\begin{align*}
&Y_{i(t + ms_T)}^* - Y_{i(t + (m-1)s_T)}^* \\
&= \alpha_i + m_i\left(\frac{t+m s_T}{T}\right) + \varepsilon_{i(t + ms_T)} \\
&\quad-  \alpha_i - m_i\left(\frac{t+(m-1) s_T}{T}\right) + \varepsilon_{i(t + (m-1)s_T)} \\
&=m_i\left(\frac{t+m s_T}{T}\right) + \varepsilon_{i(t + ms_T)}  - m_i\left(\frac{t+(m-1) s_T}{T}\right) + \varepsilon_{i(t + (m-1)s_T)}\\
& = Y_{i(t + ms_T)}^\circ - Y_{i(t + (m-1)s_T)}^\circ, 
\end{align*}
where $Y_{it}^\circ$ is the dependent variable in a well-studied standard nonparametric regression discussed in Section \ref{subsec:test:prep}.
Now, using simple arithmetic calculations, we can rewrite $\widehat{\sigma}_i^2$ as
\begin{align}
\widehat{\sigma}_i^2  &= \frac{1}{2(M-1)s_T}\sum_{m=1}^M \left[\sum_{t = 1}^{s_T} \left(Y_{i(t + ms_T)}^\circ - Y_{i(t + (m-1)s_T)}^\circ\right)\right]^2 \nonumber\\
&\quad + \frac{1}{2(M-1)s_T}\sum_{m=1}^M \left[\sum_{t = 1}^{s_T} (\widehat{\bfbeta}_i - \bfbeta_i)^\top\left(\X_{i(t+ms_T)} - \X_{i(t+(m-1)s_T)}\right) \right]^2 \nonumber \\
 &\quad - \frac{1}{(M-1)s_T}\sum_{m=1}^M \bigg[\sum_{t = 1}^{s_T}  \left(Y_{i(t + ms_T)}^\circ - Y_{i(t + (m-1)s_T)}^\circ\right) \nonumber \\ & \phantom{\quad - \frac{1}{(M-1)s_T}\sum_{m=1}^M \bigg[} \times \sum_{t = 1}^{s_T} (\widehat{\bfbeta}_i - \bfbeta_i)^\top\left(\X_{i(t+ms_T)} - \X_{i(t+(m-1)s_T)}\right) \bigg]. \label{eqA:lrv:proof1}
\end{align}
By \cite{Carlstein1986} and \cite{WuZhao2007}, we have
\begin{align}\label{eqA:lrv:proof2}
\frac{1}{2(M-1)s_T}\sum_{m=1}^M \left[\sum_{t = 1}^{s_T} \Big(Y_{i(t + ms_T)}^\circ - Y_{i(t + (m-1)s_T)}^\circ\Big)\right]^2  = \sigma_i^2 + O_p(T^{-1/3}).
\end{align}
Furthermore, by our assumption that $s_T \asymp T^{1/3}$, Assumption \ref{C-reg2} and Lemma \ref{lemma-beta-rate}, we have
\begin{align}\label{eqA:lrv:proof3}
\frac{1}{2(M-1)s_T}\sum_{m=1}^M \left[\sum_{t = 1}^{s_T} (\widehat{\bfbeta}_i - \bfbeta_i)^\top\Big(\X_{i(t+ms_T)} - \X_{i(t+(m-1)s_T)}\Big) \right]^2 = O_p(T^{-2/3}).
\end{align}
Finally, applying \eqref{eqA:lrv:proof2} and \eqref{eqA:lrv:proof3} together with the Cauchy-Schwarz inequality, we obtain
\begin{align}\label{eqA:lrv:proof4}
\frac{1}{(M-1)s_T}\sum_{m=1}^M \bigg[ & \sum_{t = 1}^{s_T}  \left(Y_{i(t + ms_T)}^\circ - Y_{i(t + (m-1)s_T)}^\circ\right) \nonumber \\
& \times \sum_{t = 1}^{s_T} (\widehat{\bfbeta}_i - \bfbeta_i)^\top\left(\X_{i(t+ms_T)} - \X_{i(t+(m-1)s_T)}\right) \bigg] = O_p(T^{-1/3}).
\end{align}


Applying \eqref{eqA:lrv:proof2}--\eqref{eqA:lrv:proof4} to \eqref{eqA:lrv:proof1}, the lemma trivially follows.
\end{proof}



\subsection*{Proof of Theorem \ref{theo:stat:global}}\label{subsec-appendix-stat-equality}


We first summarize the main proof strategy, which splits up into five steps, and then fill in the details. In particular, we defer the proofs of some intermediate results to the end of the section. 


\subsubsection*{Step 1}


To start with, we consider a simplified setting where the parameter vectors $\bfbeta_i$ are known. In this case, we can replace the estimators $\widehat{\bfbeta}_i$ in the definition of the statistic $\widehat{\Phi}_{n,T}$ by the true vectors $\bfbeta_i$ themselves. This leads to the simpler statistic 
\[ \doublehattwo{\Phi}_{n,T} = \max_{1 \le i < j \le n} \max_{(u,h) \in \mathcal{G}_T}\Bigg\{ \bigg| \frac{\doublehat{\phi}_{ij,T}(u,h)} {\{ \doublehattwo{\sigma}_i^2 + \doublehattwo{\sigma}_j^2 \}^{1/2}} \bigg| - \lambda(h)\Bigg\}, \]
where
\[ \doublehat{\phi}_{ij,T}(u,h) = \sum_{t=1}^T w_{t,T}(u,h) \big\{ (\varepsilon_{it} - \bar{\varepsilon}_i) - (\varepsilon_{jt} - \bar{\varepsilon}_j)  \big\} \]
and $\doublehattwo{\sigma}_i^2$ is computed in exactly the same way as $\widehat{\sigma}_i^2$ except that all occurrences of $\widehat{\bfbeta}_i$ are replaced by $\bfbeta_i$. By assumption, $\widehat{\sigma}_i^2 = \sigma^2_i + o_p(\rho_T)$ with $\rho_T = o(\sqrt{h_{\min}}/\log T)$. \textcolor{blue}{(Double-check which rate is needed for $\widehat{\sigma}_i^2$.)} For most estimators of $\sigma_i^2$ including those discussed in Section \ref{sec:theo}, this assumption immediately implies that $\doublehattwo{\sigma}_i^2 = \sigma^2_i + o_p(\rho_T)$ as well. In the sequel, we thus take for granted that the estimator $\doublehattwo{\sigma}_i^2$ has this property.


We now have a closer look at the statistic $\doublehattwo{\Phi}_{n,T}$. We in particular show that there exists an identically distributed version $\widetilde{\Phi}_{n, T}$ of $\doublehattwo{\Phi}_{n, T}$ which is close to the Gaussian statistic $\Phi_{n, T}$ from \eqref{eq:Phi}. More formally, we prove the following result.  
\begin{propA}\label{propA:strong_approx}
There exist statistics $\{ \widetilde{\Phi}_{n,T}: T =1,2,\ldots \}$ with the following two properties: (i) $\widetilde{\Phi}_{n, T}$ has the same distribution as $\doublehattwo{\Phi}_{n, T}$ for any $T$, and (ii)
\begin{equation*}
\big| \widetilde{\Phi}_{n, T} - \Phi_{n,T} \big| = o_p(\delta_T),
\end{equation*}
where $\delta_T = T^{1/q} / \sqrt{T h_{\min}} + \rho_T \sqrt{\log T}$ and $\Phi_{n,T}$ is a Gaussian statistic as defined in \eqref{eq:Phi}. 
\end{propA}
The proof makes heavy use of strong approximation theory for dependent processes. As it is quite technical, it is postponed to the end of this section. 


\subsubsection*{Step 2}


In this step, we establish some properties of the Gaussian statistic $\Phi_{n,T}$. Specifically, we prove the following result.  
\begin{propA}\label{propA:anticon}
It holds that 
\begin{equation*}
\sup_{x \in \reals} \pr \big( | \Phi_{n,T} - x | \le \delta_T \big) = o(1),
\end{equation*}
where $\delta_T = T^{1/q} / \sqrt{T h_{\min}} + \rho_T \sqrt{\log T}$.
\end{propA}
Roughly speaking, this proposition says that the random variable $\Phi_{n,T}$ does not concentrate too strongly in small regions of the form $[x-\delta_T,x+\delta_T]$ with $\delta_T$ converging to $0$. The main technical tool for deriving it are anti-concentration bounds for Gaussian random vectors. The details are provided below. 


\subsubsection*{Step 3}


We now use Steps 1 and 2 to prove that 
\begin{equation}\label{eq:claim-step3}
\sup_{x \in \reals} \big| \pr(\doublehattwo{\Phi}_{n, T} \le x) - \pr(\Phi_{n,T} \le x) \big| = o(1). 
\end{equation}
\begin{proof}[\textnormal{\textbf{Proof of (\ref{eq:claim-step3}).}}]
It holds that
\begin{align*}
 & \sup_{x \in \reals} \Big| \pr(\doublehattwo{\Phi}_{n, T} \le x) - \pr(\Phi_{n,T} \le x) \Big| \\
 & = \sup_{x \in \reals} \Big| \pr(\widetilde{\Phi}_{n, T} \le x) - \pr(\Phi_{n,T} \le x) \Big| \\
 & = \sup_{x \in \reals} \Big| \ex \Big[ \ind(\widetilde{\Phi}_{n, T} \le x) - \ind (\Phi_{n,T} \le x) \Big] \Big| \\
 & \le \sup_{x \in \reals} \Big| \ex \Big[ \big\{ \ind(\widetilde{\Phi}_{n, T} \le x) - \ind (\Phi_{n,T} \le x) \big\} \ind \big( |\widetilde{\Phi}_{n, T} - \Phi_{n,T}| \le \delta_T \big) \Big] \Big| \\
 & \quad + \ex \Big[ \ind \big( |\widetilde{\Phi}_{n, T} - \Phi_{n,T}| > \delta_T \big) \Big].
\end{align*} 
Moreover, since  
\[ \ex \Big[ \ind \big( |\widetilde{\Phi}_{n, T} - \Phi_{n,T}| > \delta_T \big) \Big] = \pr  \big( |\widetilde{\Phi}_{n, T} - \Phi_{n,T}| > \delta_T \big) = o(1) \]
by Step 1 and 
\begin{align*}
 & \sup_{x \in \reals} \Big| \ex \Big[ \big\{ \ind(\widetilde{\Phi}_{n, T} \le x) - \ind (\Phi_{n,T} \le x) \big\} \ind \big( |\widetilde{\Phi}_{n, T} - \Phi_{n,T}| \le \delta_T \big) \Big] \Big| \\
 & \le \sup_{x \in \reals} \ex \Big[ \ind \big( |\Phi_{n, T} - x| \le \delta_T, |\widetilde{\Phi}_{n, T} - \Phi_{n,T}| \le \delta_T \big) \Big] \\
 & \le \sup_{x \in \reals} \pr \big( |\Phi_{n, T} - x| \le \delta_T \big) = o(1)
\end{align*}
by Step 2, we arrive at \eqref{eq:claim-step3}.
\end{proof}


\subsubsection*{Step 4}


In this step, we show that the auxiliary statistic $\doublehattwo{\Phi}_{n,T}$ is close to $\widehat{\Phi}_{n,T}$ in the following sense.
\begin{propA}\label{propA:step4}
It holds that 
\[ \doublehattwo{\Phi}_{n,T} - \widehat{\Phi}_{n,T} = o_p(\delta_T) \]
with $\delta_T = T^{1/q}/\sqrt{T h_{\min}} + \rho_T \sqrt{\log T}$.
%Let $\gamma_{T} = C \delta_T$ with $\delta_T = T^{1/q}/\sqrt{T h_{\min}} + \rho_T \sqrt{\log T}$ and an arbitrary constant $C > 0$. Then 
%\[ \pr \Big(\big|\doublehattwo{\Phi}_{n,T} - \widehat{\Phi}_{n,T}\big| > \gamma_{T} \Big) = o(1). \]
%that is, $\doublehattwo{\Phi}_{n,T} - \widehat{\Phi}_{n,T} = o_p(\delta_T)$.
\end{propA}
The proof can be found at the end of this section. 
%The next lemma will turn out to be useful in the next step of the proof. 
%\begin{lemmaA}\label{lemmaA:step1}
%For any $x \in \reals$ and any $\gamma > 0$, 
%\begin{align*}
%\pr\Big( \doublehattwo{\Phi}_{n,T} \le x & - \gamma\Big) - \pr \Big(\big|\doublehattwo{\Phi}_{n,T} - \widehat{\Phi}_{n,T}\big| > \gamma \Big) \\
% & \le \pr\big(\widehat{\Phi}_{n, T} \le x\big) \le\pr\Big(\doublehattwo{\Phi}_{n,T} \le x + \gamma\Big) + \pr \Big(\big|\doublehattwo{\Phi}_{n,T} - \widehat{\Phi}_{n,T}\big| > \gamma \Big).
%\end{align*}
%\end{lemmaA}
%Combining this lemma with Proposition \ref{propA:step4} yields that 
%\begin{equation}\label{eq:prop-lemma-step1}
%\pr\Big( \doublehattwo{\Phi}_{n,T} \le x - \gamma_T\Big) - o(1) \le \pr(\widehat{\Phi}_{n, T} \le x) \le \pr\Big(\doublehattwo{\Phi}_{n,T} \le x + \gamma_T\Big) + o(1),
%\end{equation}
%which relates the distribution functions of $\doublehattwo{\Phi}_{n,T}$ and $\widehat{\Phi}_{n,T}$ to each other. The proofs of Proposition \ref{propA:step4} and Lemma \ref{lemmaA:step1} can be found at the end of this section. 


\subsubsection*{Step 5} 


We finally show that 
\begin{equation}\label{eq:claim:step5}
\sup_{x \in \reals} \big| \pr(\widehat{\Phi}_{n, T} \le x) - \pr(\Phi_{n,T} \le x) \big| = o(1).
\end{equation}
\begin{proof}[\textnormal{\textbf{Proof of (\ref{eq:claim:step5}).}}]
To start with, we verify that for any $x \in \reals$ and any $\delta > 0$, 
\begin{align}
\pr\Big( \doublehattwo{\Phi}_{n,T} \le x & - \delta\Big) - \pr \Big(\big|\doublehattwo{\Phi}_{n,T} - \widehat{\Phi}_{n,T}\big| > \delta \Big) \nonumber \\
 & \le \pr\big(\widehat{\Phi}_{n, T} \le x\big) \le\pr\Big(\doublehattwo{\Phi}_{n,T} \le x + \delta\Big) + \pr \Big(\big|\doublehattwo{\Phi}_{n,T} - \widehat{\Phi}_{n,T}\big| > \delta \Big). \label{eq:lemmaA-step1}
\end{align}
It holds that
\begin{align*} 
\pr(\widehat{\Phi}_{n, T} \le x) &= \pr \Big(\widehat{\Phi}_{n, T} \le x, \big|\doublehattwo{\Phi}_{n,T} - \widehat{\Phi}_{n,T}\big| \le \delta \Big) + \pr \Big(\widehat{\Phi}_{n, T} \le x, \big|\doublehattwo{\Phi}_{n,T} - \widehat{\Phi}_{n,T}\big| > \delta \Big) \\
& \le  \pr \Big(\widehat{\Phi}_{n, T} \le x, \widehat{\Phi}_{n,T} - \delta \le \doublehattwo{\Phi}_{n,T} \le \widehat{\Phi}_{n,T} + \delta \Big) + \pr \Big(\big|\doublehattwo{\Phi}_{n,T} - \widehat{\Phi}_{n,T}\big| > \delta \Big) \\
& \le  \pr \Big(\doublehattwo{\Phi}_{n,T} \le x + \delta \Big) + \pr \Big(\big|\doublehattwo{\Phi}_{n,T} - \widehat{\Phi}_{n,T}\big| > \delta \Big)
\end{align*}
and analogously 
\begin{align*} \pr(\doublehattwo{\Phi}_{n, T} \le x - \delta)  \le  \pr \Big(\widehat{\Phi}_{n,T} \le x \Big) + \pr \Big(\big|\doublehattwo{\Phi}_{n,T} - \widehat{\Phi}_{n,T}\big| > \delta \Big).
\end{align*}
Combining these two inequalities, we arrive at \eqref{eq:lemmaA-step1}.


Now let $x\in \reals$ be any point such that $\pr(\widehat{\Phi}_{n,T} \le x) \geq \pr(\Phi_{n, T} \le x)$. With the help of \eqref{eq:lemmaA-step1}, we get that
\begin{align*}
\Big| \pr\big(\widehat{\Phi}_{n, T} \le x\big) - \pr\big(\Phi_{n,T} \le x\big) \Big| 
 & = \pr\big(\widehat{\Phi}_{n, T} \le x\big) - \pr\big(\Phi_{n,T} \le x\big) \\ 
 & \le \pr\Big(\doublehattwo{\Phi}_{n,T} \le x + \delta_{T}\Big) + \pr \Big(\big|\doublehattwo{\Phi}_{n,T} - \widehat{\Phi}_{n,T}\big| > \delta_{T} \Big) \\ 
 & \quad - \pr\big(\Phi_{n,T} \le x\big)  \\
 & = \pr\Big(\doublehattwo{\Phi}_{n,T} \le x + \delta_{T}\Big) - \pr\Big(\Phi_{n,T} \le x + \delta_{T}\Big)  \\
 & \quad +  \pr\big(\Phi_{n,T} \le x + \delta_{T}\big)   - \pr\big(\Phi_{n,T} \le x\big) \\ 
 & \quad + \pr \Big(\big|\doublehattwo{\Phi}_{n,T} - \widehat{\Phi}_{n,T}\big| > \delta_{T} \Big). 
\end{align*}
Analogously, for any point $x\in \reals$ with $\pr(\widehat{\Phi}_{n,T}\le x) < \pr(\Phi_{n, T}\le x)$, it holds that 
\begin{align*}
\Big| \pr\big(\widehat{\Phi}_{n, T} \le x\big) - \pr\big(\Phi_{n,T} \le x\big) \Big| 
 & \le \pr\Big( \Phi_{n,T} \le x - \delta_T \Big) - \pr\Big(\doublehattwo{\Phi}_{n,T} \le x - \delta_{T}\Big) \\
 & \quad + \pr\big( \Phi_{n,T} \le x \big) - \pr\big( \Phi_{n,T} \le x - \delta_T \big) \\
 & \quad + \pr \Big(\big|\doublehattwo{\Phi}_{n,T} - \widehat{\Phi}_{n,T}\big| > \delta_{T} \Big).  
\end{align*}
Consequently,  
\begin{align*}
\sup_{x \in \reals} \Big| \pr\big(\widehat{\Phi}_{n, T} \le x\big) - \pr\big(\Phi_{n,T} \le x\big) \Big| 
 & \le \sup_{x \in \reals} \Big| \pr\big(\doublehattwo{\Phi}_{n, T} \le x\big) - \pr\big(\Phi_{n,T} \le x\big) \Big| \\
 & \quad + \sup_{x \in \reals} \pr\big( | \Phi_{n,T} - x | \le \delta_T \big) \\
 & \quad + \pr \Big(\big|\doublehattwo{\Phi}_{n,T} - \widehat{\Phi}_{n,T}\big| > \delta_{T} \Big). 
\end{align*}
Since the three terms on the right-hand side are all $o(1)$ by Steps 2--4, we arrive at \eqref{eq:claim:step5}. 
\end{proof}


\subsubsection*{Details on Steps 1--5}


\begin{proof}[\textnormal{\textbf{Proof of Proposition \ref{propA:strong_approx}}}] 
Consider the stationary process $\mathcal{E}_i = \{\varepsilon_{it}: 1 \leq t \leq T\}$ for some fixed $i \in \{1,\ldots,n\}$. By Theorem 2.1 and Corollary 2.1 in \cite{BerkesLiuWu2014}, the following strong approximation result holds true: On a richer probability space, there exist a standard Brownian motion $\mathbb{B}_i$ and a sequence $\{ \widetilde{\varepsilon}_{it}: t \in \naturals \}$ such that $[\widetilde{\varepsilon}_{i1},\ldots,\widetilde{\varepsilon}_{iT}] \stackrel{\mathcal{D}}{=} [\varepsilon_{i1},\ldots,\varepsilon_{iT}]$ for each $T$ and 
\begin{equation}\label{eq-strongapprox-dep}
\max_{1 \le t \le T} \Big| \sum\limits_{s=1}^t \widetilde{\varepsilon}_{is} - \sigma_i \mathbb{B}_i(t) \Big| = o\big( T^{1/q} \big) \quad \text{a.s.},  
\end{equation}
where $\sigma^2_i = \sum_{k \in \integers} \cov(\varepsilon_{i0}, \varepsilon_{ik})$ denotes the long-run error variance. We apply this result separately for each $i \in \{1,\ldots,n\}$. Since the error processes $\mathcal{E}_i = \{\varepsilon_{it}: 1 \leq t \leq T\}$ are independent across $i$, we can construct the processes $\widetilde{\mathcal{E}}_i = \{\widetilde{\varepsilon}_{it}: t\in \naturals\}$ in such a way that they are independent across $i$ as well. 


We now define the statistic $\widetilde{\Phi}_{n,T}$ in the same way as $\doublehattwo{\Phi}_{n, T}$ except that the error processes $\mathcal{E}_i$ are replaced by $\widetilde{\mathcal{E}}_i$. Specifically, we set
\[ \widetilde{\Phi}_{n,T} = \max_{1 \le i < j \le n}\max_{(u,h) \in \mathcal{G}_T} \Bigg\{ \bigg|\frac{\widetilde{\phi}_{ij, T}(u,h)}{\big(\widetilde{\sigma}_i^2 + \widetilde{\sigma}_j^2 \big)^{1/2}} \bigg| - \lambda(h)\Bigg\}, \]
where
\[ \widetilde{\phi}_{ij, T}(u,h) = \sum\limits_{t=1}^T w_{t,T}(u,h) \big\{ (\widetilde{\varepsilon}_{it} - \bar{\widetilde{\varepsilon}}_i)  - (\widetilde{\varepsilon}_{jt} - \bar{\widetilde{\varepsilon}}_j)\big\} \]
and the estimator $\widetilde{\sigma}^2_i$ is constructed from the sample $\widetilde{\mathcal{E}}_i$ in the same way as $\doublehattwo{\sigma}^2_i$ is constructed from $\mathcal{E}_i$. Since $[\widetilde{\varepsilon}_{i1},\ldots,\widetilde{\varepsilon}_{iT}] \stackrel{\mathcal{D}}{=} [\varepsilon_{i1},\ldots,\varepsilon_{iT}]$ and $\doublehattwo{\sigma}_i^2 = \sigma_i^2 + o_p(\rho_T)$, we have that $\widetilde{\sigma}_i^2 = \sigma_i^2 + o_p(\rho_T)$ as well. In addition to $\widetilde{\Phi}_{n,T}$, we introduce the Gaussian statistic
\[ \Phi_{n, T} = \max_{1\leq i< j \leq n}\max_{(u,h) \in \mathcal{G}_T} \bigg\{ \bigg|\frac{\phi_{ij, T}(u,h)}{\big(\sigma_i^2 + \sigma_j^2 \big)^{1/2}}\bigg| - \lambda(h) \bigg\} \]
and the auxiliary statistic 
\[ \Phi_{n, T}^{\diamond} = \max_{1\leq i<j \leq n}\max_{(u,h) \in \mathcal{G}_T} \bigg\{ \bigg|\frac{\phi_{ij, T}(u,h)}{\big(\widetilde{\sigma}_i^2 + \widetilde{\sigma}_j^2 \big)^{1/2}}\bigg| - \lambda(h) \bigg\}, \]
where $\phi_{ij,T}(u,h) = \sum\nolimits_{t=1}^T w_{t,T}(u,h) \{ \sigma_i (Z_{it} - \bar{Z}_i) - \sigma_j (Z_{jt} - \bar{Z}_j) \}$ and the Gaussian variables $Z_{it}$ are chosen as $Z_{it} = \mathbb{B}_i(t) - \mathbb{B}_i(t-1)$. With this notation, we obtain the obvious bound 
\begin{equation*}
\big| \widetilde{\Phi}_{n, T} - \Phi_{n, T} \big| \le \big| \widetilde{\Phi}_{n, T} - \Phi_{n, T}^{\diamond} \big| + \big| \Phi_{n, T}^{\diamond} - \Phi_{n, T} \big|. 
\end{equation*}
In what follows, we prove that 
\begin{align}
\big| \widetilde{\Phi}_{n, T} - \Phi_{n, T}^{\diamond} \big| & = o_p\Big( \frac{T^{1/q}}{\sqrt{Th_{\min}}} \Big) \label{eq-strongapprox-bound-A} \\
\big| \Phi_{n, T}^{\diamond} - \Phi_{n, T} \big| & = o_p(\rho_T \sqrt{\log T}), \label{eq-strongapprox-bound-B}
\end{align}
which completes the proof. 


First consider $|\widetilde{\Phi}_{n, T} - \Phi_{n, T}^{\diamond}|$. Straightforward calculations yield that 
\begin{align}
\big| \widetilde{\Phi}_{n, T} - \Phi_{n, T}^{\diamond} \big| 
 & \le \max_{1\le i < j \le n} \big(\widetilde{\sigma}_i^2 + \widetilde{\sigma}_j^2 \big)^{-1/2} \max_{1\le i < j \le n} \max_{(u,h) \in \mathcal{G}_T} \big| \widetilde{\phi}_{ij, T}(u,h) - \phi_{ij, T}(u,h) \big|\Big\}  \nonumber \\ 
 & = O_p(1) \cdot \max_{1\le i < j \le n} \max_{(u,h) \in \mathcal{G}_T} \big| \widetilde{\phi}_{ij, T}(u,h) - \phi_{ij, T}(u,h) \big|, \label{eqA:strong_approx:bound2}
\end{align}
where the last line follows from the fact that $\widetilde{\sigma}_i^2 = \sigma_i^2 + o_p(\rho_T)$. Using summation by parts (that is, $\sum_{t=1}^T a_t b_t = \sum_{t=1}^{T-1} A_t (b_t - b_{t+1}) + A_T b_T$ with $A_t = \sum_{s=1}^t a_s$), we further obtain that 
\begin{align*}
\big| & \widetilde{\phi}_{ij, T}(u,h) - \phi_{ij, T}(u,h) \big|  \\
 & =\bigg|\sum_{t=1}^T w_{t,T}(u,h) \big\{ (\widetilde{\varepsilon}_{it} - \bar{\widetilde{\varepsilon}}_i) - (\widetilde{\varepsilon}_{jt} - \bar{\widetilde{\varepsilon}}_j) -{\sigma}_i (Z_{it} - \bar{Z}_i) + {\sigma}_j (Z_{jt} - \bar{Z}_j) \big\}\bigg|  \\
 & =\Big|\sum_{t=1}^{T-1} A_{ij, t} \big(w_{t,T}(u,h) -w_{t+1,T}(u,h)\big) + A_{ij, T} w_{T,T}(u,h)\Big|,
\end{align*}
where 
\begin{align*}
A_{ij, t} = \sum_{s=1}^t \big\{ (\widetilde{\varepsilon}_{is} - \bar{\widetilde{\varepsilon}}_i)  - (\widetilde{\varepsilon}_{js} - \bar{\widetilde{\varepsilon}}_j) -{\sigma}_i (Z_{is} - \bar{Z}_i) + {\sigma}_j (Z_{js} - \bar{Z}_j) \big\}
\end{align*}
and $A_{ij, T} = 0$ for all pairs $(i, j)$ by construction. From this, it follows that 
\begin{equation}\label{eq-strongapprox-bound3}
\big| \widetilde{\phi}_{ij, T}(u,h) - \phi_{ij, T}(u,h) \big| \le W_T(u, h) \max_{1 \le t \le T} |A_{ij, t}|
\end{equation}
with $W_T(u,h) = \sum_{t=1}^{T-1} |w_{t+1,T}(u,h) - w_{t,T}(u,h)|$. Straightforward calculations yield that
\begin{align*}
\max_{1 \le t \le T} |A_{ij, t}| 
 & \le \max_{1 \le t \le T} \Big| \sum\limits_{s=1}^t \widetilde{\varepsilon}_{is} -{\sigma}_i \sum\limits_{s=1}^t Z_{is} \Big| + \max_{1 \le t \le T} \Big| t (\bar{\widetilde{\varepsilon}}_{i} - {\sigma}_i \bar{Z_i}) \Big|\\
 & \quad + \max_{1 \le t \le T} \Big| \sum\limits_{s=1}^t \widetilde{\varepsilon}_{js} - {\sigma}_j \sum\limits_{s=1}^t Z_{js} \Big| + \max_{1 \le t \le T} \Big| t (\bar{\widetilde{\varepsilon}}_{j} -{\sigma}_j \bar{Z_j}) \Big| \\
 & \le 2 \max_{1 \le t \le T} \Big| \sum\limits_{s=1}^t \widetilde{\varepsilon}_{is} -{\sigma}_i \sum\limits_{s=1}^t Z_{is} \Big| + 2 \max_{1 \le t \le T} \Big| \sum\limits_{s=1}^t \widetilde{\varepsilon}_{js} -{\sigma}_j \sum\limits_{s=1}^t Z_{js} \Big| \\
 & = 2 \max_{1 \le t \le T} \Big| \sum\limits_{s=1}^t \widetilde{\varepsilon}_{is} - {\sigma}_i \sum\limits_{s=1}^t \big(\mathbb{B}_{i}(s) - \mathbb{B}_{i}(s-1) \big) \Big| \\
 & \quad +  2 \max_{1 \le t \le T} \Big| \sum\limits_{s=1}^t \widetilde{\varepsilon}_{js} -{\sigma}_j \sum\limits_{s=1}^t \big(\mathbb{B}_{j}(s) - \mathbb{B}_{j}(s-1) \big) \Big|\\
 & = 2 \max_{1 \le t \le T} \Big| \sum\limits_{s=1}^t \widetilde{\varepsilon}_{is} - {\sigma}_i \mathbb{B}_{i}(t) \Big| + 2 \max_{1 \le t \le T} \Big| \sum\limits_{s=1}^t \widetilde{\varepsilon}_{js} - {\sigma}_j \mathbb{B}_{j}(t) \Big|.
\end{align*}
Applying the strong approximation result \eqref{eq-strongapprox-dep}, we can infer that
\[ \max_{1 \le t \le T} |A_{ij, t}| = o_p\big(T^{1/q}\big). \]
Moreover, standard arguments show that $\max_{(u,h) \in \mathcal{G}_T} W_T(u,h) = O( 1/\sqrt{Th_{\min}} )$. Plugging these two results into \eqref{eq-strongapprox-bound3}, we obtain that 
\[ \max_{1\le i < j \le n} \max_{(u,h) \in \mathcal{G}_T} \big| \widetilde{\phi}_{ij, T}(u,h) - \phi_{ij, T}(u,h) \big| = o_p \Big( \frac{T^{1/q}}{\sqrt{Th_{\min}}} \Big), \]
which in view of \eqref{eqA:strong_approx:bound2} yields that $| \widetilde{\Phi}_{n, T} - \Phi_{n, T}^{\diamond} | = o_p( T^{1/q}/\sqrt{Th_{\min}})$. This completes the proof of \eqref{eq-strongapprox-bound-A}.


Next consider $|\Phi_{n, T}^{\diamond} - \Phi_{n, T}|$. It holds that
\begin{align}
 & \big| \Phi_{n, T}^{\diamond} - \Phi_{n, T} \big| \nonumber \\
 & \le \max_{1\leq i< j \leq n}\max_{(u,h) \in \mathcal{G}_T} \Big|\frac{\phi_{ij, T}(u,h)}{\{\widetilde{\sigma}_i^2 + \widetilde{\sigma}_j^2 \}^{1/2}} - \frac{\phi_{ij, T}(u,h)}{\{{\sigma}_i^2 + {\sigma}_j^2 \}^{1/2}}\Big| \nonumber \\
 & \le \max_{1 \le i < j \le n} \left\{ \Big|\big(\widetilde{\sigma}_i^2 + \widetilde{\sigma}_j^2 \big)^{-1/2} - \big(\sigma_i^2 + \sigma_j^2 \big)^{-1/2}\Big| \right\} \max_{1 \le i < j \le n} \max_{(u,h) \in \mathcal{G}_T} \left|\phi_{ij,T}(u,h)\right| \nonumber \\
 & = o_p(\rho_T)  \max_{1 \le i < j \le n} \max_{(u,h) \in \mathcal{G}_T} \left|\phi_{ij,T}(u,h)\right|, \label{eqA:strong_approx:bound5}
\end{align}
where the last line is due to the fact that $\widetilde{\sigma}_i^2 = \sigma_i^2 + o_p(\rho_T)$ and $\widehat{\sigma}_i^2 = \sigma_i^2 + o_p(\rho_T)$. We can write $\phi_{ij, T}(u,h) = \phi_{ij, T}^{(I)}(u,h) - \phi_{ij, T}^{(II)}(u,h)$, where
\begin{align*} 
\phi_{ij, T}^{(I)}(u,h) & = \sum\limits_{t=1}^T w_{t,T}(u,h) \, (\sigma_i Z_{it} - \sigma_j Z_{jt}) \sim \normal(0,{\sigma}^2_i + {\sigma}^2_j) \\
\phi_{ij, T}^{(II)}(u,h) & = \sum\limits_{t=1}^T w_{t,T}(u,h) \, ( \sigma_i \bar{Z}_i - \sigma_j \bar{Z}_j) \sim \normal\big(0, (\sigma_i^2 + \sigma_j^2) c_T(u,h)\big) 
\end{align*}
with $c_T(u,h) = \{\sum_{t=1}^T w_{t, T}(u, h)\}^2/T \le C < \infty$ for all $(u,h) \in \mathcal{G}_T$ and $1\le i < j \le n$. This shows that $\phi_{ij, T}(u,h)$ are centred Gaussian random variables with bounded variance for all $(u,h) \in \mathcal{G}_T$ and $1\le i < j \le n$. Hence, standard results on the maximum of Gaussian random variables yield that 
\begin{equation}\label{eq:phi-bound-max-Gaussians}
\max_{1\leq i< j \leq n}\max_{(u,h) \in \mathcal{G}_T} \big|\phi_{ij, T}(u,h)\big| = O_p(\sqrt{\log T}),
\end{equation}
where we have used that $n$ is fixed and $|\mathcal{G}_T| = O(T^\theta)$ for some large but fixed constant $\theta$ by Assumption \ref{C-grid}. Plugging this into \eqref{eqA:strong_approx:bound5} yields 
$| \Phi_{n, T}^{\diamond} - \Phi_{n, T} | = o_p(\rho_T \sqrt{\log T})$, which completes the proof of \eqref{eq-strongapprox-bound-B}.
\end{proof}


\begin{proof}[\textnormal{\textbf{Proof of Proposition \ref{propA:anticon}.}}] 
The proof is an application of anti-concentration bounds for Gaussian random vectors. We in particular make use of the following anti-concentra\-tion inequality from \cite{Nazarov2003}, which can also be found as Lemma A.1 in \cite{Chernozhukov2017}. 
\begin{lemmaA}\label{lemma-Nazarov}
Let $\boldsymbol{Z} = (Z_1,\ldots,Z_p)^\top$ be a centred Gaussian random vector in $\reals^p$ such that $\ex[Z_j^2] \ge b$ for all $1 \le j \le p$ and some constant $b > 0$. Then for every $\boldsymbol{z} \in \reals^p$ and $a > 0$,
\[ \pr(\boldsymbol{Z} \le \boldsymbol{z} + a) - \pr(\boldsymbol{Z} \le \boldsymbol{z}) \le C a \sqrt{\log p}, \]  
where the constant $C$ only depends on $b$. \textcolor{blue}{(Add notation vector + scalar?)}
\end{lemmaA}
To apply this result, we introduce the following notation: We write $x = (u,h)$ and $\mathcal{G}_T = \{ x : x \in \mathcal{G}_T \} = \{x_1,\ldots,x_p\}$, where $p := |\mathcal{G}_T| \le O(T^\theta)$ for some large but fixed $\theta > 0$ by our assumptions. For $k = 1,\ldots,p$ and $1 \le i < j \le n$, we further let 
\[ Z_{ij, 2k-1} = \frac{\phi_{ij, T}(x_{k1},x_{k2})}{\{{\sigma}_i^2 + {\sigma}_j^2\}^{1/2}} \quad \text{and} \quad Z_{ij, 2k} = -\frac{\phi_{ij, T}(x_{k1},x_{k2})}{\{{\sigma}_i^2 + {\sigma}_j^2\}^{1/2}} \]  
along with $\lambda_{ij,2k-1} = \lambda(x_{k2})$ and $\lambda_{ij,2k} = \lambda(x_{k2})$, where $x_k = (x_{k1},x_{k2})$. Under our assumptions, it holds that $\ex[Z_{ij,l}] = 0$ and $\ex[Z_{ij,l}^2] \ge b > 0$ for all $i$, $j$ and $l$. We next construct the random vector $\boldsymbol{Z} = ( Z_{ij,l} : 1 \le i < j \le n, 1 \le l \le 2p)$ by stacking the variables $Z_{ij, l}$ in a certain order (which can be chosen freely) and construct the vector $\boldsymbol{\lambda} = (\lambda_{ij,l}: 1 \le i < j \le n, 1 \le l \le 2p)$ in an analogous way. Since the variables $Z_{ij,l}$ are normally distributed, $\boldsymbol{Z}$ is a Gaussian random vector of length $(n-1)np$. 


With this notation at hand, we can express the probability $\pr(\Phi_{n,T} \le q)$ as follows for each $q \in \reals$: 
\begin{align*} 
\pr(\Phi_{n,T} \le q) 
 & = \pr \Big( \max_{1\leq i< j \leq n}\max_{1 \le l \le 2p} \big\{ Z_{ij,l} - \lambda_{ij,l} \big\} \le q \Big) \\
 & = \pr \big( Z_{ij,l} \le \lambda_{ij,l} + q \text{ for all } (i,j,l) \Big) \\
 & = \pr \big( \boldsymbol{Z} \le \boldsymbol{\lambda} + q \big). 
\end{align*}
Consequently,
\begin{align*}
\pr\big( |\Phi_{n,T} - x| \le \delta_T \big) 
 & = \pr \big( x - \delta_T \le \Phi_{n,T} \le x + \delta_T \big) \\
 & = \pr \big( \Phi_{n,T} \le x + \delta_T \big) - \pr \big( \Phi_{n,T} \le x \big) \\
 & \quad + \pr \big( \Phi_{n,T} \le x \big) - \pr \big( \Phi_{n,T} \le x - \delta_T \big) \\
 & = \pr \big( \boldsymbol{Z} \le \boldsymbol{\lambda} + x + \delta_T \big) - \pr \big( \boldsymbol{Z} \le \boldsymbol{\lambda} + x \big) \\
 & \quad + \pr \big( \boldsymbol{Z} \le \boldsymbol{\lambda} + x \big) - \pr \big( \boldsymbol{Z} \le \boldsymbol{\lambda} + x - \delta_T\big) \\
 & \le 2 C \delta_T \sqrt{\log((n-1)np)},
\end{align*} 
where the last line is by Lemma \ref{lemma-Nazarov}. This immediately implies Proposition \ref{propA:anticon}.
\end{proof}


\begin{proof}[\textnormal{\textbf{Proof of Proposition \ref{propA:step4}}}] 
Straightforward calculations yield that
\begin{align*}
\begin{split}
\big| \doublehattwo{\Phi}_{n, T} - \widehat{\Phi}_{n, T} \big| &\le \max_{1 \le i < j \le n} \max_{(u,h) \in \mathcal{G}_T} \left|\frac{\doublehat{\phi}_{ij,T}(u,h)}{\big(\doublehattwo{\sigma}_i^2 + \doublehattwo{\sigma}_j^2\big)^{1/2}} - \frac{\doublehat{\phi}_{ij,T}(u,h)}{\big(\widehat{\sigma}_i^2 + \widehat{\sigma}_j^2\big)^{1/2}}\right| \\
&\quad+\max_{1 \le i < j \le n} \max_{(u,h) \in \mathcal{G}_T} \left|\frac{\doublehat{\phi}_{ij,T}(u,h)}{\big(\widehat{\sigma}_i^2 + \widehat{\sigma}_j^2\big)^{1/2}} - \frac{\widehat{\phi}_{ij,T}(u,h)} {\big( \widehat{\sigma}_i^2 + \widehat{\sigma}_j^2 \big)^{1/2}} \right|.
\end{split}
\end{align*}
Since $\doublehattwo{\sigma}_i^2 = \sigma_i^2 + o_p(\rho_T)$ and $\widehat{\sigma}_i^2 = \sigma_i^2 + o_p(\rho_T)$, we further get that 
\begin{align*}
\max_{1 \le i < j \le n} &\max_{(u,h) \in \mathcal{G}_T} \left|\frac{\doublehat{\phi}_{ij,T}(u,h)}{\big(\doublehattwo{\sigma}_i^2 + \doublehattwo{\sigma}_j^2\big)^{1/2}} - \frac{\doublehat{\phi}_{ij,T}(u,h)}{\big(\widehat{\sigma}_i^2 + \widehat{\sigma}_j^2\big)^{1/2}}\right|  \\
&\le\max_{1 \le i < j \le n} \left\{ \Big|\big(\doublehattwo{\sigma}_i^2 + \doublehattwo{\sigma}_j^2 \big)^{-1/2} - \big(\widehat{\sigma}_i^2 + \widehat{\sigma}_j^2 \big)^{-1/2}\Big| \right\} \max_{1 \le i < j \le n} \max_{(u,h) \in \mathcal{G}_T} \Big|\doublehat{\phi}_{ij,T}(u,h)\Big| \\
& = o_p(\rho_T) \max_{1 \le i < j \le n} \max_{(u,h) \in \mathcal{G}_T} \Big|\doublehat{\phi}_{ij,T}(u,h)\Big|
\end{align*}
and 
\begin{align*}
\max_{1 \le i < j \le n} &\max_{(u,h) \in \mathcal{G}_T} \left|\frac{\doublehat{\phi}_{ij,T}(u,h)}{\big(\widehat{\sigma}_i^2 + \widehat{\sigma}_j^2\big)^{1/2}} - \frac{\widehat{\phi}_{ij,T}(u,h)} {\big( \widehat{\sigma}_i^2 + \widehat{\sigma}_j^2\big)^{1/2}} \right| \\
 & \le \max_{1\le i < j \le n} \left\{ \big(\widehat{\sigma}_i^2 + \widehat{\sigma}_j^2 \big)^{-1/2} \right\} \max_{1\le i < j \le n} \max_{(u,h) \in \mathcal{G}_T} \Big| \doublehat{\phi}_{ij, T}(u,h) - \widehat{\phi}_{ij, T}(u,h) \Big| \\
 & = O_p(1) \max_{1 \le i < j \le n} \max_{(u,h) \in \mathcal{G}_T} \Big| \doublehat{\phi}_{ij, T}(u,h) - \widehat{\phi}_{ij, T}(u,h) \Big|,
\end{align*}
where the difference of the kernel averages $\doublehat{\phi}_{ij, T}(u,h) - \widehat{\phi}_{ij, T}(u,h) $ does not include the error terms (they cancel out) and can be written as
\begin{align*}
\Big| & \doublehat{\phi}_{ij, T}(u,h) - \widehat{\phi}_{ij, T}(u,h) \Big| \\
 & = \bigg| \sum_{t=1}^T w_{t,T}(u,h) \big\{ (\bfbeta_i - \widehat{\bfbeta}_i)^\top (\X_{it} - \bar{\X}_{i}) - (\bfbeta_j - \widehat{\bfbeta}_j)^\top (\X_{jt} - \bar{\X}_{j}) \big\} \bigg| \\
 & \le \Big|(\bfbeta_i - \widehat{\bfbeta}_i)^\top \sum_{t=1}^T w_{t,T}(u,h) \X_{it} \Big| +  \big|(\bfbeta_i - \widehat{\bfbeta}_i)^\top\bar{\X}_{i}\big| \bigg| \sum_{t=1}^T w_{t,T}(u,h)  \bigg| \\
 & \quad +\Big|(\bfbeta_j - \widehat{\bfbeta}_j)^\top \sum_{t=1}^T w_{t,T}(u,h) \X_{jt}  \Big| + \big|(\bfbeta_j - \widehat{\bfbeta}_j)^\top\bar{\X}_{j}\big| \bigg| \sum_{t=1}^T w_{t,T}(u,h)  \bigg|. 
\end{align*}
Hence,
\begin{equation}\label{ineq-diff-1}
\big| \doublehattwo{\Phi}_{n, T} - \widehat{\Phi}_{n, T} \big| \le o_p( \rho_T) A_{n,T} + O_p(1) \big\{ 2B_{n,T} + 2C_{n,T} \}, 
\end{equation}
where 
\begin{align*}
A_{n,T} & = \max_{1 \le i< j \le n} \max_{(u,h) \in \mathcal{G}_T} \Big|\doublehat{\phi}_{ij,T}(u,h)\Big| \\
B_{n,T} & = \max_{1 \le i \le n} \max_{(u,h) \in \mathcal{G}_T} \Big| (\bfbeta_i - \widehat{\bfbeta}_i)^\top\sum_{t=1}^T w_{t,T}(u,h) \X_{it} \Big| \\
C_{n,T} & = \max_{1 \le i \le n}\big|(\bfbeta_i - \widehat{\bfbeta}_i)^\top\bar{\X}_{i}\big| \max_{(u,h) \in \mathcal{G}_T}  \Big| \sum_{t=1}^T w_{t,T}(u,h)  \Big|. 
\end{align*}
We examine each of these three terms separately. 


We first prove that 
\begin{equation}\label{eq:Ant:1} 
A_{n,T}  = \max_{1 \le i< j \le n} \max_{(u,h) \in \mathcal{G}_T} \Big|\doublehat{\phi}_{ij,T}(u,h)\Big| = O_p\big(\sqrt{\log T}\big).
\end{equation}
From the proof of Proposition \ref{propA:strong_approx}, we know that there exist identically distributed versions $\widetilde{\phi}_{ij, T}(u, h)$ of the statistics $\doublehat{\phi}_{ij,T}(u,h)$ with the property that 
\begin{equation}\label{eq:result-from-propA-strong_approx}
\max_{1\le i < j \le n} \max_{(u,h) \in \mathcal{G}_T} \big| \widetilde{\phi}_{ij, T}(u,h) - \phi_{ij, T}(u,h) \big| = o_p \Big( \frac{T^{1/q}}{\sqrt{Th_{\min}}} \Big). 
\end{equation}
Instead of \eqref{eq:Ant:1}, it thus suffices to show that 
\begin{equation}\label{eq:Ant:2} 
\max_{1 \le i< j \le n}\max_{(u,h) \in \mathcal{G}_T}\Big|\widetilde{\phi}_{ij, T}(u, h)\Big| = O_p\big(\sqrt{\log T}\big).
\end{equation}
Since for any constant $c > 0$, 
\begin{align*}
&\pr\left( \max_{i,j,(u,h)}\left|\phi_{ij, T}(u, h)\right| \leq \frac{c \sqrt{\log T}}{2} \right) \\
&\leq \pr\left( \max_{i,j,(u,h)}\left|\widetilde{\phi}_{ij, T}(u, h)\right| \leq c \sqrt{\log T}\right) \\
& \quad + \pr\left(\left|\max_{i,j,(u,h)}\left|\widetilde{\phi}_{ij, T}(u, h)\right| - \max_{i,j,(u,h)}\left|\phi_{ij, T}(u, h)\right| \right| > \frac{c\sqrt{\log T}}{2}\right) \\
&\leq \pr\left( \max_{i,j,(u,h)}\left|\widetilde{\phi}_{ij, T}(u, h)\right| \leq c \sqrt{\log T}\right) \\
&\quad + \pr\left(\max_{i,j,(u,h)}\left|\widetilde{\phi}_{ij, T}(u, h)- \phi_{ij, T}(u, h)\right| > \frac{c\sqrt{\log T}}{2}\right)
\end{align*}
and $\pr(\max_{i,j,(u,h)}|\widetilde{\phi}_{ij, T}(u, h)- \phi_{ij, T}(u, h)| > c \sqrt{\log T}/2) = o(1)$ by \eqref{eq:result-from-propA-strong_approx}, we obtain that
\begin{align} 
\pr\bigg(\max_{i,j,(u,h)} & \left|\widetilde{\phi}_{ij, T}(u, h)\right| \leq c\sqrt{\log T} \bigg) \nonumber \\
 & \geq \pr\left( \max_{i,j,(u,h)}\left|\phi_{ij, T}(u, h)\right| \leq \frac{c\sqrt{\log T}}{2}\right) - o(1). \label{eq:Ant:intermediate}
\end{align}
Moreover, since $\max_{i,j,(u,h)}\left|\phi_{ij, T}(u, h)\right| = O_p(\sqrt{\log{T}})$ as already proven in \eqref{eq:phi-bound-max-Gaussians}, we can make the probability $\pr( \max_{i,j,(u,h)}|\phi_{ij, T}(u, h)| \leq c\sqrt{\log T}/2)$ on the right-hand side of \eqref{eq:Ant:intermediate} arbitrarily close to $1$ by choosing the constant $c$ sufficiently large. Hence, for any $\delta > 0$, we can find a constant $c > 0$ such that $\pr(\max_{i,j,(u,h)}|\widetilde{\phi}_{ij, T}(u, h)| \leq c\sqrt{\log T} ) \ge 1 - \delta$ for sufficiently large $T$. This proves \eqref{eq:Ant:2}, which in turn yields \eqref{eq:Ant:1}. 


We next turn to $B_{n,T}$. Without loss of generality, we assume that $\X_{it}$ is real-valued. The vector-valued case can be handled analogously. To start with, we have a closer look at the term $\sum_{t=1}^T w_{t,T}(u,h) \X_{it}$. By construction, the kernel weights $w_{t, T}(u, h)$ are unequal to $0$ if and only if $T(u-h) \le t \le T(u+h)$. We can use this fact to write
\begin{align*}
\Big| \sum_{t=1}^T w_{t,T}(u,h) \X_{it}   \Big|  = \bigg| \sum_{t=\lfloor T(u-h) \rfloor}^{\lceil T(u+h) \rceil} w_{t,T}(u,h) \X_{it}   \bigg|.
\end{align*}
Note that
\begin{align}\label{eq:sum_weights}
\begin{split}
\sum_{t=\lfloor T(u-h) \rfloor}^{\lceil T(u+h) \rceil} w^2_{t,T}(u,h) &= \sum_{t=1}^T w^2_{t,T}(u,h) = \sum_{t=1}^T\frac{\Lambda^2_{t,T}(u,h)}{\sum\nolimits_{s=1}^T\Lambda^2_{s,T}(u,h) } = 1.
\end{split}
\end{align}
Denoting by $D_{T, u, h}$ the number of integers between $\lfloor T(u-h) \rfloor$ and $\lceil T(u+h) \rceil$ (with the obvious bounds $2Th \leq D_{T, u, h} \leq 2Th + 2$) and using \eqref{eq:sum_weights}, we can normalize the kernel weights as follows:
\begin{align*}
\sum_{t=\lfloor T(u-h) \rfloor}^{\lceil T(u+h) \rceil} \big(\sqrt{D_{T, u, h}}\cdot w_{t,T}(u,h)\big)^2 = D_{T, u, h}.
\end{align*}
Next, we apply Proposition \ref{theo-wu2016} with the weights $a_t = \sqrt{D_{T, u, h}}\cdot w_{t,T}(u,h)$ to obtain that
\begin{align}
\pr \bigg(\bigg| \sum_{t=\lfloor T(u-h) \rfloor}^{\lceil T(u+h) \rceil} & \sqrt{D_{T, u, h}}\cdot w_{t,T}(u,h) \X_{it}  \bigg| \geq x\bigg) \nonumber \\
 & \leq C_1 \frac{\big( \sum_{t=\lfloor T(u-h) \rfloor}^{\lceil T(u+h) \rceil} |\sqrt{D_{T, u, h}}\cdot w_{t,T}(u,h)|^{q^\prime}\big) \| \X_{i \cdot}\|^{q^\prime}_{q^\prime, \alpha}}{ x^{q^\prime}} \nonumber \\
 & \qquad \qquad \qquad \qquad \qquad \qquad + C_2 \exp \left(-\frac{C_3  x^2}{D_{T, u, h}\| \X_{i\cdot}\|^{2}_{2, \alpha}}\right) \label{ineq-diff-8}
\end{align}
for any $x > 0$, where \textcolor{red}{$\| \X_{i\cdot}\|^{q^\prime}_{q^\prime, \alpha} = \sup_{t\geq 0} (t+1)^{\alpha} \sum_{s=t}^{\infty}\delta_{q^\prime}(\boldsymbol{h}_i, s)$} is the dependence adjusted norm introduced in Definition \ref{defA-DAN} and $\| \X_{i\cdot}\|^{q^\prime}_{q^\prime, \alpha} < \infty$ by Assumption \ref{C-reg3}. From \eqref{ineq-diff-8}, it follows that for any $\delta > 0$, 
\begin{align}
&\pr\left(\max_{(u, h) \in \mathcal{G}_T} \Big| \sum_{t=\lfloor T(u-h) \rfloor}^{\lceil T(u+h) \rceil} w_{t,T}(u,h) \X_{it}  \Big| \geq \delta T^{1/q} \right) \nonumber \\
&\leq \sum_{(u, h) \in \mathcal{G}_T} \pr \left( \Big| \sum_{t=\lfloor T(u-h) \rfloor}^{\lceil T(u+h) \rceil} w_{t,T}(u,h) \X_{it}  \Big| \geq \delta T^{1/q} \right) \nonumber \\
&= \sum_{(u, h) \in \mathcal{G}_T} \pr \left( \Big| \sum_{t=\lfloor T(u-h) \rfloor}^{\lceil T(u+h) \rceil} \sqrt{D_{T, u, h}}\cdot w_{t,T}(u,h) \X_{it}  \Big| \geq \delta\sqrt{D_{T, u, h}}T^{1/q}  \right) \nonumber \\
&\leq \sum_{(u, h) \in \mathcal{G}_T} \left[C_1 \frac{(\sqrt{D_{T, u, h}})^{q^\prime}\big( \sum |w_{t,T}(u,h)|^{q^\prime}\big) \| \X_{i\cdot}\|^{q^\prime}_{q^\prime, \alpha}}{ \big(\delta\sqrt{D_{T, u, h}}T^{1/q}\big)^{q^\prime}} + C_2 \exp \left(-\frac{C_3 \big(\delta\sqrt{D_{T, u, h}}T^{1/q} \big)^2}{D_{T, u, h}\| \X_{i\cdot}\|^{2}_{2, \alpha}}\right) \right] \nonumber \\
&= \sum_{(u, h) \in \mathcal{G}_T} \left[C_1 \frac{\big( \sum |w_{t,T}(u,h)|^{q^\prime}\big) \| \X_{i\cdot}\|^{q^\prime}_{q^\prime, \alpha}}{\delta^{q^\prime}T^{q^\prime/q} } + C_2 \exp \left(-\frac{C_3 \delta^2 T^{2/q} }{\|\X_{i\cdot}\|^{2}_{2, \alpha}}\right) \right] \nonumber \\
&\leq C_1 \frac{ T^\theta \|\X_{i\cdot}\|^{q^\prime}_{q^\prime, \alpha}}{\delta^{q^\prime} T^{q^\prime/q}} \max_{(u, h) \in \mathcal{G}_T} \left( \sum\nolimits_{t=\lfloor T(u-h) \rfloor}^{\lceil T(u+h) \rceil} |w_{t,T}(u,h)|^{q^\prime}\right)+ C_2 T^\theta \exp \left(-\frac{C_3 \delta^2 T^{2/q}}{\|\X_{i\cdot}\|^{2}_{2, \alpha}}\right) \nonumber \\
&= C \frac{ T^{\theta - q^\prime/q}}{\delta^{q^\prime}} + C T^\theta \exp \left(-C T^{2/q} \delta^2\right), \label{eq:exp-bound-BnT}
\end{align}
where the constant $C$ depends neither on $T$ nor on $\delta$. In the last equality of the above display, we have used the following facts:
\begin{enumerate}[label=(\roman*),leftmargin=0.85cm]
\item $\|\X_{i\cdot}\|^{q^\prime}_{q^\prime, \alpha}  < \infty$ by Assumption \ref{C-reg3}.
\item $\|\X_{i\cdot}\|^{2}_{2, \alpha} < \infty$ (which follows from (i)).
\item $\max_{(u, h) \in \mathcal{G}_T} ( \sum_{t=\lfloor T(u-h) \rfloor}^{\lceil T(u+h) \rceil} |w_{t,T}(u,h)|^{q^\prime} ) \leq 1$ for the following reason: By \eqref{eq:sum_weights}, it holds that $\sum_{t=1}^{T} w^2_{t,T}(u,h) = 1$ and thus $0 \leq w^2_{t,T}(u,h) \leq 1$ for all $t$, $T$ and $(u, h)$. This implies that $0 \leq |w_{t,T}(u,h)|^{q^\prime} =  (w^2_{t,T}(u,h))^{q^\prime/2} \leq w^2_{t,T}(u,h) \leq 1$ for all $t$, $T$ and $(u, h)$. As a result, 
\begin{align*}
\max_{(u, h) \in \mathcal{G}_T} \left( \sum_{t=\lfloor T(u-h) \rfloor}^{\lceil T(u+h) \rceil} |w_{t,T}(u,h)|^{q^\prime}\right) \leq
\max_{(u, h) \in \mathcal{G}_T} \left( \sum_{t=\lfloor T(u-h) \rfloor}^{\lceil T(u+h) \rceil} w_{t,T}^2(u,h)\right) =1.
\end{align*}
\end{enumerate}
Since $\theta - q^\prime/q <0$ by Assumption \ref{C-reg3}, the bound in \eqref{eq:exp-bound-BnT} converges to $0$ as $T \to \infty$ for any fixed $\delta >0$. Consequently, we obtain that
\begin{align}\label{ineq-diff-9}
\max_{(u, h) \in \mathcal{G}_T} \bigg| \sum_{t=\lfloor T(u-h) \rfloor}^{\lceil T(u+h) \rceil} w_{t,T}(u,h)\X_{it}  \bigg| = o_p(T^{1/q}).
\end{align}
Using this together with the fact that $\bfbeta_i - \widehat{\bfbeta}_i = O_p(1/\sqrt{T})$ (which is the statement of Lemma \ref{lemma-beta-rate}), we arrive at the bound 
\[ B_{n,T} = \max_{1 \le i \le n} \max_{(u,h) \in \mathcal{G}_T} \Big| (\bfbeta_i - \widehat{\bfbeta}_i)^\top\sum_{t=1}^T w_{t,T}(u,h) \X_{it} \Big| = o_p\Big(\frac{T^{1/q}}{\sqrt{T}}\Big). \]


We finally turn to $C_{n,T}$. Straightforward calculations yield that $| \sum_{t=1}^T w_{t,T}(u,h) | \le C \sqrt{T h_{\max}} = o(\sqrt{T})$. 
%Again, by construction the weights $w_{t, T}(u, h)$ are not equal to $0$ if and only if \linebreak $T(u-h) \le t \le T(u+h)$. We can use this fact to bound  $\left| \sum_{t=1}^T w_{t,T}(u,h)  \right|$ for all $(u, h) \in \mathcal{G}_T$ using the Cauchy-Schwarz inequality:
%\begin{align*}
%\Big| \sum_{t=1}^T w_{t,T}(u,h)   \Big| & = \left| \sum_{t=\lfloor T(u-h) \rfloor}^{\lceil T(u+h) \rceil} w_{t,T}(u,h) \cdot 1  \right|  \\
%&\leq \sqrt{\sum_{t=\lfloor T(u-h) \rfloor}^{\lceil T(u+h) \rceil} w^2_{t,T}(u,h)}\sqrt{\sum_{t=\lfloor T(u-h) \rfloor}^{\lceil T(u+h) \rceil} 1^2}\\
%&=\sqrt{1}\cdot\sqrt{D_{T, u, h}} \\
%&  \leq \sqrt{2Th + 2} \\
%&\leq \sqrt{2Th_{\max} +2} \\
%&\leq\sqrt{T+2}.
%\end{align*}
%Hence, 
%\begin{align}\label{ineq-diff-13}
%\max_{(u,h) \in \mathcal{G}_T}  \Big| \sum_{t=1}^T w_{t,T}(u,h)  \Big| = O(\sqrt{T}).
%\end{align}
Moreover, $\bar{\X}_i = O_p(1/\sqrt{T})$ by Lemma \ref{lemma-wlln} and $\bfbeta_i - \widehat{\bfbeta}_i = O_p(1/\sqrt{T})$ by Lemma \ref{lemma-beta-rate}. This immediately yields that
\[ C_{n,T} = \max_{1\le i  \le n}\big|(\bfbeta_i - \widehat{\bfbeta}_i)^\top\bar{\X}_{i}\big| \max_{(u,h) \in \mathcal{G}_T}  \Big| \sum_{t=1}^T w_{t,T}(u,h)  \Big| = o_p\Big(\frac{1}{\sqrt{T}}\Big). \]


To summarize, we have shown that 
\begin{align*}
\big| \doublehattwo{\Phi}_{n, T} - \widehat{\Phi}_{n, T} \big| 
 & \le o_p( \rho_T) A_{n,T} + O_p(1) \big\{ 2B_{n,T} + 2C_{n,T} \} \\
 & = o_p( \rho_T) O_p(\sqrt{\log T}) + o_p \Big( \frac{T^{1/q}}{\sqrt{T}} \Big) + o_p \Big( \frac{1}{\sqrt{T}} \Big), \
\end{align*}
where $\{c_T\}$ is any sequence of positive real numbers with $\sqrt{\log T} \ll c_T$. This immediately implies the desired result. 
\end{proof}


%\begin{proof}[\textnormal{\textbf{Proof of Lemma \ref{lemmaA:step1}}}] 
%It holds that
%\begin{align*} \pr(\widehat{\Phi}_{n, T} \le x) &= \pr \Big(\widehat{\Phi}_{n, T} \le x, \big|\doublehattwo{\Phi}_{n,T} - \widehat{\Phi}_{n,T}\big| \le \gamma \Big) + \pr \Big(\widehat{\Phi}_{n, T} \le x, \big|\doublehattwo{\Phi}_{n,T} - \widehat{\Phi}_{n,T}\big| > \gamma \Big) \\
%& \le  \pr \Big(\widehat{\Phi}_{n, T} \le x, \widehat{\Phi}_{n,T} - \gamma \le \doublehattwo{\Phi}_{n,T} \le \widehat{\Phi}_{n,T} + \gamma \Big) + \pr \Big(\big|\doublehattwo{\Phi}_{n,T} - \widehat{\Phi}_{n,T}\big| > \gamma \Big) \\
%& \le  \pr \Big(\doublehattwo{\Phi}_{n,T} \le x + \gamma \Big) + \pr \Big(\big|\doublehattwo{\Phi}_{n,T} - \widehat{\Phi}_{n,T}\big| > \gamma \Big).
%\end{align*}
%Analogously, 
%\begin{align*} \pr(\doublehattwo{\Phi}_{n, T} \le x - \gamma)  \le  \pr \Big(\widehat{\Phi}_{n,T} \le x \Big) + \pr \Big(\big|\doublehattwo{\Phi}_{n,T} - \widehat{\Phi}_{n,T}\big| > \gamma \Big).
%\end{align*}
%Combining these two inequalities, we arrive at the desired result.
%\end{proof}



\subsection*{Proof of Proposition \ref{prop:test}}


We first show that 
\begin{equation}\label{eq:quant-exact}
\pr(\Phi_{n,T} \le q_{n,T}(\alpha)) = 1 - \alpha. 
\end{equation}
We proceed by contradiction. Suppose that \eqref{eq:quant-exact} does not hold true. Since $\pr(\Phi_{n,T} \le q_{n,T}(\alpha)) \ge 1 - \alpha$ by definition of the quantile $q_{n,T}(\alpha)$, there exists $\xi > 0$ such that $\pr(\Phi_{n,T} \le q_{n,T}(\alpha)) = 1-\alpha + \xi$. From the proof of Proposition \ref{propA:anticon}, we know that for any $\delta > 0$, 
\begin{align*}
\pr \big(\Phi_{n,T} & \le q_{n,T}(\alpha)\big) - \pr\big(\Phi_{n,T} \le q_{n,T}(\alpha) - \delta\big) \\
 & \quad \le \sup_{x \in \reals} \pr \big(|\Phi_{n,T} - x| \le \delta \big)  \le 2 C \delta \sqrt{\log((n-1)np)}. 
\end{align*}
Hence, 
\begin{align*}
\pr \big(\Phi_{n,T} \le q_{n,T}(\alpha) - \delta \big) 
 & \ge \pr\big(\Phi_{n,T} \le q_{n,T}(\alpha) \big) - 2 C \delta \sqrt{\log((n-1)np)} \\
 & = 1-\alpha + \xi - 2 C \delta \sqrt{\log((n-1)np)} > 1-\alpha
\end{align*}
for $\delta > 0$ small enough. This contradicts the definition of the quantile $q_{n,T}(\alpha)$ according to which $q_{n,T}(\alpha) = \inf_{q \in \reals} \{ \pr(\Phi_{n,T} \le q) \ge 1-\alpha \}$. We thus arrive at \eqref{eq:quant-exact}. 


Proposition \ref{prop:test} is a simple consequence of Theorem \ref{theo:stat:global} and Equation \eqref{eq:quant-exact}. Specifically, we obtain that under $H_0$, 
\begin{align*}
\big| \pr(\widehat{\Psi}_{n, T} \le q_{n,T}(\alpha)) - (1-\alpha) \big| 
 & = \big| \pr(\widehat{\Phi}_{n, T} \le q_{n,T}(\alpha)) - (1-\alpha) \big| \\
 & = \big| \pr(\widehat{\Phi}_{n, T} \le q_{n,T}(\alpha)) - \pr(\Phi_{n,T} \le q_{n,T}(\alpha)) \big| \\
 & \le \sup_{x \in \reals} \big| \pr(\widehat{\Phi}_{n, T} \le x) - \pr(\Phi_{n,T} \le x) \big| = o(1). 
\end{align*}



\subsection*{Proof of Proposition \ref{prop:test:power}}%\label{subsec:app:power}


To start with, note that for some sufficiently large constant $C$ we have
\begin{equation}\label{eqA:power:lambda}
\lambda(h) = \sqrt{2\log\{1/(2h)\}} \le \sqrt{2\log\{1/(2h_{\min})\}} \le C \sqrt{\log T}.
\end{equation}
Write $\widehat{\psi}_{ij, T}(u,h) = \widehat{\psi}_{ij, T}^A(u,h) + \widehat{\psi}_{ij, T}^B(u,h)$ with 
\begin{align*}
\widehat{\psi}^A_{ij,T}(u,h) &= \sum_{t=1}^T w_{t,T}(u,h) \big\{ (\varepsilon_{it} - \bar{\varepsilon}_i) + (\bfbeta_i - \widehat{\bfbeta}_i)^\top (\X_{it} - \bar{\X}_{i}) - \bar{m}_{i, T} \\
& \quad \quad - (\varepsilon_{jt} - \bar{\varepsilon}_j) -  (\bfbeta_j - \widehat{\bfbeta}_j)^\top (\X_{jt} - \bar{\X}_{j}) + \bar{m}_{j, T} \big\} \\
\widehat{\psi}_{ij, T}^B(u,h) &= \sum\nolimits_{t=1}^T w_{t,T}(u,h) \bigg(m_{i, T}\Big(\frac{t}{T}\Big) - m_{j, T}\Big(\frac{t}{T}\Big) \bigg),
\end{align*}
where $\bar{m}_{i, T} = T^{-1} \sum_{t=1}^T m_{i, T} (t/T)$. Without loss of generality, consider the following scenario: there exists $(u_0,h_0) \in \mathcal{G}_T$ with $[u_0-h_0,u_0+h_0] \subseteq [0,1]$ such that \begin{align}\label{eqA:power2}
m_{i,T}(w) - m_{j,T}(w) \ge c_T \sqrt{\log T/(Th_0)}
\end{align}
for all $w \in [u_0-h_0,u_0+h_0]$. 


We first derive a lower bound on the term $\widehat{\psi}_{ij, T}^B(u_0,h_0)$.  
Since the kernel $K$ is symmetric and $u_0 = t/T$ for some $t$, it holds that $S_{T,1}(u_0,h_0) = 0$ and thus,
\begin{align*} 
w_{t,T}(u_0,h_0) 
&= \frac{K\Big(\frac{\frac{t}{T}-u_0}{h_0}\Big) S_{T, 2}(u_0, h_0)}{\Big\{ \sum_{t=1}^T K^2\Big(\frac{\frac{t}{T}-u_0}{h_0}\Big)S^2_{T, 2}(u_0, h_0) \Big\}^{1/2}} \\
&=\frac{K\Big(\frac{\frac{t}{T}-u_0}{h_0}\Big)}{\Big\{ \sum_{t=1}^T K^2\Big(\frac{\frac{t}{T}-u_0}{h_0}\Big)\Big\}^{1/2}} \ge 0.
\end{align*}
Together with \eqref{eqA:power2}, this implies that 
\begin{equation}\label{eq1-proof-prop-test-power}
\widehat{\psi}_{ij, T}^B(u_0,h_0) \ge c_T \sqrt{\frac{\log T}{Th_0}} \sum\limits_{t=1}^T w_{t,T}(u_0,h_0).
\end{equation}
Using the Lipschitz continuity of the kernel $K$, we can show by straightforward calculations that for any $(u,h) \in \mathcal{G}_T$ and any natural number $\ell$, 
\begin{equation}\label{eq-riemann-sum}
\Big| \frac{1}{Th} \sum\limits_{t=1}^T K\Big(\frac{\frac{t}{T}-u}{h}\Big) \Big(\frac{\frac{t}{T}-u}{h}\Big)^\ell - \int_0^1 \frac{1}{h} K\Big(\frac{w-u}{h}\Big) \Big(\frac{w-u}{h}\Big)^\ell dw \Big| \le \frac{C}{Th}, 
\end{equation}
where the constant $C$ does not depend on $u$, $h$ and $T$. With the help of \eqref{eq-riemann-sum}, we obtain that for any $(u,h) \in \mathcal{G}_T$ with $[u-h,u+h] \subseteq [0,1]$, 
\begin{equation}\label{eq2-proof-prop-test-power}
\Big| \sum\limits_{t=1}^T w_{t,T}(u,h) - \frac{\sqrt{Th}}{\kappa} \Big| \le \frac{C}{\sqrt{Th}}, 
\end{equation}
where $\kappa = (\int K^2(\varphi)d\varphi)^{1/2}$ and the constant $C$ does once again not depend on $u$, $h$ and $T$. From \eqref{eq2-proof-prop-test-power}, it follows that $\sum\nolimits_{t=1}^T w_{t,T}(u,h) \ge \sqrt{Th} / (2\kappa)$ for sufficiently large $T$ and any $(u,h) \in \mathcal{G}_T$ with $[u-h,u+h] \subseteq [0,1]$. This together with \eqref{eq1-proof-prop-test-power} allows us to infer that 
\begin{equation}\label{eqA:power:psiB}
\widehat{\psi}_{ij, T}^B(u_0,h_0) \ge \frac{c_T \sqrt{\log T}}{2 \kappa} 
\end{equation}
for sufficiently large $T$. 


We next analyze $\widehat{\psi}^A_{ij,T}(u_0,h_0)$, which can be expressed as $\widehat{\psi}^A_{ij,T}(u_0,h_0) = \widehat\phi_{ij,T}(u, h) + (\bar{m}_{j, T} - \bar{m}_{i, T}) \sum_{t=1}^T w_{t, T}(u, h)$. The proof of Proposition \ref{propA:step4} shows that 
\begin{equation*}
\max_{1 \le i < j \le n} \max_{(u,h) \in \mathcal{G}_T} \Big| \widehat{\phi}_{ij, T}(u,h) \Big| = O_p(\sqrt{\log T}). 
\end{equation*}
%\[ \max_{1 \le i < j \le n} \max_{(u,h) \in \mathcal{G}_T} \Big| \doublehat{\phi}_{ij, T}(u,h) - \widehat{\phi}_{ij, T}(u,h) \Big| = o_p(\frac{T^{1/q}}{\sqrt{T}} + \frac{\log^{2/{q^\prime}}(T)}{\sqrt{T}} \Big). \]
%Moreover, by \eqref{eq:Ant:1},
%\[ \max_{1 \le i < j \le n} \max_{(u,h) \in \mathcal{G}_T} \Big| \doublehat{\phi}_{ij, T}(u,h) = o_p(c_T), \]
Using this together with the bounds $\bar{m}_{i, T} \le C/T$ and $\sum_{t=1}^T w_{t, T}(u, h) \le C \sqrt{T}$, we can infer that 
\begin{align} 
 & \max_{1 \le i < j \le n} \max_{(u,h) \in \mathcal{G}_T} \Big| \widehat\psi_{ij,T}^A(u, h) \Big| \nonumber \\[-0.2cm]
 & = \max_{1 \le i < j \le n} \max_{(u,h) \in \mathcal{G}_T} \Big| \widehat\phi_{ij,T}(u, h) + (\bar{m}_{j, T} - \bar{m}_{i, T}) \sum_{t=1}^T w_{t, T}(u, h) \Big| = O_p(\sqrt{\log T}). \label{eqA:power:psiA}
\end{align}
With the help of \eqref{eqA:power:psiB}, \eqref{eqA:power:psiA}, \eqref{eqA:power:lambda} and the assumption that $\widehat{\sigma}^2_i = \sigma^2_i + o_p(\rho_T)$, we finally arrive at 
\begin{align}
\widehat{\Psi}_{n, T}  
 & \ge \max_{1 \le i < j \le n} \max_{(u,h) \in \mathcal{G}_T} \frac{|\widehat{\psi}_{ij, T}^B(u,h)|}{\{\widehat{\sigma}_i^2 + \widehat{\sigma}_j^2\}^{1/2}} - \max_{1 \le i < j \le n} \max_{(u,h) \in \mathcal{G}_T} \bigg\{ \frac{|\widehat{\psi}_{ij, T}^A(u,h)|}{\{\widehat{\sigma}^2_i + \widehat{\sigma}_j^2\}^{1/2}} + \lambda(h) \bigg\} \nonumber \\
 & = \max_{1 \le i < j \le n} \max_{(u,h) \in \mathcal{G}_T} \frac{|\widehat{\psi}_{ij, T}^B(u,h)|}{\{\widehat{\sigma}_i^2 + \widehat{\sigma}_j^2\}^{1/2}} + O_p(\sqrt{\log T}) \nonumber \\
 & \ge \frac{c_T \sqrt{\log T}}{2 \kappa} \min_{1 \le i < j \le n}\{\widehat{\sigma}_i^2 + \widehat{\sigma}_j^2\}^{-1/2} + O_p(\sqrt{\log T}) \nonumber \\
 & \ge C\frac{c_T \sqrt{\log T}}{2 \kappa} + O_p(\sqrt{\log T}). \label{eq5-proof-prop-test-power}
\end{align}
Since $q_{n, T}(\alpha) = O(\sqrt{\log T})$ for any fixed $\alpha \in (0,1)$ and $c_T \to \infty$, \eqref{eq5-proof-prop-test-power} immediately implies that $\pr(\widehat{\Psi}_{n, T} \le q_{n, T}(\alpha)) = o(1)$. 



\subsection*{Proof of Proposition \ref{prop:test:fwer}}\label{subsec:app:fwer}


Denote by $\mathcal{M}_0$ the set of quadruples $(i, j, u, h) \in \{1\ldots, n\}^2 \times \grid$ for which $H_0^{[i, j]}(u, h)$ is true. Then we can write the $\text{FWER}$ as
\begin{align*}
\text{FWER}(\alpha)
 & = \pr \Big( \exists (i,j,u, h) \in \mathcal{M}_0: \widehat{\psi}^0_{ij,T}(u, h) > q_{n, T}(\alpha) \Big) \\
 & = \pr \Big( \max_{(i, j, u, h) \in \mathcal{M}_0} \widehat{\psi}^0_{ij,T}(u, h) > q_{n, T}(\alpha) \Big) \\
 & = \pr \Big( \max_{(i,j,u, h) \in \mathcal{M}_0} \widehat{\phi}^0_{ij,T}(u, h) > q_{n, T}(\alpha) \Big) \\
 & \le \pr \Big( \max_{1 \le i < j \le n} \max_{(u, h) \in \grid} \widehat{\phi}_{ij,T}^0(u, h) > q_{n, T}(\alpha) \Big) \\
 & = \pr \big( \widehat{\Phi}_{n, T} > q_{n, T}(\alpha) \big) = \alpha + o(1),
\end{align*}
where the third equality uses that $\hat{\psi}^0_{ijk,T} = \hat{\phi}^0_{ijk,T}$ under $H_0^{[i, j]}(u, h)$.



\subsection*{Proof of Corollary \ref{corollary1}}\label{subsec:app:corollary}


By Proposition \ref{prop:test:fwer}, 
\begin{align*}
1 - \alpha + o(1) 
 & \le 1 - \textnormal{FWER}(\alpha) \\
 & = \pr \Big( \nexists (i,j,u, h) \in \mathcal{M}_0: \hat{\psi}^0_{ij,T}(u, h) > q_{n, T}(\alpha) \Big) \\
& = \pr\Big( \forall (i,j,u, h) \in \mathcal{M}_0: \hat{\psi}^0_{ij,T}(u, h) \le q_{n, T}(\alpha) \Big)\\
&=\pr\Big( \forall \, i,j \in \{1, \ldots, n\}, (u, h) \in \grid \text{ such that }\\
&\quad\quad \quad H_0^{[i, j]}(u, h) \text{ is true}: \hat{\psi}^0_{ij,T}(u, h) \le q_{n,T}(\alpha) \Big),
\end{align*}
which is the statement of Corollary \ref{corollary1}.


\subsection*{Proof of Proposition \ref{prop:clustering:1}}

For the sake of brevity, we introduce the following notation. For each $i$ and $j$, we define the statistics $\widehat{\Psi}_{ij,T} : = \max_{(u, h) \in \mathcal{G}_T}\hat{\psi}^0_{ij, T}(u, h)$ which can be interpreted as a distance measure between the two curves $m_i$ and $m_j$ on the whole interval $[0, 1]$. Using this notation, we can rewrite the dissimilarity measure defined in \eqref{dissimilarity} as 
\begin{equation*}
\widehat{\Delta}(S,S^\prime) = \max_{\substack{i \in S, \\ j \in S^\prime}} \widehat{\Psi}_{ij,T}. 
\end{equation*}
Now consider the event  
\[ B_{n,T} = \Big\{ \max_{1 \le \ell \le N} \max_{i,j \in G_\ell} \widehat{\Psi}_{ij,T} \le q_{n,T}(\alpha) \ \text{ and } \ \min_{1 \le \ell < \ell^\prime \le N} \min_{\substack{i \in G_\ell, \\ j \in G_{\ell^\prime}}} \widehat{\Psi}_{ij,T} > q_{n,T}(\alpha) \Big\}. \]
The term $\max_{1 \le \ell \le N} \max_{i,j \in G_\ell} \widehat{\Psi}_{ij,T}$ is the largest multiscale distance between two time series $i$ and $j$ from the same group, whereas $\min_{1 \le \ell < \ell^\prime \le N} \min_{i \in G_\ell, \, j \in G_{\ell^\prime}} \widehat{\Psi}_{ij,T}$ is the smallest multiscale distance between two time series from two different groups. On the event $B_{n,T}$, it obviously holds that 
\begin{equation}\label{eq1-prop-clustering-1}
\max_{1 \le \ell \le N} \max_{i,j \in G_\ell} \widehat{\Psi}_{ij,T} < \min_{1 \le \ell < \ell^\prime \le N} \min_{\substack{i \in G_\ell, \\ j \in G_{\ell^\prime}}} \widehat{\Psi}_{ij,T}. 
\end{equation}
Hence, any two time series from the same class have a smaller distance than any two time series from two different classes. With the help of Proposition \ref{prop:test}, it is easy to see that
\[  \pr \Big( \max_{1 \le \ell \le N} \max_{i,j \in G_\ell} \widehat{\Psi}_{ij,T} \le q_{n,T}(\alpha) \Big) \ge (1 - \alpha) + o(1). \]
Moreover, the same arguments as for Proposition \ref{prop:test:power} show that 
\[  \pr \Big( \min_{1 \le \ell < \ell^\prime \le N} \min_{\substack{i \in G_\ell, \\ j \in G_{\ell^\prime}}} \widehat{\Psi}_{ij,T} \le q_{n,T}(\alpha) \Big) = o(1). \]
Taken together, these two statements imply that 
\begin{equation}\label{eq2-prop-clustering-1}
\pr \big( B_{n,T} \big) \ge (1-\alpha) + o(1). 
\end{equation}
In what follows, we show that on the event $B_{n,T}$, (i) $\{ \widehat{G}_1^{[n-N]},\ldots,\widehat{G}_N^{[n-N]} \big\} = \big\{ G_1,\ldots$ $\ldots,G_N \}$ and (ii) $\widehat{N} = N$. From (i), (ii) and \eqref{eq2-prop-clustering-1}, the statements of Proposition \ref{prop:clustering:1} easily follow. 


\begin{proof}[\textnormal{\textbf{Proof of (i).}}]
Suppose we are on the event $B_{n,T}$. The proof proceeds by induction on the iteration steps $r$ of the HAC algorithm. 
\vspace{7pt}

\textit{Base case} ($r=0$): In the first iteration step, the HAC algorithm merges two singleton clusters $\widehat{G}_i^{[0]} = \{ i \}$ and $\widehat{G}_j^{[0]} = \{ j \}$ with $i$ and $j$ belonging to the same group $G_k$. This is a direct consequence of \eqref{eq1-prop-clustering-1}. The algorithm thus produces a partition $\{ \widehat{G}_1^{[1]},\ldots,\widehat{G}_{n-1}^{[1]} \}$ whose elements $\widehat{G}_\ell^{[1]}$ all have the following property: $\widehat{G}_\ell^{[1]} \subseteq G_k$ for some $k$, that is, each cluster $\widehat{G}_\ell^{[1]}$ contains elements from only one group. 
\vspace{7pt}

\textit{Induction step} ($r \curvearrowright r+1$): Now suppose we are in the $r$-th iteration step for some $r < n-N$. Assume that the partition $\{\widehat{G}_1^{[r]},\ldots,\widehat{G}_{n-r}^{[r]}\}$ is such that for any $\ell$, $\widehat{G}_\ell^{[r]} \subseteq G_k$ for some $k$. Because of \eqref{eq1-prop-clustering-1}, the dissimilarity $\widehat{\Delta}(\widehat{G}_\ell^{[r]},\widehat{G}_{\ell^\prime}^{[r]})$ gets minimal for two clusters $\widehat{G}_\ell^{[r]}$ and $\widehat{G}_{\ell^\prime}^{[r]}$ with the property that $\widehat{G}_\ell^{[r]} \cup \widehat{G}_{\ell^\prime}^{[r]} \subseteq G_k$ for some $k$. Hence, the HAC algorithm produces a partition $\{ \widehat{G}_1^{[r+1]},\ldots,\widehat{G}_{n-(r+1)}^{[r+1]} \}$ whose elements $\widehat{G}_\ell^{[r+1]}$ are all such that $\widehat{G}_\ell^{[r+1]} \subseteq G_k$ for some $k$. 
\vspace{7pt}

The above induction argument shows the following: For any $r \le n - N$, the partition $\{ \widehat{G}_1^{[r]},\ldots,\widehat{G}_{n-r}^{[r]} \}$ consists of clusters $\widehat{G}_\ell^{[r]}$ which all have the property that $\widehat{G}_\ell^{[r]} \subseteq G_k$ for some $k$. This in particular holds for the partition $\{ \widehat{G}_1^{[n-N]},\ldots,\widehat{G}_N^{[n-N]} \}$, which implies that $\{ \widehat{G}_1^{[n-N]},\ldots,\widehat{G}_N^{[n-N]} \} =\{ G_1,\ldots,G_N \}$.  
\end{proof}


\begin{proof}[\textnormal{\textbf{Proof of (ii).}}]
To start with, consider any partition $\{ \widehat{G}_1^{[n-r]},\ldots,\widehat{G}_r^{[n-r]} \}$ with $r < N$ elements. Such a partition must contain at least one element $\widehat{G}_\ell^{[n-r]}$ with the following property: $\widehat{G}_\ell^{[n-r]} \cap G_k \ne \emptyset$ and $\widehat{G}_\ell^{[n-r]} \cap G_{k^\prime} \ne \emptyset$ for some $k \ne k^\prime$. On the event $B_{n,T}$, it obviously holds that $\widehat{\Delta}(S) > q_{n,T}(\alpha)$ for any $S$ with the property that $S \cap G_k \ne \emptyset$ and $S \cap G_{k^\prime} \ne \emptyset$ for some $k \ne k^\prime$. Hence, we can infer that on the event $B_{n,T}$, $\max_{1 \le \ell \le r} \widehat{\Delta} ( \widehat{G}_\ell^{[n-r]} ) > q_{n,T}(\alpha)$ for any $r < N$. 

Next consider the partition $\{ \widehat{G}_1^{[n-r]},\ldots,\widehat{G}_r^{[n-r]} \}$ with $r = N$ and suppose we are on the event $B_{n,T}$. From (i), we already know that $\{ \widehat{G}_1^{[n-N]},\ldots,\widehat{G}_N^{[n-N]} \} =\{ G_1,\ldots,G_N \}$. Moreover, it is easy to see that $\widehat{\Delta}(G_\ell) \le q_{n,T}(\alpha)$ for any $\ell$. Hence, we obtain that $\max_{1 \le \ell \le N} \widehat{\Delta} ( \widehat{G}_\ell^{[n-N]} ) = \max_{1 \le \ell \le N} \widehat{\Delta} (G_\ell) \le q_{n,T}(\alpha)$.

Putting everything together, we can conclude that on the event $B_{n,T}$, 
\[ \min \Big\{ r = 1,2,\ldots \Big| \max_{1 \le \ell \le r} \widehat{\Delta} \big( \widehat{G}_\ell^{[n-r]} \big) \le q_{n,T}(\alpha) \Big\} = N, \]
that is, $\widehat{N} = N$. 
\end{proof}



\subsection*{Proof of Proposition \ref{prop:clustering:2}}


We consider the event
\[ D_{n,T} = \Big\{ \widehat{\Phi}_{n,T} \le q_{n,T}(\alpha) \, \text{ and } \,  \min_{1 \le \ell < \ell^\prime \le N} \min_{\substack{i \in G_\ell, \\ j \in G_{\ell^\prime}}} \widehat{\Psi}_{ij,T} > q_{n,T}(\alpha) \Big\}, \]
where we write the statistic $\widehat{\Phi}_{n,T}$ as
\[ \widehat{\Phi}_{n,T} = \max_{1 \le i < j \le n} \max_{(u,h) \in \mathcal{G}_T} \Big\{ \Big| \frac{\widehat{\psi}_{ij,T}(u,h)- \ex \widehat{\psi}_{ij,T}(u,h)} {(\widehat{\sigma}_i^2 + \widehat{\sigma}_j^2)^{1/2}} \Big| - \lambda(h) \Big \}. \]
The event $D_{n,T}$ can be analysed by the same arguments as those applied to the event $B_{n,T}$ in the proof of Proposition \ref{prop-clustering-1}. In particular, analogous to \eqref{eq2-prop-clustering-1} and statements (i) and (ii) in this proof, we can show that
\begin{equation}\label{eq1-prop-clustering-2}
\pr \big( D_{n,T} \big) \ge (1-\alpha) + o(1)
\end{equation}
and 
\begin{equation}\label{eq2-prop-clustering-2}
D_{n,T} \subseteq \big\{ \widehat{N} = N \text{ and } \widehat{G}_\ell = G_\ell \text{ for all } \ell \big\}.
\end{equation}
Moreover, we have that
\begin{equation}\label{eq3-prop-clustering-2}
D_{n,T} \subseteq \bigcap_{1 \le \ell < \ell^\prime \le \widehat{N}} E_{n,T}(\ell,\ell^\prime),
\end{equation}
which is a consequence of the following observation: For all $i$, $j$ and $(u,h) \in \mathcal{G}_T$ with 
\[ \Big|\frac{\widehat{\psi}_{ij,T}(u,h) - \ex \widehat{\psi}_{ij,T}(u,h)}{(\widehat{\sigma}_i^2 + \widehat{\sigma}_j^2)^{1/2}}\Big| - \lambda(h) \le q_{n,T}(\alpha) \quad \text{and} \quad \Big|\frac{\widehat{\psi}_{ij,T}(u,h)}{(\widehat{\sigma}_i^2 + \widehat{\sigma}_j^2)^{1/2}}\Big| - \lambda(h) > q_{n,T}(\alpha), \]
it holds that $\ex[\widehat{\psi}_{ij,T}(u,h)] \ne 0$, which in turn implies that $m_i(v) - m_j(v) \ne 0$ for some $v \in I_{u,h}$. From \eqref{eq2-prop-clustering-2} and \eqref{eq3-prop-clustering-2}, we obtain that 
\[ D_{n,T} \subseteq \Big\{ \bigcap_{1 \le \ell < \ell^\prime \le \widehat{N}} E_{n,T}(\ell,\ell^\prime) \Big\} \cap \big\{ \widehat{N} = N \text{ and } \widehat{G}_\ell = G_\ell \text{ for all } \ell \big\} = E_{n,T}. \] 
This together with \eqref{eq1-prop-clustering-2} implies that $\pr(E_{n,T}) \ge (1-\alpha) + o(1)$, thus completing the proof. 



%\subsection*{Proof of Proposition \ref{theo:beta}}


%We first derive some auxiliary lemmas. To do so, we use the notation 
%$\Delta \boldsymbol{X}_{it} :=\boldsymbol{H}_i(\mathcal{U}_{it}) - \boldsymbol{H}_i(\mathcal{U}_{it-1}) =: \Delta \boldsymbol{H}_i(\mathcal{U}_{it})$
%and
%$\Delta \varepsilon_{it} := \varepsilon_{it} - \varepsilon_{it-1} = G_i(\mathcal{J}_{it}) - G_i(\mathcal{J}_{it-1}) =: \Delta G_i(\mathcal{J}_{it})$. 
%%\begin{propA}\label{propA:beta1}
%%Under Assumptions \ref{C-reg1} and \ref{C-reg3}, $\| \Delta \boldsymbol{H}_i(\mathcal{U}_{it})\|_4 < \infty$.
%%\end{propA}
%%\begin{proof}[\textnormal{\textbf{Proof of Proposition \ref{propA:beta1}}}]
%%By Assumption \ref{C-reg3} and the triangle inequality,
%%\[ \| \Delta \boldsymbol{H}_i(\mathcal{U}_{it})\|_4 \leq  \|\boldsymbol{H}_i(\mathcal{U}_{it})\|_4 +  \| \boldsymbol{H}_i(\mathcal{U}_{it-1})\|_4 < \infty. \]
%%\end{proof} 
%%\begin{lemmaA}\label{propA-reg-2}
%%Under Assumption \ref{C-reg-err1}, $\Delta \boldsymbol{X}_{it}$ (elementwise) and $\Delta \varepsilon_{it}$ are uncorrelated for each $t\in \{1, \ldots, T\}$.
%%\end{lemmaA}
%%\begin{proof}[\textnormal{\textbf{Proof of Proposition \ref{propA-reg-2}}}]
%%By Assumption \ref{C-reg-err1},
%%\begin{align*}
%%\ex [\Delta \boldsymbol{X}_{it}\Delta \varepsilon_{it}] &= \ex \big[(\boldsymbol{X}_{it} - \boldsymbol{X}_{it-1}) (\varepsilon_{it} - \varepsilon_{it-1})\big] \\
%%&=\ex[\boldsymbol{X}_{it}  \varepsilon_{it}] - \ex[\boldsymbol{X}_{it-1}  \varepsilon_{it}]- \ex[\boldsymbol{X}_{it}  \varepsilon_{it-1}] + \ex[\boldsymbol{X}_{it-1}  \varepsilon_{it-1}] \\
%%&= \ex[\boldsymbol{X}_{it}]\ex[  \varepsilon_{it}] - \ex[\boldsymbol{X}_{it-1}]\ex[  \varepsilon_{it}]- \ex[\boldsymbol{X}_{it}]\ex[  \varepsilon_{it-1}] + \ex[\boldsymbol{X}_{it-1}]\ex[  \varepsilon_{it-1}] \\
%%&= \big( \ex[\boldsymbol{X}_{it}] - \ex[\boldsymbol{X}_{it-1}]\big)\big(\ex[  \varepsilon_{it}]  - \ex[\varepsilon_{it-1}]\big) \\
%%&= \ex [\Delta \boldsymbol{X}_{it}]\ex[\Delta \varepsilon_{it}]
%%\end{align*}
%%\end{proof} 


%\begin{lemmaA}\label{propA-reg-3}
%Define the process $\Delta \boldsymbol{V}_i$ by
%\[ \Delta \boldsymbol{V}_i(\mathcal{I}_{it}) := \Delta \boldsymbol{H}_i(\mathcal{U}_{it}) \Delta G_i(\mathcal{J}_{it}).
%\]
%%where $\mathcal{I}_{it} = (\ldots, \zeta_{i, t-1}, \zeta_{it})$ with $\zeta_{it} = (u_{it}, \eta_{it})^\top$. 
%Under Assumptions \ref{C-err2}, \ref{C-err3}, \ref{C-reg3}, \ref{C-reg4} and \ref{C-reg-err2}, it holds that $\sum_{s=1}^\infty \delta_2(\Delta \boldsymbol{V}_i, s) < \infty$.
%\end{lemmaA}


%\begin{proof}[\textnormal{\textbf{Proof of Lemma \ref{propA-reg-3}}}]
%By the triangle inequality and the definition of the physical dependence measure $\delta_2$, we have that
%\begin{align*}
%\delta_2(\Delta \boldsymbol{V}_i, t) 
% & = \| \Delta\boldsymbol{V}_i(\mathcal{I}_{it}) - \Delta \boldsymbol{V}_i(\mathcal{I}_{it}^\prime) \| \\
% &= \| \Delta \boldsymbol{H}_i(\mathcal{U}_{it}) \Delta G_i(\mathcal{J}_{it}) -  \Delta \boldsymbol{H}_i(\mathcal{U}_{it}^\prime) \Delta G_i(\mathcal{J}_{it}^\prime) \|\\
%%  &= \|\boldsymbol{H}_i(\mathcal{U}_{it})G_i(\mathcal{J}_{it}) - \boldsymbol{H}_i(\mathcal{U}_{it-1})G_i(\mathcal{J}_{it}) - \boldsymbol{H}_i(\mathcal{U}_{it})G_i(\mathcal{J}_{it-1})  + \boldsymbol{H}_i(\mathcal{U}_{it-1})G_i(\mathcal{J}_{it-1})  \\
%% &\quad - \boldsymbol{H}_i(\mathcal{U}_{it}^\prime)G_i(\mathcal{J}_{it}^\prime) + \boldsymbol{H}_i(\mathcal{U}_{it-1}^\prime)G_i(\mathcal{J}_{it}^\prime) + \boldsymbol{H}_i(\mathcal{U}_{it}^\prime)G_i(\mathcal{J}_{it-1}^\prime)  - \boldsymbol{H}_i(\mathcal{U}_{it-1}^\prime)G_i(\mathcal{J}_{it-1}^\prime) \| \\
% &\leq \|\boldsymbol{H}_i(\mathcal{U}_{it})G_i(\mathcal{J}_{it}) - \boldsymbol{H}_i(\mathcal{U}_{it}^\prime)G_i(\mathcal{J}_{it}^\prime) \| \\
% &\quad + \|\boldsymbol{H}_i(\mathcal{U}_{it-1})G_i(\mathcal{J}_{it-1}) -  \boldsymbol{H}_i(\mathcal{U}_{it-1}^\prime)G_i(\mathcal{J}_{it-1}^\prime)\|  \\
% &\quad + \|\boldsymbol{H}_i(\mathcal{U}_{it-1})G_i(\mathcal{J}_{it}) -\boldsymbol{H}_i(\mathcal{U}_{it-1}^\prime)G_i(\mathcal{J}_{it}^\prime)    \| \\
% &\quad + \|\boldsymbol{H}_i(\mathcal{U}_{it})G_i(\mathcal{J}_{it-1}) -  \boldsymbol{H}_i(\mathcal{U}_{it}^\prime)G_i(\mathcal{J}_{it-1}^\prime) \|  \\
% & = \| \{ \boldsymbol{H}_i(\mathcal{U}_{it}) - \boldsymbol{H}_i(\mathcal{U}_{it}^\prime)\} G_i(\mathcal{J}_{it}) + \boldsymbol{H}_i(\mathcal{U}_{it}^\prime) \{G_i(\mathcal{J}_{it}) - G_i(\mathcal{J}_{it}^\prime)%\} \| \\
% & \quad + \| \{ \boldsymbol{H}_i(\mathcal{U}_{it-1}) - \boldsymbol{H}_i(\mathcal{U}_{it-1}^\prime)\} G_i(\mathcal{J}_{it-1}) + \boldsymbol{H}_i(\mathcal{U}_{it-1}^\prime) \{G_i(\mathcal{J}_{it-1}) - G_i(\mathcal{J}_{it-1}^\prime)\} \| \\
% &\quad + \| \{ \boldsymbol{H}_i(\mathcal{U}_{it-1}) - \boldsymbol{H}_i(\mathcal{U}_{it-1}^\prime)\} G_i(\mathcal{J}_{it}) + \boldsymbol{H}_i(\mathcal{U}_{it-1}^\prime) \{G_i(\mathcal{J}_{it}) - G_i(\mathcal{J}_{it}^\prime)\} \| \\
% &\quad +  \| \{ \boldsymbol{H}_i(\mathcal{U}_{it}) - \boldsymbol{H}_i(\mathcal{U}_{it}^\prime)\} G_i(\mathcal{J}_{it-1}) + \boldsymbol{H}_i(\mathcal{U}_{it}^\prime) \{G_i(\mathcal{J}_{it-1}) - G_i(\mathcal{J}_{it-1}^\prime)\} \| \\
% &\leq \delta_2(\boldsymbol{H}_i, t) \| G_i \| + \| \boldsymbol{H}_i \| \delta_2(G_i, t) \\
% &\quad + \delta_2(\boldsymbol{H}_i, t-1) \| G_i \| + \| \boldsymbol{H}_i \| \delta_2(G_i, t-1) \\
% &\quad + \delta_2(\boldsymbol{H}_i, t-1) \| G_i \| + \| \boldsymbol{H}_i \| \delta_2(G_i, t) \\
% &\quad + \delta_2(\boldsymbol{H}_i, t) \| G_i \| + \| \boldsymbol{H}_i \| \delta_2(G_i, t-1), 
%%  &\quad +\|\big(\boldsymbol{H}_i(\mathcal{U}_{it-1}) - \boldsymbol{H}_i(\mathcal{U}_{it-1}^\prime)\big) G_i(\mathcal{J}_{it})\| +  \|\boldsymbol{H}_i(\mathcal{U}_{it-1}^\prime)\big(G_i(\mathcal{J}_{it}) - G_i(\mathcal{J}_{it}^\prime)\big)    \|\\
%% &\quad + \|\big(\boldsymbol{H}_i(\mathcal{U}_{it}) -\boldsymbol{H}_i(\mathcal{U}_{it}^\prime)\big)G_i(\mathcal{J}_{it-1})\| + \|\boldsymbol{H}_i(\mathcal{U}_{it}^\prime)\big(G_i(\mathcal{J}_{it-1}) -G_i(\mathcal{J}_{it-1}^\prime)\big) \|  \\
%% &\leq \delta_2(\boldsymbol{V}_i, t) + \delta_2(\boldsymbol{V}_i, t-1)  \\
%%&\quad + \big(\delta_2(\boldsymbol{H}_i, t-1) +  \delta_2(\boldsymbol{H}_i, t)\big) \|G_i \| + \big( \delta_2(G_i, t-1) +  \delta_2(G_i, t)\big)\|\boldsymbol{H}_i \|.
%\end{align*}
%where $\mathcal{U}_{it}^\prime  = (\ldots, u_{i(-1)}, u^\prime_{i0}, u_{i1}, \ldots, u_{it-1}, u_{it})$, $\mathcal{U}_{i(t-1)}^\prime  = (\ldots, u_{i(-1)}, u^\prime_{i0}, u_{i1}, \ldots, u_{it-1})$, $\mathcal{J}_{it}^\prime  = (\ldots, \eta_{i(-1)}, \eta^\prime_{i0}, \eta_{i1}, \ldots, \eta_{it-1}, \eta_{it})$, $\mathcal{J}_{i(t-1)}^\prime  = (\ldots, \eta_{i(-1)}, \eta^\prime_{i0}, \eta_{i1}, \ldots, \eta_{it-1})$ are coupled processes with $u_{i0}^\prime$ being an i.i.d.\ copy of $u_{i0}$ and $\eta_{i0}^\prime$ being an i.i.d.\ copy of $\eta_{i0}$. From this and Assumptions ??, it immediately follows that $\sum_{s=1}^\infty \delta_2(\Delta \boldsymbol{V}_i, s) \le C < \infty$. 
%\end{proof}


%\begin{lemmaA}\label{propA:beta4}
%Under Assumptions \ref{C-err1}--\ref{C-reg-err2},
%\[ \Big| \frac{1}{\sqrt{T}}\sum_{t=2}^T \Delta \boldsymbol{X}_{it}\Delta \varepsilon_{it} \Big| = O_p(1).
%\]
%\end{lemmaA}


%\begin{proof}[\textnormal{\textbf{Proof of Lemma \ref{propA:beta4}.}}]
%Let
%\[ \kappa_{i, s}^{\mathcal{P}} = \frac{1}{T-1}\sum_{t=2}^T \mathcal{P}_{i,t-s}\big( \Delta \boldsymbol{X}_{it}\Delta \varepsilon_{it}\big) \quad \text{with} \quad \mathcal{P}_{i,t-s}(\cdot) = \ex[ \, \cdot \, |\mathcal{I}_{i(t-s)}] -\ex[ \, \cdot \, |\mathcal{I}_{i(t-s-1)}] \]
%for $s \ge 0$. Since $\ex[ \Delta \boldsymbol{X}_{it}\Delta \varepsilon_{it}] = 0$ by Assumption \ref{C-reg-err1}, it holds that 
%\begin{align*}
%\frac{1}{T-1}\sum_{t=2}^T \Delta \boldsymbol{X}_{it}\Delta \varepsilon_{it} 
%&= \frac{1}{T-1}\sum_{t=2}^T \big( \ex [ \Delta \boldsymbol{X}_{it}\Delta \varepsilon_{it}|\mathcal{I}_{it} ] - \ex [ \Delta\boldsymbol{X}_{it}\Delta \varepsilon_{it}] \big) \\
%&= \frac{1}{T-1}\sum_{t=2}^T \sum_{s=0}^\infty \big( \ex [ \Delta \boldsymbol{X}_{it}\Delta \varepsilon_{it}|\mathcal{I}_{i(t-s)} ] - \ex [ \Delta \boldsymbol{X}_{it}\Delta \varepsilon_{it}|\mathcal{I}_{i(t-s-1)} ] \big) \\
%&= \frac{1}{T-1}\sum_{t=2}^T \sum_{s=0}^\infty \mathcal{P}_{i, t-s} (\Delta \boldsymbol{X}_{it}\Delta \varepsilon_{it}) \\
%&= \sum_{s=0}^\infty \kappa_{i, s}^{\mathcal{P}}.
%\end{align*}
%In what follows, we show that 
%\begin{equation}\label{propA:beta4:claim}
%\Big\| \frac{1}{T-1}\sum_{t=2}^T \Delta \boldsymbol{X}_{it}\Delta \varepsilon_{it} \Big\| \le \sum_{s=0}^\infty \big\| \kappa_{i, s}^{\mathcal{P}} \big\| = O\Big(\frac{1}{\sqrt{T}}\Big), 
%\end{equation}
%which implies the statement of the lemma. 


%To prove \eqref{propA:beta4:claim}, we derive an upper bound on $\| \kappa_{i, s}^{\mathcal{P}} \|^2$. As the projection operator $\mathcal{P}_{i, t}(\cdot)$ has the property that $\ex[ \mathcal{P}_{i, t-s}(\Delta \boldsymbol{X}_{it} \Delta \varepsilon_{it}) \mathcal{P}_{i, t^\prime-s}(\Delta \boldsymbol{X}_{it^\prime} \Delta \varepsilon_{it^\prime}) = 0$ for $t \ne t^\prime$, we get that 
%\begin{align*}
%\|\kappa_{i, s}^{\mathcal{P}}\|^2 
%&\leq \frac{1}{(T-1)^2} \sum_{t=2}^T \Big\| \mathcal{P}_{i, t-s}\big( \Delta \boldsymbol{X}_{it}\Delta \varepsilon_{it}\big) \Big\|^2\\
%&= \frac{1}{(T-1)^2} \sum_{t=2}^T \Big\| \ex \big(\Delta \boldsymbol{X}_{it}\Delta \varepsilon_{it}|\mathcal{I}_{i(t-s)}\big) -\ex \big(\Delta \boldsymbol{X}_{it}\Delta \varepsilon_{it}|\mathcal{I}_{i(t-s-1)}\big) \Big\|^2\\
%&= \frac{1}{(T-1)^2} \sum_{t=2}^T \Big\| \ex \big(\Delta \boldsymbol{X}_{it}\Delta \varepsilon_{it}|\mathcal{I}_{i(t-s)}\big) -\ex \big(\Delta \boldsymbol{X}_{it, s}^\prime\Delta \varepsilon_{it, s}^\prime|\mathcal{I}_{i(t-s)}\big) \Big\|^2,
%\end{align*}
%where $\Delta \boldsymbol{X}_{it, s}^\prime\Delta \varepsilon_{it, s}^\prime$ denotes $\Delta \boldsymbol{X}_{it}\Delta \varepsilon_{it}$ with $\zeta_{i, t-s}$ replaced by an i.i.d.\ copy $\zeta_{i, t-s}^\prime$. By Jensen's inequality and the definition of the norm $\|\cdot\| = (\ex|\cdot|^2)^{1/2}$, it further follows that 
%\begin{align*}
%\|\kappa_{i, s}^{\mathcal{P}}\|^2 &\leq \frac{1}{(T-1)^2} \sum_{t=2}^T \Big\| \ex (\Delta \boldsymbol{X}_{it}\Delta \varepsilon_{it}|\mathcal{I}_{i(t-s)}) -\ex (\Delta \boldsymbol{X}_{it, s}^\prime\Delta \varepsilon_{it, s}^\prime|\mathcal{I}_{i(t-s)}) \Big\|^2 \\
%%&\leq \frac{1}{(T-1)^2} \sum_{t=2}^T \ex \bigg[ \Big\| \Delta \boldsymbol{X}_{it}\Delta \varepsilon_{it} -\Delta \boldsymbol{X}_{it, s}^\prime\Delta \varepsilon_{it, s}^\prime\Big\|^2\bigg| |\mathcal{I}_{i(t-s)}\bigg] \\
%&\le \frac{1}{(T-1)^2} \sum_{t=2}^T \Big\| \Delta \boldsymbol{X}_{it}\Delta \varepsilon_{it} -\Delta \boldsymbol{X}_{it, s}^\prime\Delta \varepsilon_{it, s}^\prime\Big\|^2 \\
%%& = \frac{1}{(T-1)^2} \sum_{t=2}^T \Big\| \Delta \boldsymbol{H}_i(\mathcal{U}_{it})  \Delta G_i(\mathcal{J}_{it}) - \Delta \boldsymbol{H}_i(\mathcal{U}_{it, s}^\prime)  \Delta G_i(\mathcal{J}_{it, s}^\prime)\Big\|^2\\
%& = \frac{1}{(T-1)^2} \sum_{t=2}^T \Big\| \Delta \boldsymbol{V}_i(\mathcal{I}_{it})  - \Delta \boldsymbol{V}_i(\mathcal{I}_{it, s}^\prime) \Big\|^2 \\
%& \leq \frac{1}{(T-1)^2} \sum_{t=2}^T \delta_2^2(\Delta \boldsymbol{V}_i, s)
% = \frac{1}{T-1}\delta_2^2(\Delta \boldsymbol{V}_i, s)
%\end{align*}
%%with $\mathcal{U}_{it, s}^\prime = (\ldots, u_{i(t-s-1)}, u^\prime_{i(t-s)}, u_{i(t-s+1)}, \ldots, u_{it})$, $u_{i(t-s)}^\prime$ being an i.i.d.\ copy of $u_{i(t-s)}$, $\mathcal{J}_{it, s}^\prime = (\ldots, \eta_{i(t-s-1)}, \eta^\prime_{i(t-s)}, \eta_{i(t-s+1)}, \ldots, \eta_{it})$, $\eta_{i(t-s)}^\prime$ being an i.i.d.\ copy of $\eta_{i(t-s)}$, and $\zeta^\prime_{it} = (u_{it}^\prime, \eta_{it}^\prime)^\top$ and 
%with $\mathcal{I}_{it,s}^\prime =(\ldots, \zeta_{i(t-s-1)}, \zeta^\prime_{i(t-s)}, \zeta_{i(t-s+1)}, \ldots, \zeta_{it})$ and $\zeta^\prime_{it} = (u_{it}^\prime, \eta_{it}^\prime)^\top$an i.i.d.\ copy of $\zeta_{it} = (u_{it}, \eta_{it})^\top$. Together with Lemma \ref{propA-reg-3}, this yields that $\sum_{s=0}^\infty \| \kappa_{i, s}^{\mathcal{P}} \| = O(1/\sqrt{T})$, which completes the proof. 
%\end{proof}


%%Define $\Delta m_{it} = m_i \left( \frac{t}{T} \right) - m_i \left(\frac{t-1}{T}\right)$. Then, by Assumption \ref{C-reg1}, we can rewrite the first-differenced regressors $\Delta  \boldsymbol{X}_{it}$ as
%%\[ \Delta \boldsymbol{X}_{it} =\boldsymbol{H}_i(\mathcal{U}_{it}) - \boldsymbol{H}_i(\mathcal{U}_{it-1}) := \Delta \boldsymbol{H}_i(\mathcal{U}_{it}) \]
%%with $\Delta \boldsymbol{H}_i(\mathcal{U}_{it}) := (\Delta H_{i1}, \Delta H_{i2}, \ldots, \Delta H_{id})^\top$. Similarly, by Assumption \ref{C-err1}, we have
%%\[\Delta \varepsilon_{it} = \varepsilon_{it} - \varepsilon_{it-1} = G_i(\mathcal{J}_{it}) - G_i(\mathcal{J}_{it-1}) = \Delta G_i(\mathcal{J}_{it}).\]


%We now turn to the proof of Proposition \ref{theo:beta}. The estimator $\widehat{\bfbeta}_i$ can be written as
%\begin{align*}
%\widehat{\bfbeta}_i &= \Big( \sum_{t=2}^T \Delta \boldsymbol{X}_{it} \Delta \boldsymbol{X}_{it}^\top \Big)^{-1} \sum_{t=2}^T \Delta \boldsymbol{X}_{it} \Delta Y_{it} \\
%& =  \Big( \sum_{t=2}^T \Delta \boldsymbol{X}_{it} \Delta \boldsymbol{X}_{it}^\top \Big)^{-1} \sum_{t=2}^T \Delta \boldsymbol{X}_{it} \bigg(\Delta \boldsymbol{X}_{it}^\top \bfbeta_i +  \Delta m_{it}+ \Delta \varepsilon_{it} \bigg) \\
%&= \bfbeta_i +   \Big( \sum_{t=2}^T \Delta \boldsymbol{X}_{it} \Delta \boldsymbol{X}_{it}^\top \Big)^{-1} \sum_{t=2}^T \Delta \boldsymbol{X}_{it} \Delta m_{it} +  \Big( \sum_{t=2}^T \Delta \boldsymbol{X}_{it} \Delta \boldsymbol{X}_{it}^\top \Big)^{-1} \sum_{t=2}^T \Delta \boldsymbol{X}_{it} \Delta \varepsilon_{it}, 
%\end{align*}
%where $\Delta \boldsymbol{X}_{it} = X_{it} - X_{it-1}$, $\Delta \varepsilon_{it} = \varepsilon_{it} - \varepsilon_{it-1}$ and $\Delta m_{it} = m_i (\frac{t}{T}) - m_i(\frac{t-1}{T})$. Hence, 
%\begin{align}
% \sqrt{T}( \widehat{\bfbeta}_i - \bfbeta_i) = &\Big( \frac{1}{T}\sum_{t=2}^T \Delta \boldsymbol{X}_{it} \Delta \boldsymbol{X}_{it}^\top \Big)^{-1} \frac{1}{\sqrt{T}}\sum_{t=2}^T \Delta \boldsymbol{X}_{it} \Delta m_{it} \nonumber \\
%&\quad+  \Big(\frac{1}{T} \sum_{t=2}^T \Delta \boldsymbol{X}_{it} \Delta \boldsymbol{X}_{it}^\top \Big)^{-1}\frac{1}{\sqrt{T}} \sum_{t=2}^T \Delta \boldsymbol{X}_{it} \Delta \varepsilon_{it}. \label{theo:beta:proof1}
%\end{align}
%In what follows, we show that 
%\begin{align}
%\frac{1}{\sqrt{T}} \sum_{t=2}^T \Delta \boldsymbol{X}_{it} \Delta \varepsilon_{it} & = O_p(1) \label{theo:beta:claim1} \\
%\Big( \frac{1}{T}\sum_{t=2}^T \Delta \boldsymbol{X}_{it} \Delta \boldsymbol{X}_{it}^\top \Big)^{-1} & = O_p(1) \label{theo:beta:claim2} \\
%\frac{1}{\sqrt{T}} \sum_{t=2}^T \Delta \boldsymbol{X}_{it} \Delta m_{it} & = O_p\Big(\frac{1}{\sqrt{T}}\Big). \label{theo:beta:claim3}
%\end{align} 
%Theorem \ref{theo:beta} follows from applying these three statements together with standard arguments to formula \eqref{theo:beta:proof1}.


%It remains to prove \eqref{theo:beta:claim1}--\eqref{theo:beta:claim3}. Equation \eqref{theo:beta:claim1} has already been verified in Lemma \ref{propA:beta4}. By essentially the same arguments as those in the proof of Lemma \ref{propA:beta4}, we obtain that 
%\[ \frac{1}{T}\sum_{t=2}^T \big\{ \Delta \boldsymbol{X}_{it} \Delta \boldsymbol{X}_{it}^\top - \ex [\Delta \boldsymbol{X}_{it} \Delta \boldsymbol{X}_{it}^\top] \big\} = o_p(1). \]
%Since the matrix $\ex [\Delta \boldsymbol{X}_{it} \Delta \boldsymbol{X}_{it}^\top]$ is invertible by assumption, \eqref{theo:beta:claim1} follows from an application of Slutsky's lemma. We finally turn to the proof of \eqref{theo:beta:claim3}. We write $T^{-1/2} \sum_{t=2}^T \Delta \boldsymbol{X}_{it} \Delta m_{it} = (T^{-1/2} \sum_{t=2}^T \Delta \boldsymbol{X}_{it,j} \Delta m_{it})_{j=1,\ldots,d}$ and consider each component separately. By assumption, $m_i(\cdot)$ is Lipschitz continuous, which implies that $|\Delta m_{it}| = \left|m_i \left( \frac{t}{T} \right) - m_i \left(\frac{t-1}{T}\right) \right| \leq C \frac{1}{T}$ for all $t \in \{1, \ldots, T\}$ and some constant $C > 0$. Hence, 
%\begin{align*}
%\Big| \frac{1}{\sqrt{T}}\sum_{t=2}^T \Delta X_{it,j} \Delta m_{it}\Big| &\leq \frac{1}{\sqrt{T}}\sum_{t=2}^T \big|\Delta X_{it,j} \big| \cdot \big| \Delta m_{it} \big| \\%
%	& \leq \frac{C}{\sqrt{T}} \cdot \frac{1}{T} \sum_{t=2}^T \left|\Delta X_{it,j} \right| = O_p\Big(\frac{1}{\sqrt{T}}\Big),
%\end{align*}
%where we have used that $T^{-1} \sum_{t=2}^T |\Delta \boldsymbol{X}_{it,j}| = O_p(1)$ by Markov's inequality.



