\documentclass[a4paper,11pt]{article}
\usepackage{amsmath, bm}
\usepackage{amssymb,amsthm,graphicx}
\usepackage{enumitem}
\usepackage{color}
\usepackage{epsfig}
\usepackage{graphics}
\usepackage{pdfpages}
\usepackage{subcaption}
\usepackage[font=small]{caption}
\usepackage[hang,flushmargin]{footmisc} 
\usepackage{float}
\usepackage{rotating,tabularx}
\usepackage{booktabs}
\usepackage[mathscr]{euscript}
\usepackage{natbib}
\usepackage{setspace}
\usepackage{placeins}
\usepackage{ulem}
\usepackage[left=3cm,right=3cm,bottom=3cm,top=3cm]{geometry}
\numberwithin{equation}{section}
\allowdisplaybreaks[3]

\input{macros}



\begin{document}



\heading{Clustering of the epidemic time trends:}{the case of COVID-19}

%\authors{Marina Khismatullina\renewcommand{\thefootnote}{1}\footnotemark[1]}{University of Bonn}{Michael Vogt\renewcommand{\thefootnote}{2}\footnotemark[2]}{Ulm University} 
%\footnotetext[1]{Corresponding author. Address: Bonn Graduate School of Economics, University of Bonn, 53113 Bonn, Germany. Email: \texttt{marina.k@uni-bonn.de}.}
%\renewcommand{\thefootnote}{2}
%\footnotetext[2]{Address: Institute of Statistics, Department of Mathematics and Economics, Ulm University, 89081 Ulm, Germany. Email: \texttt{m.vogt@uni-ulm.de}.}
%\renewcommand{\thefootnote}{\arabic{footnote}}
%\setcounter{footnote}{2}

%\vspace{-0.85cm}

%\renewcommand{\baselinestretch}{1.2}\normalsize

%\renewcommand{\abstractname}{}
%\begin{abstract}
%\noindent The COVID-19 pandemic is one of the most pressing issues at present. A question which is particularly important for governments and policy makers is the following: Does the virus spread in the same way in different countries? Or are there significant differences in the development of the epidemic? In this paper, we devise new inference methods that allow to detect differences in the development of the COVID-19 epidemic across countries in a statistically rigorous way. In our empirical study, we use the methods to compare the outbreak patterns of the epidemic in a number of European countries.
%\end{abstract}

%\noindent \textbf{Key words:} simultaneous hypothesis testing; multiscale test; time trend; panel data; COVID-19.

%\noindent \textbf{JEL classifications:} C12; C23; C54.

%\noindent \textbf{AMS 2010 subject classifications:} 62E20; 62G10; 62G15; 62G20.

\renewcommand{\baselinestretch}{1.5}\normalsize



%\section{Introduction}


%\section{Extensions}
We consider the following nonparametric regression equation:
\begin{equation*}
\X_{it} = c_i \lambda_i\Big(\frac{t}{T}\Big) + \varepsilon_{it} \quad \text{with} \quad \varepsilon_{it} = \sigma \sqrt{\lambda_i\Big(\frac{t}{T}\Big)} \eta_{it}, 
\end{equation*}
where $c_i$ is the country-specific scaling parameter that accounts for the size of the country or population density. We introduce this additional parameter in order to be able to compare countries that differ substantially in terms of the population, i.e. Luxembourg and Russia.  In what follows, we present a method that allows researchers to test the hypothesis that the time trends of new COVID-19 cases in different countries are the same up to some scaling parameter and to cluster the countries based on the differences.

For the identification purposes, we need to assume that for each $i \in \mathcal{C}$ we have $\int_0^1 \lambda_i(u)du = 1$. Only then we are able to estimate the scaling parameter $c_i$. Thus, the testing procedure is as follows.

\textit{Step 1}

First, we estimate the scaling parameter:
\begin{align*}
\widehat{c_i} &= \frac{1}{T}\sum_{t = 1}^T X_{it} \\
&= c_i \frac{1}{T}\sum_{t = 1}^T \lambda_i\Big(\frac{t}{T}\Big) + \sigma\frac{1}{T}\sum_{t = 1}^T \sqrt{\lambda_i\Big(\frac{t}{T}\Big)} \eta_{it}\\
& = c_i \frac{1}{T}\sum_{t = 1}^T \lambda_i\Big(\frac{t}{T}\Big) + o_P(1)\\
& = c_i + o_P(1),
\end{align*}
where in the last inequality we used the normalization $\int_0^1 \lambda_i(u)du = 1$. Hence, for any fixed $i \in \mathcal{C}$, $\widehat{c}_i$ is a consistent estimator of $c_i$.

\textit{Step 2}

Instead of working with $X_{it}$, we consider the following variables:
\begin{align*}
X^*_{it} &= \frac{X_{it}}{\frac{1}{T}\sum_{t = 1}^T X_{it}} \\
&= \frac{c_i}{\widehat{c}_i} \lambda_i \Big(\frac{t}{T}\Big) + \frac{\sigma}{\widehat{c}_i} \sqrt{\lambda_i\Big(\frac{t}{T}\Big)} \eta_{it}.
\end{align*}

A statistic to test the hypothesis $H_0^{(ijk)}$ for a given triple $(i,j,k)$ is then constructed as follows. We work with the following quantity
\[ \hat{s}_{ijk,T} = \frac{1}{\sqrt{Th_k}} \sum\limits_{t=1}^T \ind\Big(\frac{t}{T} \in \mathcal{I}_k\Big) (\X_{it}^* - \X_{jt}^*). \]
Then
\begin{align*}
\frac{\hat{s}_{ijk,T}}{\sqrt{Th_k}} =& \frac{1}{Th_k} \sum\limits_{t=1}^T \ind\Big(\frac{t}{T} \in \mathcal{I}_k\Big) (\X_{it}^* - \X_{jt}^*)\\
=& \frac{1}{Th_k} \sum\limits_{t=1}^T \ind\Big(\frac{t}{T} \in \mathcal{I}_k\Big) \bigg( \lambda_i \Big(\frac{t}{T}\Big)  - \lambda_j \Big(\frac{t}{T}\Big)\bigg) + R_1 + R_2,
\end{align*}
where
\begin{align*}
R_1 &= \frac{1}{Th_k} \sum\limits_{t=1}^T \ind\Big(\frac{t}{T} \in \mathcal{I}_k\Big) \bigg( \Big(\frac{c_i}{\widehat{c}_i} - 1 \Big) \lambda_i \Big(\frac{t}{T}\Big)  - \Big(\frac{c_j}{\widehat{c}_j} - 1 \Big) \lambda_j \Big(\frac{t}{T}\Big)\bigg),\\
R_2& =  \frac{1}{Th_k} \sum\limits_{t=1}^T \ind\Big(\frac{t}{T} \in \mathcal{I}_k\Big) \Big( \frac{\sigma}{\widehat{c}_i} \sqrt{\lambda_i\Big(\frac{t}{T}\Big)} \eta_{it} - \frac{\sigma}{\widehat{c}_j} \sqrt{\lambda_j\Big(\frac{t}{T}\Big)} \eta_{jt} \Big).
\end{align*}
Since $\widehat{c}_i = c_i + o_P(1)$ and $0 \leq  \sum\nolimits_{t=1}^T \ind\big(\frac{t}{T} \in \mathcal{I}_k\big) \lambda_i \big(\frac{t}{T}\big) \leq h_k \lambda_{max}$, we have
\begin{align}\label{eq:aux1}
|R_1| &\leq \Big|\frac{c_i}{\widehat{c}_i} - 1 \Big| \frac{1}{Th_k} \sum\limits_{t=1}^T \ind\Big(\frac{t}{T} \in \mathcal{I}_k\Big) \lambda_i \Big(\frac{t}{T}\Big)  + \Big|\frac{c_j}{\widehat{c}_j} - 1 \Big| \frac{1}{Th_k} \sum\limits_{t=1}^T \ind\Big(\frac{t}{T} \in \mathcal{I}_k\Big) \lambda_j \Big(\frac{t}{T}\Big),\nonumber \\
&\leq o_P(1) \cdot \frac{\lambda_{max}}{T} + o_P(1) \cdot \frac{\lambda_{max}}{T} = o_P\Big(\frac{1}{T}\Big).
\end{align}
Furthermore, applying the law of large numbers, we get:
\begin{align*}
 \frac{1}{Th_k} \sum\limits_{t=1}^T \ind\Big(\frac{t}{T} \in \mathcal{I}_k\Big) \sqrt{\lambda_i\Big(\frac{t}{T}\Big)} \eta_{it}  = o_P(1).
\end{align*}
Hence, if we uniformly bound the scaling parameters away from 0, i.e. $\exists \, c_{min}$ such that for all $i \in \mathcal{C}$ we have $0 < c_{min} \leq c_i$, we can use the fact that $\frac{\sigma}{\widehat{c}_i} = O_P(1)$ to get that
\begin{align}\label{eq:aux2}
R_2& =  \frac{\sigma}{\widehat{c}_i} \frac{1}{Th_k} \sum\limits_{t=1}^T \ind\Big(\frac{t}{T} \in \mathcal{I}_k\Big) \sqrt{\lambda_i\Big(\frac{t}{T}\Big)} \eta_{it} -\frac{\sigma}{\widehat{c}_j}\frac{1}{Th_k} \sum\limits_{t=1}^T \ind\Big(\frac{t}{T} \in \mathcal{I}_k\Big)  \sqrt{\lambda_j\Big(\frac{t}{T}\Big)} \eta_{jt}\nonumber\\
& = o_P(1).
\end{align}
Combining \eqref{eq:aux1} and \eqref{eq:aux2} together, we get $\hat{s}_{ijk,T}/\sqrt{Th_k} = (Th_k)^{-1} \sum_{t=1}^T \ind(t/T \in \mathcal{I}_k) \{\lambda_i(t/T) - \lambda_j(t/T)\} + o_p(1)$ for any fixed pair of countries $(i,j)$. Hence, the statistic $\hat{s}_{ijk,T}/\sqrt{Th_k}$ estimates the average distance between the functions $\lambda_i$ and $\lambda_j$ on the interval $\mathcal{I}_k$. The variance of $\hat{s}_{ijk,T}$ can not be easily calculated:
\begin{align*}
 \var(\hat{s}_{ijk,T})  =&\frac{1}{Th_k} \var \Big( \sum\limits_{t=1}^T \ind\Big(\frac{t}{T} \in \mathcal{I}_k\Big) (X_{it}^* - X_{jt}^*) \Big)\\
=&\frac{1}{Th_k} \var \Big( \sum\limits_{t=1}^T \ind\Big(\frac{t}{T} \in \mathcal{I}_k\Big) X_{it}^*\Big) + \frac{1}{Th_k} \var \Big( \sum\limits_{t=1}^T \ind\Big(\frac{t}{T} \in \mathcal{I}_k\Big) X_{jt}^*\Big)\\
 = &\frac{1}{Th_k} \var \bigg( \frac{\sum\nolimits_{t=1}^T \ind\big(\frac{t}{T} \in \mathcal{I}_k\big) X_{it}}{\frac{1}{T}\sum\nolimits_{t=1}^T X_{it}} \bigg) + \frac{1}{Th_k} \var \bigg( \frac{\sum\nolimits_{t=1}^T \ind\big(\frac{t}{T} \in \mathcal{I}_k\big) X_{jt}}{\frac{1}{T}\sum\nolimits_{t=1}^T X_{jt}} \bigg),
\end{align*}
hence, we "normalize" $\hat{s}_{ijk,T}$ intuitively by dividing it by the following value:
\[ \hat{\nu}_{ijk,T}^2 = \frac{\hat{\sigma}^2}{Th_k} \sum\limits_{t=1}^T \ind\Big(\frac{t}{T} \in \mathcal{I}_k\Big) \{ \X^*_{it} + \X^*_{jt} \}.\]

{\color{red} Instead look at the bootstrap!}

Normalizing the statistic $\hat{s}_{ijk,T}$ by the estimator $\hat{\nu}_{ijk,T}$ yields the expression 
\begin{equation*}
\hat{\psi}_{ijk,T} := \frac{\hat{s}_{ijk,T}}{\hat{\nu}_{ijk,T}} = \frac{\sum\nolimits_{t=1}^T \ind(\frac{t}{T} \in \mathcal{I}_k) (\X_{it} - \X_{jt})}{ \hat{\sigma}\{ \sum\nolimits_{t=1}^T \ind(\frac{t}{T} \in \mathcal{I}_k) (\X^*_{it} + \X^*_{jt}) \}^{1/2}}, 
\end{equation*}
which serves as our test statistic of the hypothesis $H_0^{(ijk)}$. For later reference, we additionally introduce the statistic 
\begin{equation*}
\hat{\psi}_{ijk,T}^{ 0} = \frac{\sum\nolimits_{t=1}^T \ind(\frac{t}{T} \in \mathcal{I}_k) \Big(\big( \frac{c_i}{\hat{c}_i} - \frac{c_j}{\hat{c}_j} \big)\overline{\lambda}_{ij} + \overline{\lambda}_{ij}^{1/2}(\frac{t}{T}) (\frac{\sigma}{\hat{c}_i} \eta_{it} - \frac{\sigma}{\hat{c}_j} \eta_{jt}) \Big)}{ \hat{\sigma} \{ \sum\nolimits_{t=1}^T \ind(\frac{t}{T} \in \mathcal{I}_k) (\X^*_{it} + \X^*_{jt}) \}^{1/2}}
\end{equation*}
with $\overline{\lambda}_{ij}(u) = \{ \lambda_i(u) + \lambda_j(u) \}/2$, which is identical to $\hat{\psi}_{ijk,T}$ under $H_0^{(ijk)}$. 


%where $(\hat{\sigma}^*)^2$ is defined as follows: For each country $i$, let 
%\begin{align*}
%(\hat{\sigma}_i^*)^2 = \frac{\sum_{t=2}^T (\X^*_{it}-\X^*_{it-1})^2}{2 \sum_{t=1}^T \X^*_{it}} = \frac{\sum_{t=2}^T (\frac{\X_{it}}{\widehat{c}_i}-\frac{\X_{it-1}}{\widehat{c}_i})^2}{2 \sum_{t=1}^T \frac{\X_{it}}{\widehat{c}_i}} = \frac{\widehat{\sigma}_i^2}{\widehat{c}_i}
%\end{align*}
%and set $(\widehat{\sigma}^*)^2 = |\countries|^{-1} \sum_{i \in \countries} (\widehat{\sigma}^*_i)^2$. We have already shown that $\widehat{\sigma}_i^2 = \sigma^2 + o_p(1)$ and $\hat{c}_i = c_i + o_P(1)$ for any $i$ and thus $(\widehat{\sigma}_i^*)^2 = \frac{\sigma^2}{c_i} + o_p(1)$.


\subsection{Construction of the test} 


Our multiscale test is carried out as follows: For a given significance level $\alpha \in (0,1)$ and each $(i,j,k) \in \indexset$, we reject $H_0^{(ijk)}$ if 
\[ |\hat{\psi}_{ijk,T}| > c_{ijk,T}(\alpha), \]
where $c_{ijk,T}(\alpha)$ is the critical value for the $(i,j,k)$-th test problem. The critical values $c_{ijk,T}(\alpha)$ are chosen such that the familywise error rate (FWER) is controlled at level $\alpha$, which is defined as the probability of wrongly rejecting $H_0^{(ijk)}$ for at least one $(i,j,k)$. More formally speaking, for a given significance level $\alpha \in (0,1)$, the FWER is 
\begin{align*}
\text{FWER}(\alpha) 
 & = \pr \Big( \exists (i,j,k) \in \indexset_0: |\hat{\psi}_{ijk,T}| > c_{ijk,T}(\alpha) \Big) \\
 & =  1 - \pr \Big( \forall (i,j,k) \in \indexset_0: |\hat{\psi}_{ijk,T}| \le c_{ijk,T}(\alpha) \Big) \\
 & = 1 - \pr\Big( \max_{(i,j,k) \in \indexset_0} |\hat{\psi}_{ijk,T}| \le c_{ijk,T}(\alpha) \Big), 
\end{align*}
where $\indexset_0 \subseteq \indexset$ is the set of triples $(i,j,k)$ for which $H_0^{(ijk)}$ holds true. As before, the critical values are chosen as
\begin{equation*}
c_{ijk,T}(\alpha) = c_T(\alpha,h_k) := b_k + q_T(\alpha)/a_k, 
\end{equation*}
where $a_k = \{\log(e/h_k)\}^{1/2} / \log \log(e^e / h_k)$ and $b_k = \sqrt{2 \log(1/h_k)}$ are scale-dependent constants and the quantity $q_T(\alpha)$ is determined by the following consideration: Since 
\begin{align*}
\text{FWER}(\alpha)
  & = \pr \Big( \exists (i,j,k) \in \indexset_0: |\hat{\psi}_{ijk,T}| > c_T(\alpha,h_k) \Big)  \\
 & =  1 - \pr \Big( \forall (i,j,k) \in \indexset_0: |\hat{\psi}_{ijk,T}| \le c_T(\alpha,h_k) \Big) \\
 & =  1 - \pr \Big( \forall (i,j,k) \in \indexset_0: a_k \big(|\hat{\psi}_{ijk,T}| - b_k\big) \le q_T(\alpha) \Big) \\
 & = 1 - \pr\Big( \max_{(i,j,k) \in \indexset_0} a_k \big( |\hat{\psi}_{ijk,T}| - b_k \big) \le q_T(\alpha) \Big),
\end{align*}
we need to choose the quantity $q_T(\alpha)$ as the $(1-\alpha)$-quantile of the statistic 
\[ \hat{\Psi}_T = \max_{(i,j,k) \in \indexset} a_k \big( |\hat{\psi}_{ijk,T}^{0}| - b_k \big) \]
in order to ensure control of the FWER at level $\alpha$. As the quantiles $q_T(\alpha)$ are not known in practice, we cannot compute the critical values $c_T(\alpha,h_k)$ exactly in practice but need to approximate them. This can be achieved as follows: Under appropriate regularity conditions, it can be shown that \textcolor{red}{???}
\begin{align*}
\hat{\psi}_{ijk,T}^{0} &= \frac{\sum\nolimits_{t=1}^T \ind(\frac{t}{T} \in \mathcal{I}_k) \Big(\big( \frac{c_i}{\hat{c}_i} - \frac{c_j}{\hat{c}_j} \big)\overline{\lambda}_{ij} + \overline{\lambda}_{ij}^{1/2}(\frac{t}{T}) (\frac{\sigma}{\hat{c}_i} \eta_{it} - \frac{\sigma}{\hat{c}_j} \eta_{jt}) \Big)}{ \hat{\sigma} \{ \sum\nolimits_{t=1}^T \ind(\frac{t}{T} \in \mathcal{I}_k) (\X^*_{it} + \X^*_{jt}) \}^{1/2}}\\
 & \approx \frac{1}{\sqrt{2Th_k}} \sum\limits_{t=1}^T \ind\Big(\frac{t}{T} \in \mathcal{I}_k\Big) \Big\{ \frac{\eta_{it}}{\hat{c}_i} - \frac{\eta_{jt}}{\hat{c}_j} \Big\}.
\end{align*} 
A Gaussian version of the statistic displayed in the final line above is given by 
\begin{equation*}
\phi_{ijk,T} = \frac{1}{\sqrt{2Th_k}} \sum\limits_{t=1}^T \ind\Big(\frac{t}{T} \in \mathcal{I}_k\Big) \Big\{ \frac{Z_{it}}{c_i} - \frac{Z_{jt}}{c_j} \Big\},
\end{equation*}
where $Z_{it}$ are independent standard normal random variables for $1 \le t \le T$ and $1 \le i \le n$. Hence, the statistic 
\[ \Phi_T = \max_{(i,j,k) \in \indexset} a_k \big( |\phi_{ijk,T}| - b_k \big) \]
can be regarded as a Gaussian version of the statistic $\hat{\Psi}_T$. 


To summarize, we propose the following procedure to simultaneously test the hypothesis $H_0^{(ijk)}$ for all $(i,j,k) \in \indexset$ at the significance level $\alpha \in (0,1)$: 
\begin{equation}\label{eq:test}
\text{For each } (i,j,k) \in \indexset, \text{ reject } H_0^{(ijk)} \text{ if } |\hat{\psi}_{ijk,T}| > c_{T,\text{Gauss}}(\alpha,h_k),
\end{equation}
where $c_{T,\text{Gauss}}(\alpha,h_k) = b_k + q_{T,\text{Gauss}}(\alpha)/a_k$ with $a_k = \{\log(e/h_k)\}^{1/2} / \log \log(e^e / h_k)$ and $b_k = \sqrt{2 \log(1/h_k)}$. 

{\color{red}
All of the above can probably be proven using the same methods as before.
}

\subsection{Multiplier bootstrap}

Now we have the problem that $c_i$ and $c_j$ are unknown in real life. This means that the quantile $q_T(\alpha)$ of $\Phi_T$ are not known and can not be approximated by usual Monte Carlo simulations. We need to find another way of approximating them, for example, multiplier bootstrap from \cite{Chernozhukov2017}.

%We next prove that 
%\begin{equation}\label{eq:kolmogorov-distance}
%\sup_{q \in \reals} \Big| \pr \big( \Psi_T \le q \big) - \pr \big( \Phi_T \le q \big) \Big| = o(1).
%\end{equation}
First, we rewrite the statistics $\Phi_T$ as follows. Define 
\begin{equation*}
W^{(ijk)}_t = W^{(ijk)}_{t,T} := \sqrt{\frac{T}{2Th_k}} \ind\Big(\frac{t}{T} \in \mathcal{I}_k\Big) \Big\{ \frac{Z_{it}}{c_i} - \frac{Z_{jt}}{c_j} \Big\}
\end{equation*}
with $Z_{it}$ i.i.d.\ standard normal and let $\boldsymbol{W}_t = (W_t^{(ijk)}: (i,j,k) \in \indexset)$. The vector $\boldsymbol{W}_t$ is a Gaussian vector with zero mean $\ex[\boldsymbol{W}_t] = \boldsymbol{0}$. With this notation, we get that $\phi_{ijk,T} = T^{-1/2} \sum_{t=1}^T W^{(ijk)}_t$ and  
\begin{align*}
\Phi_T 
 & = \max_{(i,j,k) \in \indexset} a_k \big( |\phi_{ijk,T}| - b_k \big) \\
 & = \max_{(i,j,k) \in \indexset} a_k \Big\{ \Big|\frac{1}{\sqrt{T}} \sum_{t=1}^T W^{(ijk)}_t\Big| - b_k \Big\}.
\end{align*} 
For any $q \in \reals$, it holds that
\begin{align*}
\pr \big( \Phi_T \le q \big) 
 & = \pr \Big( \max_{(i,j,k) \in \indexset} a_k \Big\{ \Big|\frac{1}{\sqrt{T}} \sum_{t=1}^T W^{(ijk)}_t\Big| - b_k \Big\} \le q \Big) \\
 & = \pr \Big( \Big|\frac{1}{\sqrt{T}} \sum_{t=1}^T W^{(ijk)}_t\Big| \le c_{ijk}(q) \text{ for all } (i,j,k) \in \indexset \Big) \\
 & = \pr \Big( \Big|\frac{1}{\sqrt{T}} \sum_{t=1}^T \boldsymbol{W}_t\Big| \le \boldsymbol{c}(q) \Big),
\end{align*} 
where $\boldsymbol{c}(q) = (c_{ijk}(q): (i,j,k) \in \indexset)$ is the $\reals^p$-vector with the entries $c_{ijk}(q) = q/a_k + b_k$, we use the notation $|v| = (|v_1|,\ldots,|v_p|)^\top$ for vectors $v \in \reals^p$ and the inequality $v \le w$ is to be understood componentwise for $v,w \in \reals^p$. 

Then, consider a sequence of i.i.d.\, standard normal random variables $e_1, \ldots, e_T$, that are independent of $\eta_{it}$ and $Z_{it}$ for all $i$ and $t$. Define 
\begin{equation*}
V^{(ijk)}_t = V^{(ijk)}_{t,T} := \sqrt{\frac{T}{2Th_k}} \ind\Big(\frac{t}{T} \in \mathcal{I}_k\Big)\Big\{ \frac{\eta_{it}}{\hat{c}_i} - \frac{\eta_{jt}}{\hat{c}_j} \Big\}
\end{equation*}
for $(i,j,k) \in \indexset$ and let $\boldsymbol{V}_t = (V_t^{(ijk)}: (i,j,k) \in \indexset)$ be the $p$-dimensional random vector with the entries $V_t^{(ijk)}$. Additionally, define 
\begin{equation*}
\bar{V}^{(ijk)}_t = \bar{V}^{(ijk)}_{t,T} := \frac{1}{T} \sum_{t=1}^T \sqrt{\frac{T}{2Th_k}} \ind\Big(\frac{t}{T} \in \mathcal{I}_k\Big)\Big\{ \frac{\eta_{it}}{\hat{c}_i} - \frac{\eta_{jt}}{\hat{c}_j} \Big\}
\end{equation*}
and let $\bar{\boldsymbol{V}}_t = (\bar{V}_t^{(ijk)}: (i,j,k) \in \indexset)$ be the $p$-dimensional random vector with the entries $\bar{V}_t^{(ijk)}$. We consider the following conditional probability:
\begin{align*}
\pr &\Big( \Big| \frac{1}{\sqrt{T}} \sum_{t=1}^T e_t (\boldsymbol{V}_t - \bar{\boldsymbol{V}_t}) \Big| \leq \boldsymbol{c}(q) \Big| \big\{V_t^{(ijk)}\big\} \Big) = \\
 & = \pr \Big( \Big|\frac{1}{\sqrt{T}} \sum_{t=1}^T e_t(V^{(ijk)}_t - \bar{V}^{(ijk)}_t )\Big| \le c_{ijk}(q) \text{ for all } (i,j,k) \in \indexset \Big| \big\{V_t^{(ijk)}\big\}\Big) \\
 & = \pr \Big( \max_{(i,j,k) \in \indexset} a_k \Big\{ \Big|\frac{1}{\sqrt{T}} \sum_{t=1}^Te_t(V^{(ijk)}_t - \bar{V}^{(ijk)}_t )\Big| - b_k \Big\} \le q\Big| \big\{V_t^{(ijk)}\big\} \Big) 
\end{align*}


{\color{red} Need to check the same mean and variance! In particular, $\ex[\boldsymbol{W}_t] = \ex[\boldsymbol{V}_t] = 0$ and $\ex[\boldsymbol{W}_t \boldsymbol{W}_t^\top] = \ex[\boldsymbol{V}_t \boldsymbol{V}_t^\top]$.}

With this notation at hand, we can make use of Corollary 4.2 from \cite{Chernozhukov2017}. In our context, this proposition can be stated as follows: 
\begin{propA}\label{prop:Chernozhukov}
Let $\alpha\in (0, e^{-1})$ be a constant and assume that 
\begin{enumerate}[label=(\alph*),leftmargin=0.7cm]
\item $T^{-1} \sum_{t=1}^T \ex (V^{(ijk)}_t)^2 \ge \delta > 0$ for all $(i,j,k) \in \indexset$.
\item $T^{-1} \sum_{t=1}^T \ex[ |V^{(ijk)}_t|^{2+r} ] \le B_T^r$ for all $(i,j,k) \in \indexset$ and $r=1,2$, where $B_T \ge 1$ are constants that may tend to infinity as $T \rightarrow \infty$.  
\item $\ex[ \{ \max_{(i,j,k) \in \indexset} |V^{(ijk)}_t| / B_T \}^\theta ] \le 2$ for all $t$ and some $\theta > 4$.  
\end{enumerate}
Then  we have with probability at least $1 - \alpha$,
\begin{align}
\sup_{\boldsymbol{c} \in \reals^p} \Big| \pr &\Big( \Big| \frac{1}{\sqrt{T}} \sum_{t=1}^T e_t (\boldsymbol{V}_t - \bar{\boldsymbol{V}_t}) \Big| \leq \boldsymbol{c}(q) \Big| \big\{V_t^{(ijk)}\big\} \Big)  - \pr \Big( \Big|\frac{1}{\sqrt{T}} \sum_{t=1}^T \boldsymbol{W}_t\Big| \le \boldsymbol{c} \Big) \Big| \nonumber \\ & \le C \Big\{ \Big( \frac{B_T^2 \log^5(pT)\log^2(1/\alpha)}{T} \Big)^{1/6} + \Big( \frac{B_T^2 \log^3(pT)}{\alpha^{2/\theta}T^{1-2/\theta}} \Big)^{1/3} \Big\}, \label{eq:Chernozhukov}
\end{align}
where $C$ depends only on $\delta$ and $\theta$. 
\end{propA}
{\color{red}
Need to verify that assumptions (a)--(c) are satisfied for sufficiently large $T$! where $B_T$ can be chosen as $B_T = C p^{1/\theta} h_{\min}^{-1/2}$ with $C$ sufficiently large. Moreover, it can be shown that the right-hand side of \eqref{eq:Chernozhukov} is $o(1)$ for this choice of $B_T$. Hence, Proposition \ref{prop:Chernozhukov} yields that 
\[ \sup_{\boldsymbol{c} \in \reals^p} \Big| \pr \Big( \Big| \frac{1}{\sqrt{T}} \sum_{t=1}^T e_t (\boldsymbol{V}_t - \bar{\boldsymbol{V}_t}) \Big| \leq \boldsymbol{c}(q) \Big| \big\{V_t^{(ijk)}\big\} \Big) - \pr \Big( \Big|\frac{1}{\sqrt{T}} \sum_{t=1}^T \boldsymbol{W}_t\Big| \le \boldsymbol{c} \Big) \Big| = o(1), \]
which in turn implies \eqref{eq:kolmogorov-distance}. 
}
\bibliographystyle{ims}
{\small
\setlength{\bibsep}{0.35em}
\bibliography{bibliography}}



\end{document}
