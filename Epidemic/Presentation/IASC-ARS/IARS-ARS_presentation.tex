\documentclass[10pt]{beamer}

\usetheme[progressbar=frametitle]{metropolis}

\usepackage{appendixnumberbeamer}

\usepackage{booktabs}
\usepackage{blkarray}
%\usepackage{ccicons}
\usepackage{graphicx}
\usepackage{color}

\definecolor{UniBlue}{RGB}{7,82,154}
\definecolor{UniYellow}{RGB}{234,185,12}

%\setbeamercolor{title}{fg=UniBlue, bg = UniYellow}
%\setbeamercolor{frametitle}{fg=UniBlue, bg= UniYellow}
%\setbeamercolor{structure}{fg=UniBlue, bg= UniYellow}
%\setbeamercolor{progress bar}{fg=UniBlue, bg= UniYellow}
\usepackage{xspace}

\title{Nonparametric comparison of epidemic time trends: the case of COVID-19}
\author{Marina Khismatullina \and \hspace{11mm} Michael Vogt \\ Erasmus University Rotterdam \and University of Ulm}
\setbeamertemplate{frame footer}{Nonparametric comparison of epidemic time trends: the case of COVID-19}
\date{\vspace{5mm} IASC-ARS Interim Conference 2022 \\ 12 December, 2022}
\metroset{block=fill}
% \titlegraphic{\hfill\includegraphics[height=1.5cm]{logo.pdf}}

\newcommand{\Prob}{\mathrm{P}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Corr}{\mathrm{Corr}}
\newcommand{\sgn}{\text{sgn}}
\newtheorem{prop}{Proposition}
\newcommand{\ind}{\boldsymbol{1}\Big( \frac{t}{T} \in \mathcal{I}_k \Big)} % indicator function
\newcommand{\indsmall}{\boldsymbol{1}\big( \frac{t}{T} \in \mathcal{I}_k \big)} % indicator function

\begin{document}

\maketitle

\begin{frame}{Table of contents}
  \setbeamertemplate{section in toc}[sections numbered]
  \tableofcontents[hideallsubsections]
\end{frame}

\section{Introduction}


\begin{frame}{Motivation}

{\onslide<1-> \begin{block}{Aim of the paper}
	To develop new inference methods that allow to \textit{identify} and \textit{locate} differences between epidemic time trends.
\end{block}}
	{\onslide<2>\begin{figure}
    		\centering
    		\includegraphics[height=0.45\textheight]{plots/Germany_and_Italy.pdf}
  	\end{figure}}
	{\onslide<3>
	\vspace{-46,81mm}
	\begin{figure}
    		\centering
    		\includegraphics[height=0.45\textheight]{plots/Germany_and_Italy_1.pdf}
  	\end{figure}}	
	{\onslide<4->
	\vspace{-46,81mm}
	\begin{figure}
    		\centering
    		\includegraphics[height=0.45\textheight]{plots/Germany_and_Italy_2.pdf}
  	\end{figure}}	
\vspace{-3mm}
{\onslide<5->\textbf{Research question:}
	Out of many given intervals, how to find those where the trends are significantly different?}


%\textbf{Aim:} to compare the underlying trends on different intervals simultaneously.}
\end{frame}

\begin{frame}{Motivation}
\vspace{-4mm}
\textbf{Why is it relevant?}

Finding systematic differences between trends = basis for further research

$\quad \quad \Rightarrow$ understanding which government policies are more effective.\pause

\vspace{3mm}

\textbf{Why is it difficult?}	

Testing many hypotheses at the same time = multiple testing problem

$\quad \quad \Rightarrow$ large probability of one true null hypothesis being rejected.\pause

\vspace{3mm}

\textbf{Is it limited to COVID-19?}

No! Our method = general method for comparing nonparametric trends

$\quad \quad \Rightarrow$ new statistical test for equality of nonparametric trend curves.
 
\end{frame}

\begin{frame}{Literature}
	Comparison of deterministic trends:
	\begin{itemize}
		\item {\color<2>{mLightBrown}Park et al. (2009)}, Degras et al. (2012), Zhang et al. (2012), Hidalgo and Lee (2014), Chen and Wu (2019).
	\end{itemize}\pause\pause
	Multiscale tests:
	\begin{itemize}
		\item Chaudhuri and Marron (1999, 2000), Hall and Heckman (2000), D{\"u}mbgen and Spokoiny (2001), {\color<4>{mLightBrown}Park et al. (2009)}.
	\end{itemize}\pause\pause
	Studies of COVID-19:
	\begin{itemize}
		\item Dong et al. (2020), Gu et al. (2020), Li and Linton (2020), Jiang et al. (2020) and many others.
	\end{itemize}
\end{frame}


\section{Model}
\begin{frame}{Motivation for the model}
We observe $n$ time series $\mathcal{X}_i = \{X_{it}: 1 \le t \le T \}$ of length $T$.\pause

\vspace{3mm}

$X_{it}$ are non-negative integers $\Rightarrow$ can be modelled by a Poisson distribution with time-varying parameter $\lambda_i(t/T)$: $X_{it} \sim P_{\lambda_i(t/T)}$.\pause
\vspace{3mm}

Since $\lambda_i(t/T) = \E[X_{it}] = \Var(X_{it})$, we can rewrite $X_{it}$ as
\vspace{-2mm}
%\begin{overlayarea}{\textwidth}{2cm}
\begin{align*}
X_{it} &= \lambda_i\Big(\frac{t}{T}\Big) + \only<3 | handout: 0 >{u_{it} }\onslide<4->{\sqrt{\lambda_i\Big(\frac{t}{T}\Big)} \eta_{it},}
\end{align*}
%\end{overlayarea}
\onslide<4->where $\eta_{it}$ has zero mean and unit variance.

\vspace{3mm}

\onslide<5->In applications the variance can be larger than the mean $\Rightarrow$ quasi-Poisson models.
\end{frame}

\begin{frame}{Model}
Quasi-Poisson model:
\begin{equation*}
X_{it} = \lambda_i \Big( \frac{t}{T} \Big) + \sigma\sqrt{\lambda_i \Big( \frac{t}{T} \Big)} \eta_{it},
\end{equation*}\pause
\vspace{-3mm}
where
\begin{itemize}
\item $\sigma$ is the overdispersion parameter;\pause
\item $\lambda_i$ are unknown trend functions on $[0,1]$;\pause
\item $\eta_{it}$ are error terms that are independent across $i$ and $t$ and have zero mean and unit variance.
\end{itemize}
\end{frame}

\section{Testing procedure}
\begin{frame}{Testing problem}
\begin{align*}
H_0: \lambda_1 = \lambda_2 = \ldots = \lambda_n
\end{align*}\pause

\vspace{-6mm}
\textbf{Question}: if we reject the global null, how to locate the differences between the trends? \pause


Let $\mathcal{F} :=\{ \mathcal{I}_k \subseteq [0, 1]: 1 \le k \le K\}$ be a family of rescaled time intervals on $[0, 1]$, and for each triplet $(i, j, k)$ consider the null hypothesis that the functions $\lambda_i$ and $\lambda_j$ are equal on an interval $\mathcal{I}_k$\pause, i.e.
\begin{align*}
H_0^{(ijk)}:\quad  \lambda_i(w) = \lambda_j(w) \text{ for all } w\in \mathcal{I}_k
\end{align*}\pause
\vspace{-2mm}

We want to test $H_0^{(ijk)}$ simultaneously for all pairs of countries $i$ and $j$ and all intervals $\mathcal{I}_k$ in the family $\mathcal{F}$ and we want to control the familywise error rate (FWER) at level $\alpha$:
\begin{align*}
\text{FWER}(\alpha) = \Prob \Big( \exists (i,j,k): \text{we wrongly reject } H_0^{(ijk)} \Big).
\end{align*}
%where $\mathcal{M}_0$ is the set of triplets $(i, j, k)$ for which $H_0^{(ijk)}$ holds true.
\end{frame} 


\begin{frame}[label = frame_teststatistic]{Test statistic}
For a given interval $\mathcal{I}_k$ and a pair of time series $i$ and $j$ we calculate
\begin{equation*}
\hat{s}_{ijk} = \frac{1}{T h_k} \sum\limits_{t=1}^T \ind (X_{it} -X_{jt}), 
\end{equation*}
%\vspace{-3mm}
where $h_k$ is the length of $\mathcal{I}_k$. \pause $\hat{s}_{ijk}$ estimates the average distance between $\lambda_i$ and $\lambda_j$ on $\mathcal{I}_k$. \pause Under certain assumptions, 
\begin{align*}
\Var(\hat{s}_{ijk})  & = \frac{\sigma^2}{T^2 h_k^2} \sum\limits_{t=1}^T \ind \Big\{ \lambda_i\Big(\frac{t}{T}\Big) + \lambda_j\Big(\frac{t}{T}\Big) \Big\},
\end{align*}\pause
which can be estimated by
%In order to normalize the variance of the statistic $\hat{s}_{ijk}$, we scale it by an estimator of its variance:
\[ \widehat{\Var(\hat{s}_{ijk})} = \frac{\hat{\sigma}^2}{T^2 h_k^2} \sum\limits_{t=1}^T \ind (X_{it} + X_{jt} ), \]
with $\hat{\sigma}^2$ being an appropriate estimator of $\sigma^2$. \hyperlink{frame_sigma}{\beamerbutton{Details}}

%\begin{align*}
%\hat{s}_{ijk} = &\frac{1}{Th_k} \sum\limits_{t=1}^T \ind \bigg( \lambda_i \Big(\frac{t}{T}\Big)  - \lambda_j \Big(\frac{t}{T}\Big)\bigg) \\
%&+  {\color<4-5>{mLightBrown} \frac{\sigma}{Th_k} \sum\limits_{t=1}^T \ind \Big(\sqrt{\lambda_i\Big(\frac{t}{T}\Big)} \eta_{it} -  \sqrt{ \lambda_j\Big(\frac{t}{T}\Big)} \eta_{jt} \Big)}\\
%%\end{align*}
%%Applying a law of large numbers, we get that
%%\begin{align*}
%%\hat{s}_{ijk} 
%\onslide<5-6>{= &\frac{1}{Th_k} \sum\limits_{t=1}^T \ind \bigg( \lambda_i \Big(\frac{t}{T}\Big)  - \lambda_j \Big(\frac{t}{T}\Big)\bigg) + {\color<5>{mLightBrown}o_P(1)}}
%\end{align*}
\end{frame}


\begin{frame}{Test statistic, part 2}
Test statistic for the hypothesis $H_0^{(ijk)}$ is then defined as
\begin{equation*}
\widehat{\psi}_{ijk} : = \frac{\hat{s}_{ijk}}{\sqrt{\widehat{\Var(\hat{s}_{ijk})} }}= \frac{\sum\nolimits_{t=1}^T \indsmall (X_{it} -X_{jt})}{\hat{\sigma} \big\{ \sum\nolimits_{t=1}^T \indsmall  (X_{it} + X_{jt} )\big\}^{1/2}}
\end{equation*}
%Under certain conditions and under the null, $\widehat{\psi}_{ijk}$ can be approximated by a Gaussian version of the test statistic:
%\begin{align*}
%\phi_{ijk} = \frac{1}{\sqrt{2 T h_k}} \sum\limits_{t=1}^T \ind (Z_{it} - Z_{jt}), 
%\end{align*}
%where $Z_{it}$ are independent standard normal random variables.
\end{frame}

\begin{frame}[label = frame_critval]{Critical values}
How to construct critical values $c_{ijk}(\alpha)$?\pause
\begin{itemize} 
\item Traditional approach: $c_{ijk}(\alpha) = c(\alpha)$ for all $(i,j,k)$. \pause
\item More modern approach: $c_{ijk}(\alpha)$ depend on the length $h_k$ of the time interval (D{\"u}mbgen and Spokoiny (2001)):
\[c_{ijk}(\alpha) = c(\alpha,h_k) := b_k + q(\alpha)/a_k,\] where $a_k$ and $b_k$ are scale-dependent constants and $q(\alpha)$ is chosen such that we control FWER.
\hyperlink{frame_scaleconstants}{\beamerbutton{Details}}
\end{itemize}
\end{frame}


\begin{frame}{Critical values, part 2}
We want to control FWER. \pause Let $\mathcal{M}_0: = \big\{(i, j, k) | H_0^{(ijk)} \text{ is true} \big\}$, then
\begin{align*}
\text{FWER}(\alpha) &= \Prob \Big( \exists (i,j,k) \in \mathcal{M}_0:  |\widehat{\psi}_{ijk} | > c_{ijk}(\alpha) \Big) \\
\onslide<3->{&= 1 - \Prob \Big( \forall (i,j,k) \in \mathcal{M}_0: |\widehat{\psi}_{ijk} | \le c_{ijk}(\alpha) \Big)\\
& =  1 - \Prob \Big( \forall (i,j,k) \in \mathcal{M}_0: a_k \big(|\hat{\psi}_{ijk}| - b_k\big) \le q(\alpha) \Big)\\
& = 1 - \Prob\Big( \max_{(i,j,k) \in \mathcal{M}_0} a_k \big( |\hat{\psi}_{ijk}| - b_k \big) \le q(\alpha) \Big)\\
& \leq 1 - \Prob\Big( \max_{(i,j,k)} a_k \big( |{\color<4>{mLightBrown}\hat{\psi}_{ijk}^0}| - b_k \big) \le q(\alpha) \Big)}
\end{align*}
\onslide<5->{Hence, we choose $q(\alpha)$ as the $(1-\alpha)$-quantile of the statistic 
\[ \hat{\Psi}_T = \max_{(i,j,k)} a_k \big( |\hat{\psi}_{ijk}^0| - b_k \big), \]
where $\hat{\psi}_{ijk}^0$ is equal to $\hat{\psi}_{ijk}$ under the null.}
\end{frame}

\begin{frame}{Critical values, part 3}

But we do not know the distribution of $\hat{\Psi}_T$ in practice!\pause

$\Rightarrow$ the quantiles $q(\alpha)$ are also not known. How to approximate them?\pause

Under our assumptions, 
\[ \hat{\psi}_{ijk}^0 \approx \frac{1}{\sqrt{2 T h_k}} \sum\limits_{t=1}^T \ind  (\eta_{it} - \eta_{jt} ), \]
can be approximated by a Gaussian version of the test statistic:
\begin{align*}
\phi_{ijk} = \frac{1}{\sqrt{2 T h_k}} \sum\limits_{t=1}^T \ind (Z_{it} - Z_{jt}), 
\end{align*}
where $Z_{it}$ are independent standard normal random variables.


\end{frame}



\begin{frame}[label = frame_test]{Test procedure}

\begin{enumerate}
	\item Consider the Gaussian test statistic 
	\vspace{-2mm} \[ \Phi_T = \max_{(i,j,k)} a_k \big( |\phi_{ijk}| - b_k \big), \] where $a_k$ and $b_k$ are scale-dependent constants and $\phi_{ijk}$ are weighted averages of the differences of standard normal random variables.\pause
	\item Compute a $(1-\alpha)$-quantile $q_{\text{Gauss}} (\alpha)$ of $\Phi_T$ by Monte Carlo simulations.\pause
	\item Adjust $q_{ \text{Gauss}} (\alpha)$ by the scale-dependent constants \vspace{-2mm}  \[c_{\text{Gauss}}(\alpha,h_k) = b_k + q_{\text{Gauss}}(\alpha)/a_k\] \pause
\end{enumerate}
\vspace{-5mm}
\begin{block}{Test procedure}
For the given significance level $\alpha \in (0,1)$ and for each $(i,j,k)$, reject $H_0^{(ijk)}$ if $|\widehat{\psi}_{ijk}| > c_{\text{Gauss}}(\alpha,h_k)$.

\end{block}
\end{frame}


\begin{frame}{Theoretical properties, part 1}
\begin{prop}\label{prop1}
Let $\mathcal{M}_0$ be the set of triplets $(i, j, k)$ for which $H_0^{(ijk)}$ holds true. Then under certain assumptions, it holds that 
\vspace{-2mm}
\begin{align*}
 \Prob\Big( \forall (i,j,k) \in \mathcal{M}_0: |\hat{\psi}_{ijk}| \le c_{\textnormal{Gauss}}(\alpha,h_k) \Big) \ge 1 - \alpha + o(1)
\end{align*}
\end{prop}\pause
\begin{corollary}
\vspace{-3mm}
\begin{align*}
	FWER(\alpha) \leq \alpha.
\end{align*}
\end{corollary}
\end{frame}


\begin{frame}{Theoretical properties, part 2}
\begin{prop}\label{prop2}
Consider a sequence of functions $\lambda_{i} = \lambda_{i,T}$, $\lambda_{j} = \lambda_{j, T}$ such that 
\begin{equation}\label{eq:localt}
\exists \, \mathcal{I}_{k}:  \lambda_{i}(w) - \lambda_{j}(w) \ge c_T \sqrt{\log T / (T h_{k})} \,\, \forall w \in \mathcal{I}_{k},
\end{equation} and $c_T \rightarrow \infty$ faster than $\frac{\sqrt{\log T}\sqrt{\log \log T}}{\log \log \log T}$.  Let $\mathcal{M}_1$ be the set of triplets $(i, j, k)$ for which \eqref{eq:localt} holds true. Then under certain assumptions, it holds that
%\vspace{-2mm}
\begin{equation*}
\Prob\Big( \forall (i,j,k) \in \mathcal{M}_1: |\hat{\psi}_{ijk}| > c_{\textnormal{Gauss}}(\alpha,h_k) \Big) = 1 - o(1)
\end{equation*}
\end{prop}
\end{frame}



\section{Application}

%\begin{frame}{Graphical representation}
%How to represent the results of the test? \pause
%
%Plot the results of pairwise comparison $\mathcal{F}_{\text{reject}}(i, j)$:
%\begin{align*}
% \Prob\Big( \forall (i,j,k) \in \mathcal{M}_0: \mathcal{I}_k \notin \mathcal{F}_{\text{reject}}(i, j) \Big) \ge 1 - \alpha + o(1)
%\end{align*}
%
%\pause
%\begin{block}{Minimal intervals}
%An interval $\mathcal{I}_k \in \mathcal{F}_{\text{reject}}(i, j)$ is called \textbf{minimal} if there is no other interval $\mathcal{I}_{k^\prime} \in \mathcal{F}_{\text{reject}}(i, j)$ with $\mathcal{I}_{k^\prime} \subset \mathcal{I}_k$.
%
%The set of minimal intervals is denoted $\mathcal{F}_{\text{reject}}^{\min} (i, j)$.
%\end{block}\pause
%We can make similar confidence statements about minimal intervals:
%\begin{align*}
% \Prob\Big( \forall (i,j,k) \in \mathcal{M}_0: \mathcal{I}_k \notin \mathcal{F}_{\text{reject}}^{\min} (i, j) \Big) \ge 1 - \alpha + o(1)
%\end{align*}
%\end{frame}

\begin{frame}{Application setting}
\begin{itemize}
\item Five countries: Germany, Italy, Spain, France and the UK.
\item $T = 150$ days. 
\item The data is aligned by weekdays: first Monday after reaching $100$ cases as $t=1$.
\item Lengths of time intervals $7, 14, 21, 28$ days. The intervals start at days $1, 8, 15, \ldots$ and $4, 11, 19, \ldots$
\item $\alpha = 0.05$.
\item $5000$ Monte Carlo simulation runs to produce critical values.
\end{itemize}
\end{frame}

\begin{frame}{Application results}
	\begin{figure}
		\includegraphics[width=0.49\textwidth]{plots/DEU_vs_ITA_presentation}
   		%\caption{Observed new cases per day in Germany and Italy}    			\label{fig:DEUvsITA}
		\hfill
		\includegraphics[width=0.49\textwidth]{plots/DEU_vs_ESP_presentation}
	\end{figure}
\end{frame}

\begin{frame}{Application results, part 2}
	\begin{figure}
		\includegraphics[width=0.49\textwidth]{plots/DEU_vs_GBR_presentation}
   		%\caption{Observed new cases per day in Germany and Italy}    			\label{fig:DEUvsITA}
		\hfill
		\includegraphics[width=0.49\textwidth]{plots/DEU_vs_FRA_presentation}
	\end{figure}
\end{frame}


\begin{frame}{Discussion}
We can claim, with confidence of about $95\%$, that the null hypothesis is violated for all intervals (and all pairs of countries) for which our test rejects the null.\pause

However, we can not say anything about the causes of such differences. This question requires further (probably not purely statistical) analysis.\pause

Further possible extensions:
\vspace{-1mm}
\begin{itemize}
	\item introduce scaling factor in the trend function, that will allow to adjust for the size of the country (population, density, testing regimes, etc.);
	\item include dependence in the error terms;
	\item cluster the countries based on the trends they exhibit.
\end{itemize}
\end{frame}

\begin{frame}{Where to find more?}
	Contact information:
	\begin{itemize}
		\item \url{https://marina-khi.github.io}
		\item \url{https://github.com/marina-khi/multiscale}
		\item \href{mailto:khismatullina@ese.eur.nl}{khismatullina@ese.eur.nl}
	\end{itemize}
	Reference:
	\begin{itemize}
		\vspace{-1mm}
		\item Khismatullina, M. and Vogt, M. (2021). Nonparametric comparison of epidemic time trends: the case of COVID-19. \textit{Journal of Econometrics.}
	\end{itemize}
\end{frame}

\begin{frame}[standout]
  Thank you!
\end{frame}


\appendix

\begin{frame}{Assumptions}
\begin{itemize}
\item[$\mathcal{C}1$] \label{C1} The functions $\lambda_i$ are uniformly Lipschitz continuous: $|\lambda_i(u) - \lambda_i(v)| \le L |u-v|$ for all $u, v \in [0,1]$.
\item[$\mathcal{C}2$] \label{C2} $0 < \lambda_{\min} \le \lambda_i(w) \le \lambda_{\max} < \infty$ for all $w \in [0, 1]$ and all $i$. 
\item[$\mathcal{C}3$] \label{C3} $\eta_{it}$ are independent both across $i$ and $t$.
\item[$\mathcal{C}4$] \label{C4} $\E[\eta_{it}] = 0$, $\E[\eta_{it}^2] = 1$ and $\E[|\eta_{it}|^\theta] \le C_\theta < \infty$ for some $\theta > 4$. 
\item[$\mathcal{C}5$] \label{C5} $h_{\max} = o(1/\log T)$ and $h_{\min} \ge CT^{-b}$ for some $b \in (0,1)$.
\item[$\mathcal{C}6$] \label{C6} $p := \{ \# (i, j, k) \} = O(T^{(\theta/2)(1-b)-(1+\delta)})$ for some small $\delta > 0$.
\end{itemize}
\end{frame}


\begin{frame}{Family of time intervals}
	\begin{figure}
		\includegraphics[width=0.95\textwidth]{plots/all_intervals}
	\end{figure}
\end{frame}

\begin{frame}{Simulation results for the size of the test}
\begin{figure}[t!]
	\includegraphics[height = 0.4\textheight]{plots/lambda_fct}
\end{figure}
\vspace{-2mm}
\scriptsize{\begin{table}[t]
\begin{center}
\caption{Size of the multiscale test}
\label{tab:size_shape}
\input{plots/size_overdispersion_15}
\end{center}
\end{table}}
\end{frame}

\begin{frame}{Simulation results for the power of the test}
\begin{figure}[t!]
	\onslide<1->\includegraphics[width = 0.49\textwidth, height = 0.4\textheight]{plots/lambda_fcts_height}
	\onslide<2->\includegraphics[width = 0.49\textwidth, height = 0.4\textheight]{plots/lambda_fcts_shift}	
\end{figure}\pause
\vspace{-5mm}
{\onslide<1>\scriptsize{\begin{table}[t]
\begin{center}
\caption{Power of the multiscale test for scenario A}
\label{tab:size_shape}
\input{plots/power_sigma_15_higher_peak}
\end{center}
\end{table}}}
{\onslide<2>
\vspace{-39.5mm}
\scriptsize{\begin{table}[t]
\begin{center}
\caption{Power of the multiscale test for scenario B}
\label{tab:size_shape}
\input{plots/power_sigma_15_shifted_peak}
\end{center}
\end{table}}}
\end{frame}

\begin{frame}[label = frame_sigma]{Estimator of ${\sigma}^2$}
We estimate the overdispersion paramter $\sigma^2$ by \[\widehat{\sigma}^2= \frac{1}{n} \sum_{i = 1}^n \hat{\sigma}_i^2 \text{ and } \hat{\sigma}_i^2 = \frac{\sum_{t=2}^T (X_{it}-X_{it-1})^2}{2 \sum_{t=1}^T X_{it}}\] \pause
We assume that $\lambda_i$ is Lipschitz continuous. Then
\[ X_{it} - X_{it-1} = \sigma \sqrt{\lambda_i\Big(\frac{t}{T}\Big)} (\eta_{it} - \eta_{it-1}) + r_{it}, \]
where $|r_{it}| \le C(1+|\eta_{it-1}|)/T$ with a sufficiently large $C$.\pause \, Hence,
\[ \frac{1}{T} \sum_{t=2}^T (X_{it} - X_{it-1})^2 = 2 \sigma^2 \Big\{ \frac{1}{T} \sum_{t=2}^T \lambda_i(t/T) \Big\} + o_p(1)\] \pause
Together with \[ \frac{1}{T} \sum_{t=1}^T X_{it} = \frac{1}{T} \sum_{t=1}^T \lambda_i(t/T) + o_p(1), \] we get that $\hat{\sigma}_i^2 = \sigma^2 + o_p(1)$ for any $i$ and thus $\hat{\sigma}^2 = \sigma^2 + o_p(1)$. \hyperlink{frame_teststatistic<4>}{\beamerbutton{Go back}}
\end{frame}


\begin{frame}[label = frame_notation]{Notation}
\begin{center}
In order to proceed with the proof, we will need the following notation:
\end{center}
\vspace{-2mm}
\begin{align*}
\hat{\psi}_{ijk, T} &= \frac{\sum\nolimits_{t=1}^T \indsmall (X_{it} -X_{jt})}{\hat{\sigma} \big\{ \sum\nolimits_{t=1}^T \indsmall  (X_{it} + X_{jt} )\big\}^{1/2}} &&\\
\hat{\psi}_{ijk,T}^0 &= \frac{\sum\nolimits_{t=1}^T \indsmall \, \sigma \overline{\lambda}_{ij}^{1/2}(\frac{t}{T}) (\eta_{it} - \eta_{jt})}{ \hat{\sigma} \{ \sum\nolimits_{t=1}^T \indsmall (X_{it} + X_{jt}) \}^{1/2}} &&\hat{\Psi}_T^0 = \max_{(i,j,k)} a_k (|\hat{\psi}_{ijk,T}^0| - b_k)\\
\psi_{ijk,T}^0 &= \frac{1}{\sqrt{2Th_k}} \sum\limits_{t=1}^T \ind (\eta_{it} - \eta_{jt}) &&\Psi_T = \max_{(i,j,k)} a_k (|\psi_{ijk,T}^0| - b_k)\\
\phi_{ijk,T} &= \frac{1}{\sqrt{2 T h_k}} \sum\limits_{t=1}^T \ind (Z_{it} - Z_{jt}) &&\Phi_T = \max_{(i,j,k)} a_k (|\phi_{ijk,T}| - b_k)
\end{align*}
\end{frame}

\begin{frame}{Strategy of the proof}
\begin{enumerate}
\item We prove that  $\big| \hat{\Psi}_T^0 - \Psi_T \big| = o_p(r_T)$, where $\{r_T\}$ is some null sequence.\pause
\item With the help of results from Chernozhukov et al. (2017), we prove
\begin{equation*}\label{eq:kolmogorov-distance}
\sup_{q \in \mathbf{R}} \Big| \Prob\big( \Psi_T \le q \big) - \Prob\big( \Phi_T \le q \big) \Big| = o(1)
\end{equation*}\pause
\vspace{-2mm}
\item By using these two results, we now show that 
\begin{equation}\label{eq:kolmogorov-distance-hat}
\sup_{q \in \mathbb{R}} \Big| \Prob\big( \hat{\Psi}_T^0 \le q \big) - \Prob\big( \Phi_T \le q \big) \Big| = o(1)
\end{equation}\pause
\vspace{-2mm}
\item It can be shown that $\Prob (\Phi_T \le q_{\text{Gauss}}(\alpha)) = 1-\alpha$. From this and \eqref{eq:kolmogorov-distance-hat}, it immediately follows that  
\begin{equation*}
\Prob\big( \hat{\Psi}_T^0 \le q_{\text{Gauss}}(\alpha) \big) = 1 - \alpha + o(1), 
\end{equation*}
which in turn implies the desired statement. 
\end{enumerate}
\end{frame}


\begin{frame}[label = frame_scaleconstants]{Idea behind $a_k$ and $b_k$}

D{\"u}mbgen and Spokoiny (2001): the critical values $c_{ijk}(\alpha)$ depend on the scale of the testing problem, i.e. the length $h_k$ of the time interval.\pause 

Specifically, 
\[c_{ijk}(\alpha) = c(\alpha,h_k) := b_k + q(\alpha)/a_k,\] where $a_k = \{\log(e/h_k)\}^{1/2} / \log \log(e^e / h_k)$ and $b_k = \sqrt{2 \log(1/h_k)}$ are scale-dependent constants and $q(\alpha)$ is chosen such that we control FWER.
\end{frame}

\begin{frame}{Idea behind $a_k$ and $b_k$, part 2}

This choice of scale-dependent constants helps us balance the significance of hypotheses between the time intervals of different lengths $h_k$:

\begin{figure}
    		\centering
	\includegraphics[width=0.95\textwidth]{plots/size_with_correction}
\end{figure}


\hyperlink{frame_critval<4>}{\beamerbutton{Go back}}
\end{frame}



\begin{frame}{Idea behind the additive correction}
Consider the uncorrected Gaussian statistic
\begin{align*}
\Phi^{\text{uncor}} = \max_{(i,j,k)} |\phi_{ijk}|
\end{align*}\pause
and let the family of intervals be \[\mathcal{F} = \big\{[(m-1) h_l, m h_l] \text{ for } 1\le m \le 1/h_l, 1 \le l \le L\big\}\]\pause
Then we can rewrite the uncorrected test statistic as
\begin{align*}
\Phi^{\text{uncor}} = \max_{i, j} \max_{\substack{1 \le l \le L, \\ 1\le m \le 1/h_l}} \Big|\frac{1}{\sqrt{2 T h_l}} \sum\limits_{t=1}^T 1 \Big( \frac{t}{T} \in [(m-1) h_l, m h_l] \Big) (Z_{it} - Z_{jt})\Big|
\end{align*}\pause
$\Rightarrow \quad \max_m \ldots =\sqrt{2\log(1/h_l)} + o_P(1) \to \infty$ as $h \to 0$ and the stochastic behavior of $\Phi^{\text{uncor}}$ is dominated by the elements with small bandwidths $h_l$. \hyperlink{frame_test<4>}{\beamerbutton{Go back}}
\end{frame}



\end{document}
