\documentclass[a4paper,12pt]{article}
\usepackage{amsmath}
\usepackage{amssymb,amsthm,graphicx}
\usepackage{titlesec}
\usepackage{textcomp}
\usepackage{enumitem}
\usepackage{color}
\usepackage{epsfig}
\usepackage{graphics}
\usepackage{pdfpages}
\usepackage{subcaption}
\usepackage[font=small]{caption}
\usepackage[hang,flushmargin]{footmisc} 
\usepackage{float}
\usepackage{rotating,tabularx}
\usepackage{booktabs}
\usepackage[mathscr]{euscript}
\usepackage{natbib}
\usepackage{setspace}
\usepackage{mathrsfs}
\usepackage[left=3cm,right=3cm,bottom=3cm,top=3cm]{geometry}
%\parindent0pt

\input{macros}

\setcounter{secnumdepth}{4}

\titleformat{\paragraph}
{\normalfont\normalsize\bfseries}{\theparagraph}{1em}{}
\titlespacing*{\paragraph}
{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}
\renewcommand{\baselinestretch}{1.2}



\begin{document}

 
\noindent {\Large \bf Project Description} 
\vspace{0.25cm}

\noindent \hrulefill
\vspace{0.5cm}

\noindent\begin{tabular}{ll}
\large{Applicant:} & \noindent {\large Michael Vogt, University of Bonn} \\[0.1cm]
\large{Project Title:} & \noindent {\large New Methods and Theory for the Comparison of} \\
 & \noindent {\large Nonparametric Trend Curves}
\end{tabular}
\vspace{0.5cm}

\noindent \hrulefill



%\noindent {\Large \bf A \hspace{0.2cm} General information}


%\section{Applicant(s)}

%PI \\
%date of birth \\
%address \\
%telephone \\
%e-mail 


%\section{List of abbrevations} 



%\section{Summary} 


%The comparison of trend curves is an important topic in many statistical applications. In economics, one may be interested in comparing the trends in the long-term interest rates of different countries. Similarly, one may want to find out whether there are different trends in real GDP growth for different countries. In finance, large amounts of data on a huge number of stocks ara available nowadays. One may want to compare wherther there the volatility trend is different for different stocks. In climatotology, large spatial data sets have been collected which comprise long temperature time series for many different spatial locations. Climatologists are very much interested in analsing the trending behaviour of these time series. In particular, they would like to know how the trending behaviour varies with the location. 


%The main aim of this project is to develop new methods and theory for the comparison of nonparametric trend curves. Classically, time trends are modelled stochastically in econometrics, e.g.\ by a unit root model. Recently, there has been a growing interest in models with a detemrinsitic time trend. An interesting modelling framework considered in ?? among others is as follows: Suppose we observe a number of time series $\mathcal{Y}_i = \{ Y_{it}: 1 \le t \le T \}$ for $1 \le i \le n$. Each time series $\mathcal{Y}_i$ is modelled by the equation
%\begin{equation}\label{model}
%Y_{it} = m_i \Big( \frac{t}{T} \Big) + \beta_i^\top X_{it} + \alpha_i + \varepsilon_{it}
%\end{equation}
%for $1 \le t \le T$, where $m_i$ is a nonparametric trend curve, $X_{it} = (X_{it,1},\ldots,X_{it,d})$ is a $d$-dimensional vector of regressors or controls and $\beta_i$ is the corresponding parameter vector, $\alpha_i$ are so-called fixed effect error terms and $\varepsilon_{it}$ are standard regression errors with $\ex[\varepsilon_{it}] = 0$ for all $t$. 


%Within model \eqref{model}, one may consider several interesting statistical questions. The first one is the following: Are all time trends $m_i$ the same? That is, do all time series in the sample exhibit the same tending behaviour. This question can be approached formally as a testing problem. The null hypothesis is of interest can be formulated as $H_0: m_1 = \ldots = m_n$. [Literature on comparison of trend curves and more generally of regression curves.] 


%%%%%%%%%%%


%The main aim of this project is to develop new methods for the comparison and clustering of nonparametric curves. Depending on the application context, the curves of interest are densities, distribution, quantile or regression functions. We focus attention to the regression context. In particular, we consider a model with nonparametric regression functions which can be interpreted as nonparametric time trend curves. Since Stock and Watson (1988), there has been in particular a large literature in econometrics on testing for common trends. Our project aims to introduce and establish novel methods for the comparison of trend curves which are superior to existing methods in at least two respects: (a) Unlike the great bulk of existing methods, the proposed procedures are free of classical smoothing or bandwidth parameters that are notoriously difficult to select in practice. (b) The methods are much more informative than standard tests. They do not only allow to test whether the curves of interest are different. They also allow It also allows to detect, with a given statistical confidence, which time curves are different and in which regions of the support they differ. Based on our test method, we further construct an algorithm which clusters the observed time series into groups with the same trend.


%%%%%%%%%%%


%Testing for equality of nonparametric curves is a classical theme in econometrics and statistics. Depending on the application context, the curves of interest are densities, distribution, quantile or regression functions. A large number of different tests have been developed over the last few decades. 

%A classical theme in econometrics and statistics is the comparison of nonparametric curves. A number of different statistical methods have been developed to test for equality curves. Depending on the application context, the curves of interest are densities, distribution, quantile or regression functions. Since Stock and Watson (1988), there has been in particular a large literature on testing for the equality of trend curves. In this project, we 

%[Testing for equality of curves] A classical theme in econometrics and statistics is the comparison of nonparametric curves. The comparison is carried out by statistical tests for equality of curves. Most commonly, the curves of interest are densities, distribution and regression functions. The great bulk of tests extisting in the literature proceed in two steps: They first estimate the curves of interest by nonparametric methods and then construct a distance measure between the estimated curves. This distance measure serves as a test statistic. These tests all depend on a smoothing parameter. However, there is no theory available for a proper choice of the bandwidth/smoothing parameter. In particular, the optimal bandwidth for curve fitting cannot be used as optimal bandwidth for the testing problem. 

%A classical way out is to use empirical process theoryx and partial sum processes to get a bandwidth-free test statistic. A more modern way which is related to these partial sum processes are so-called multiscale tests. The idea is as follows: ?? However, multiscale tests for the comparison of nonparametric curves under general conditions are not available to the best of our knoweldge. One aim of the oprpject is to develop such a test for nonparametric regression curves. 

%[Clustering] In classical application, the number of curves to be compared is small. Indeed, much of the classical theory is restricted to tge case that there are two curves to be compared. In modern applications, large data sets are available which comprise information on a large number of curves. For example, there may dasta avaiable on a hundereds of different stocks or households which are characterized by some density or regression function. In this conztext, clasical tests for equality of curves are not an appropriate tool any more: In most aplications, it is very unliely that all individuals are characterized by the same curve. hence, quite likely at least some of the cruves are different. It is very natural to impose a group structure on the curves: One may assume that the curves that not each curve is completely different. There may rather be a small number of different curves, groups ??? Most clustering methods deopend on a number of bandwidths. Bandwidth free methods are very limited. In this project, we further develop a suggestions of ??. 



%\section{Scientific discipline and field of work} 
%
%Applied statistics, mathematical statistics, econometrics.

%\section{Scheduled total duration}

%2 years from ?? to ??.

%\section{Application period} 

%Start: ?? \\
%Finish: ??.


%\section{Summary}


%General topic of the project: Comparison and clustering of nonparametric curves. 

%Depending on the applications, the curves of interest may be densities, distribution, regression or quantile functions. 

%Since Stock and Watson (1988), there has been a large literature on comparing nonparametric trend curves. In the project, we focus attention to nonparametric trend curves, that is, to regression cruves with fixed design points. 

%The project develops and studies new methods for the comparison and clustering of nonparametric curves. Depending on the application, the curves of interest may be densities, distribution, regression or quantile functions. In the project, we focus attention on the case of regression functions as this is the prevalent case in econometrics. The modelling framework can be summarized as follows: We observe $n$ different data sets $\mathcal{D}_i$ for $1 \le i \le n$, where the data of the $i$-th sample $\mathcal{D}_i$ follow a nonparametric regression model with the unknown function $m_i$ and fixed design points. 

%Depending on the application context, the curves of interest may be densities, distibution, quantile or reuression functions among other things. In the econometrics context, most applications presumably involve regression functions. Since Stock and Watson (1988), there is in particular a large literautre in econometrics on testing for common trends. Trend functions can be regarded as regression functions with fixed equidistant design points. In our project, we concentrate on this situation as it is on the one hand interesting for many applications and on the otrher hand a good starting point for extensions.  

%Consider the following situation: Suppose there are $n$ different data sets $\mathcal{D}_i$ available for $1 \le i \le n$. Each data set corresponds to some unknown function $m_i$ which 

%Similar as before, our method does not only allow
%to test whether the null hypothesis is violated. It also allows to detect, with a given
%statistical confidence, which time trends are different and in which time regions they
%2differ from each other. Based on our test method, we further construct an algorithm
%which clusters the observed time series into groups with the same trend.

%method to test for equality of nonparametric curves. Moreover, we use this method to construct a clustering method 

%\section{Combined summary}



%\vspace{30pt}

%\noindent {\Large \bf B \hspace{0.2cm} Project description}
%\setcounter{section}{0}


\section{State of the art and preliminary work}

There exists a large and growing body of literature has investigated on modelling deterministic time trends in non- and semi-parametric settings. Most papers on nonparametric modelling involve kernels and, therefore, bandwidths. To date there has been little agreement on the solution to the bandwidth selection proble. In recent years, there has been an increasing interest in so-called multiscale tests, i.e. tests that are constructed  taking into account the predefined range of bandwidths. The obvious advantage of these tests is their independence of the choice of bandwidth. However, the literature on multiscale testing is not exhaustive.
<<<<<<< HEAD

{\bf For independent data, multiscale tests have been developed in a variety of different contexts in recent years. In the regression context, \cite{ChaudhuriMarron1999,ChaudhuriMarron2000} introduced the so-called SiZer method which has been extended in various directions; see e.g.\ \cite{HannigMarron2006} where a refined distribution theory for SiZer is derived. \cite{HallHeckman2000} constructed a multiscale test on monotonicity of a regression function. \cite{DuembgenSpokoiny2001} developed a multiscale approach which works with additively corrected supremum statistics and derived theoretical results in the context of a continuous Gaussian white noise model. Rank-based multiscale tests for nonparametric regression were proposed in \cite{Duembgen2002} and \cite{Rohde2008}. More recently, \cite{ProkschWernerMunk2018} have constructed multiscale tests for inverse regression models. In the context of density estimation, multiscale tests have been investigated in \cite{DuembgenWalther2008}, \cite{RufibachWalther2010}, \cite{SchmidtHieber2013} and \cite{EckleBissantzDette2017} among others.}

{\bf Whereas a large number of multiscale tests for independent data have been developed in recent years, multiscale tests for dependent data are much rarer. Most notably, there are some extensions of the SiZer approach to a time series context. \cite{Rondonotti2004} and \cite{Rondonotti2007} have introduced SiZer methods for dependent data which can be used to find local increases/decreases of a trend and which may thus be regarded as an alternative to our multiscale test. However, these SiZer methods are mainly designed for data exploration rather than for rigorous statistical inference. Our multiscale method, in contrast, is a rigorous level-$\alpha$-test of the hypo\-thesis $H_0$ which allows to make simultaneous confidence statements about the time regions where the trend $m$ is increasing/decreasing. Some theoretical results for dependent SiZer methods have been derived in \cite{ParkHannigKang2009}, but only under a quite severe restriction: Only time windows $[u-h,u+h]$ with window sizes or scales $h$ are taken into account that remain bounded away from zero as the sample size $T$ grows. Scales $h$ that converge to zero as $T$ increases are excluded. This effectively means that only large time windows $[u-h,u+h]$ are taken into consideration. Our theory, in contrast, allows to simultaneously consider scales $h$ of fixed size and scales $h$ that converge to zero at various different rates. We are thus able to take into account time windows of many different sizes.}

{\bf Our multiscale approach is also related to Wavelet-based methods: Similar to the latter, it takes into account different locations $u$ and resolution levels or scales $h$ simultaneously. However, while our multiscale approach is designed to test for local increases/decreases of a nonparametric trend, Wavelet methods are commonly used for other purposes. Among other things, they are employed for estimating/reconstructing nonparametric regression curves [see e.g.\ \cite{Donoho1995} or \cite{vonSachsMacGibbon2000}] and for change point detection [see e.g.\ \citet{ChoFryzlewicz2012}].}


%Cai (2007) studies a time-varying coefficient time series model with a time trend function and serially correlated errors to characterize the non-linearity, non-stationarity, and trending phenomenon. Robinson (2010) considers non-parametric trending regression in panel data models with cross-sectional dependence. Atak et al. (2011) propose a semi-parametric panel data model to model climate change in the United Kingdom (UK hereafter), where seasonal dummies enter the model linearly with heterogeneous coefficients and the time trend enters non-parametrically. Li et al. (2011) extend the work of Cai (2007) to panel data time-varying coefficient models. Chen et al. (2011, CGL hereafter) extend Robinson’s (2010) non-parametric trending panel data models to semi-parametric partially linear panel data models with crosssectional dependence where all individual unit share a common time trend that enters the model non-parametrically. They propose a semi-parametric profile likelihood approach to estimate the model.

Much of the current literature on deterministic time trends heavily relies on the critical assumption of the common trend structure. This assumption means that each individual in the panel exhibits the same trend behavior. Most of the works discussed above can not be easily generalized to the setting where the trend functions are not the same for different individuals. Therefore, it is vital to be able to test the assumption of the common trend before imposing it. 


Moreover, the comparison of trend curves is an important topic in many statistical applications. Economists, for example, are interested in comparing the trends of long-term interest rates for different countries. Moreover, they may want to assess whether the trends in real GDP growth differ across countries. In finance, massive amounts of data on thousands of stocks are available today. One question of interest is to compare how the volatility of different stocks evolves over time. Finally, in climatotology, large spatial data sets have been collected which comprise long temperature time series for many different locations. Climatologists are very much interested in analyzing the trending behaviour of these time series. In particular, they would like to know how the temperature trend varies across locations. 
=======

<<<<<<< HEAD
The comparison of nonparametric curves is a classical topic in econometrics and statistics. Depending on the application context, the curves of interest are densities, distribution functions, time trends or regression curves. The problem of testing for equality of densities has been studied for example in ??, ?? and ??. Tests for equality of distribution functions can be found in ??, ?? or ??. Tests for equality of trend and regression curves have been developed in ??, ?? and ?? among many others. In the proposed project, we focus on the comparison of nonparametric trend curves.

=======
{\bf For independent data, multiscale tests have been developed in a variety of different contexts in recent years. In the regression context, \cite{ChaudhuriMarron1999,ChaudhuriMarron2000} introduced the so-called SiZer method which has been extended in various directions; see e.g.\ \cite{HannigMarron2006} where a refined distribution theory for SiZer is derived. \cite{HallHeckman2000} constructed a multiscale test on monotonicity of a regression function. \cite{DuembgenSpokoiny2001} developed a multiscale approach which works with additively corrected supremum statistics and derived theoretical results in the context of a continuous Gaussian white noise model. Rank-based multiscale tests for nonparametric regression were proposed in \cite{Duembgen2002} and \cite{Rohde2008}. More recently, \cite{ProkschWernerMunk2018} have constructed multiscale tests for inverse regression models. In the context of density estimation, multiscale tests have been investigated in \cite{DuembgenWalther2008}, \cite{RufibachWalther2010}, \cite{SchmidtHieber2013} and \cite{EckleBissantzDette2017} among others.}

{\bf Whereas a large number of multiscale tests for independent data have been developed in recent years, multiscale tests for dependent data are much rarer. Most notably, there are some extensions of the SiZer approach to a time series context. \cite{Rondonotti2004} and \cite{Rondonotti2007} have introduced SiZer methods for dependent data which can be used to find local increases/decreases of a trend and which may thus be regarded as an alternative to our multiscale test. However, these SiZer methods are mainly designed for data exploration rather than for rigorous statistical inference. Our multiscale method, in contrast, is a rigorous level-$\alpha$-test of the hypo\-thesis $H_0$ which allows to make simultaneous confidence statements about the time regions where the trend $m$ is increasing/decreasing. Some theoretical results for dependent SiZer methods have been derived in \cite{ParkHannigKang2009}, but only under a quite severe restriction: Only time windows $[u-h,u+h]$ with window sizes or scales $h$ are taken into account that remain bounded away from zero as the sample size $T$ grows. Scales $h$ that converge to zero as $T$ increases are excluded. This effectively means that only large time windows $[u-h,u+h]$ are taken into consideration. Our theory, in contrast, allows to simultaneously consider scales $h$ of fixed size and scales $h$ that converge to zero at various different rates. We are thus able to take into account time windows of many different sizes.}

{\bf Our multiscale approach is also related to Wavelet-based methods: Similar to the latter, it takes into account different locations $u$ and resolution levels or scales $h$ simultaneously. However, while our multiscale approach is designed to test for local increases/decreases of a nonparametric trend, Wavelet methods are commonly used for other purposes. Among other things, they are employed for estimating/reconstructing nonparametric regression curves [see e.g.\ \cite{Donoho1995} or \cite{vonSachsMacGibbon2000}] and for change point detection [see e.g.\ \citet{ChoFryzlewicz2012}].}

>>>>>>> vogt

%Cai (2007) studies a time-varying coefficient time series model with a time trend function and serially correlated errors to characterize the non-linearity, non-stationarity, and trending phenomenon. Robinson (2010) considers non-parametric trending regression in panel data models with cross-sectional dependence. Atak et al. (2011) propose a semi-parametric panel data model to model climate change in the United Kingdom (UK hereafter), where seasonal dummies enter the model linearly with heterogeneous coefficients and the time trend enters non-parametrically. Li et al. (2011) extend the work of Cai (2007) to panel data time-varying coefficient models. Chen et al. (2011, CGL hereafter) extend Robinson’s (2010) non-parametric trending panel data models to semi-parametric partially linear panel data models with crosssectional dependence where all individual unit share a common time trend that enters the model non-parametrically. They propose a semi-parametric profile likelihood approach to estimate the model.

Much of the current literature on deterministic time trends heavily relies on the critical assumption of the common trend structure. This assumption means that each individual in the panel exhibits the same trend behavior. Most of the works discussed above can not be easily generalized to the setting where the trend functions are not the same for different individuals. Therefore, it is vital to be able to test the assumption of the common trend before imposing it. 


Moreover, the comparison of trend curves is an important topic in many statistical applications. Economists, for example, are interested in comparing the trends of long-term interest rates for different countries. Moreover, they may want to assess whether the trends in real GDP growth differ across countries. In finance, massive amounts of data on thousands of stocks are available today. One question of interest is to compare how the volatility of different stocks evolves over time. Finally, in climatotology, large spatial data sets have been collected which comprise long temperature time series for many different locations. Climatologists are very much interested in analyzing the trending behaviour of these time series. In particular, they would like to know how the temperature trend varies across locations. 
>>>>>>> ca3deda860260ecca959ad7fd2239415fd4598b7

The statistical problem of comparing trends has a wide range of applications in economics, finance and other fields such as climatology and biology. In economics, a common issue is to compare trends in real GDP across different countries. It is highly debated whether real GDP growth is faster in economies in transition (such as Brazil, China or India) than in developed countries (cp.\ ??). Another example concerns the dynamics of long-term interest rates. To better understand these dynamics, researchers aim to compare the yields of US Treasury bills at different maturities over time (cp.\ ??). In finance, it is also of interest to compare the volatility trends of different stocks (cp.\ ??). A final example comes from climatotology. In recent years, large spatial data sets have been collected which comprise long temperature time series for many different locations. Climatologists are very much interested in analyzing the trending behaviour of these time series (cp.\ ??). In particular, they would like to know how the temperature trend varies across locations. 


Classically, time trends are modelled stochastically in econometrics; see e.g.\ \cite{Stock1988}. Recently, however, there has been a growing interest in econometric models with deterministic time trends; see \cite{Cai2007}, \cite{Atak2011}, \cite{Robinson2012} and \cite{ChenGaoLi2012} among others. Non- and semiparametric trend modelling has attracted particular interest in a panel data context. \cite{LiChenGao2010}, \cite{Atak2011}, \cite{Robinson2012} and \cite{ChenGaoLi2012} considered panel models where the observed time series have a common time trend. In many applications, however, the assumption of a common time trend is quite harsh. In particular when the number of observed time series is large, it is quite natural to suppose that the time trend may differ across time series. More flexible panel settings with heterogeneous trends have been studied, for example, in \cite{Zhang2012} and \cite{Hidalgo2014}. 


%In the proposed project, 
In what follows, we consider a general panel framework with heterogeneous trends which is useful for a number of economic and financial applications: 
Suppose we observe a panel of $n$ time series $\mathcal{Z}_i = \{ (Y_{it},X_{it}): 1 \le t \le T \}$ for $1 \le i \le n$, where $Y_{it}$ are real-valued random variables and $X_{it} = (X_{it,1},\ldots,X_{it,d})^\top$ are $d$-dimensional random vectors. Each time series $\mathcal{Z}_i$ is modelled by the equation
\begin{equation}\label{model}
Y_{it} = m_i \Big( \frac{t}{T} \Big) + \beta_i^\top X_{it} + \alpha_i + \varepsilon_{it}
\end{equation}
for $1 \le t \le T$, where $m_i: [0,1] \rightarrow \mathbb{R}$ is a nonparametric (deterministic) trend function, $X_{it}$ is a vector of regressors or controls and $\beta_i$ is the corresponding parameter vector. Moreover, $\alpha_i$ are so-called fixed effect error terms and $\varepsilon_{it}$ are standard regression errors with $\ex[\varepsilon_{it}|X_{it}] = 0$ for all $t$. Model \eqref{model} nests a number of panel settings which have recently been considered in the literature. Special cases of model \eqref{model} with a nonparametric trend specification are for example considered in \cite{Atak2011}, \cite{Zhang2012} and \cite{Hidalgo2014}. Versions of model \eqref{model} with a parametric trend are studied in \cite{Vogelsang2005}, \cite{Sun2011} and \cite{Xu2012} among others.


As usual in nonparametric regression, the trend functions $m_i$ in model \eqref{model} depend on rescaled time $t/T$ rather than on real time $t$; cp.\ ??, ?? and ?? for the use and some discussion of the rescaled time argument. The functions $m_i$ are only identified up to an additive constant in model \eqref{model}: One can reformulate the model equation as $Y_{it} = [m_i(t/T) + c_i] + \beta_i^\top X_{it} + [\alpha_i - c_i] + \varepsilon_{it}$, that is, one can freely shift additive constants between the trend $m_i(t/T)$ and the error component $\alpha_i$. In order to obtain identification, one may impose different normalization constraints on the trends $m_i$. One possibility is to normalize them such that $\int_0^1 m_i(u) du = 0$ for all $i$. In what follows, we take for granted that the trends $m_i$ satisfy this constraint. 
%Model \eqref{model} nests a number of panel settings which have recently been considered in the literature. Special cases of model \eqref{model} with a nonparametric trend specification are for example considered in \cite{Atak2011}, \cite{Zhang2012} and \cite{Hidalgo2014}. Versions of model \eqref{model} with a parametric trend are studied in \cite{Vogelsang2005}, \cite{Sun2011} and \cite{Xu2012} among others. 
Within the general framework of model \eqref{model}, we can formulate a number of interesting statistical questions concerning the trend functions $m_i$ for $1 \le i \le n$. 
\vspace{15pt}


\noindent \textbf{(a) Testing for equality of nonparametric trend curves } 
\vspace{10pt} 

 
<<<<<<< HEAD
\noindent In many application contexts, an important question is whether the time trends $m_i$ are all the same. Put differently, the question is whether the observed time series have a common trend. This question can formally be adressed by a statistical test of the null hypothesis 
\[ H_0: \text{There exists a function } m: [0,1] \rightarrow \mathbb{R} \text{ such that } m_i = m  \text{ for all } 1 \le i \le n. \]
A closely related question is whether all time trends have the same parametric form. To formulate the corresponding null hypothesis, let $m(\theta,\cdot): [0,1] \rightarrow \mathbb{R}$ be a function which is known up to the finite-dimensional parameter $\theta_0 \in \Theta$, where $\Theta$ denotes the parameter space. The null hypothesis of interest now reads as follows:  
\[ H_{0,\text{para}}: \text{ There exists } \theta \in \Theta \text{ such that } m_i(\cdot) = m(\theta,\cdot) \text{ for all } 1 \le i \le n. \]  
If $m(\theta,w) = a + b w$ with $\theta = (a,b)$, for example, then $H_0$ is the hypothesis that all trends $m_i$ are linear with the same intercept $a$ and slope $b$. A somewhat simpler but yet important hypothesis is given by 
\[ H_{0,\text{const}}: m_i \equiv 0 \text{ for all } 1 \le i \le n. \]
Under this hypothesis, there is no time trend at all in the observed time series. Put differently, all the time trends $m_i$ are constant. (Note that under the normalization constraint $\int_0^1 m_i(w) dw = 0$, $m_i$ must be equal to zero if it is a constant function.) A major aim of our project is to develop new tests for the hypotheses $H_0$, $H_{0,\text{para}}$ and $H_{0,\text{const}}$ in model \eqref{model}. In order to keep the exposition focused, we restrict attention to the hypothesis $H_0$ in what follows. Tests of $H_{0,\text{para}}$, $H_{0,\text{const}}$ and related hypotheses were for example studied in \cite{Lyubchich2016} and \cite{ChenWu2018}. 
%A closely related question is whether there is no time trend at all in the observed time series. Put differently, the question is whether the time trends $m_i$ are constant for all $i$. The corresponding null hypothesis can be formulated as 
%\[ H_{0, \text{const}}: \text{There exists some constant } c \text{ such that } m_i \equiv c \text{ for all } 1 \le i \le n. \]
%There are also other interesting hypotheses that one may test in this context. For example, one may be interested in the hypothesis whether the trends $m_i$ are all linear or, more generally, whether they all have the same parametric form. Tests of such hypotheses were for example studied in \cite{Lyubchich2016} and \cite{ChenWu2018}. In order to keep the exposition focused, we restrict attention to the hypothesis $H_0$ in what follows.  


In recent years, a number of different approaches have been developed to test the hypothesis $H_0$. \cite{DegrasWu2012} considered the problem of testing $H_0$ within the model framework
=======
\noindent The first question is the following: Are all time trends $m_i$ the same? That is, do all time series in the sample exhibit the same trending behaviour? This question can be approached formally by means of a statistical test. The null hypothesis can be formulated as $H_0: m_1 = \ldots = m_n$. 
\begin{itemize}[label=--,leftmargin=0.5cm]
\item \cite{Stock1988} is one of the first papers to compare trend curves in a multiple time series. However, the focus of the author's attention is rather on stochastic trends than deterministic ones. The authors develop two tests for detecting common stochastic trends in a number of time series. They apply these tests to the economic time series, in particularly, the postwar U.S. data on the federal funds rate and the three- and twelve-month treasury bill rates. All three time series on inerest rates appear to share a common stochastic trend.

%Both tests involve the roots of the ordinary least squares coefficient matrix obtained by regressing the series onto its first lag. 
\item \cite{Vogelsang2005} consider a simple linear model
\begin{equation}\label{model-vogelsang}
Y_{it} =\alpha_i + \beta_i t + \varepsilon_{it}, \quad i=1, \ldots, n, \quad t=1, \ldots, T,
\end{equation}
where $\beta_i t$ is a linear trend function, $\alpha_i$ are so-called fixed effect error terms and $\varepsilon_{it}$ are standard regression errors such that a functional central limit theorem is applicable to $\{\varepsilon_{it}\}$. This model can be considered as a special case for our model \eqref{model} with nonparametric time curve $m_i(t)$ being linear in $t$. The authors propose two $F-$tests and a $t$-test to test the null hypothesis 
\begin{equation}\label{hypothesis-vogelsang}
H_0: R\beta = r,
\end{equation}
where $R$ is $q \times n$ known deterministic matrix and $r$ is $q\times 1$ known deterministic vector. They derive an asymptotic theory for these tests and provide relevant critical values. As an empirical application, the authors compare the postwar European time series of gross domestic product (GDP) to the time series of GDP in Italy. They reject the null hypothesis that the rates of growth between Italy and 6 other European countries in the years 1950 to 1992 are the same.

%A key issue is the estimation of the asymptotic covariance matrix, for which we aim to compare three different approaches, amongst which is the familiar heteroskedasticity autocorrelation consistent (HAC) estimator. The other two approaches are new andare basedon extensions of the approach proposedby Kiefer and Vogelsang (2002a). 
\item \cite*{Park2008}, \cite*{ParkHannigKang2009} and \cite*{Park2009} extend the well-known SiZer method for analyzing one time series, which was originally proposed by \cite{ChaudhuriMarron1999}, and advanced as a procedure to analyze time series \cite{Rondonotti2007}. In \cite*{Park2008}, only the model with independent idiosyncratic errors $\{\varepsilon_{it}\}$ is considered, whereas in \cite*{ParkHannigKang2009} and \cite*{Park2009} the errors are allowed to be dependent across $t$. The authors use a regression function estimation to fit a local linear function to obtain a kernel etimate. This kernel estimate depends on a location $x$ and a bandwidth $h$. The proposed SiZer method then uses the color map to display the significance of differences between two regression functions for a range of locations $x$ and locations $h$. This is a useful graphical tool that can indicate the regions where the differences between trend curves should be investigated further. However, this method has its limitations since the comparison should be done only pairwise

%for example seismic recordings of earthquakes and nuclear explosions, gait analysis, temperature precipitation patterns, brain potentials evoked by flashes of light, packet/byte counts in Internet traffic, and so on.
\item \cite*{DegrasWu2012} is a seminal paper on the parallelism between the deterministic trends in multiple time series. The model considered is 
>>>>>>> ca3deda860260ecca959ad7fd2239415fd4598b7
\begin{equation}\label{model-degras}
Y_{it} = m_i \Big( \frac{t}{T} \Big) + \alpha_i + \varepsilon_{it} \qquad (1 \le t \le T, \, 1 \le i \le n), 
\end{equation}
where $\mathbb{E}[\varepsilon_{it}] = 0$ for all $i$ and $t$ and the terms $\alpha_i$ are assumed to be deterministic. Obviously, \eqref{model-degras} is a special case of \eqref{model} which does not include additional regressors. \cite{DegrasWu2012} construct an $L_2$-type statistic to test $H_0$. The statistic is based on nonparametric kernel estimators $\hat{m}_{i,h}$ and $\hat{m}_h$ of the functions $m_i$ and $m$, where $h$ denotes the bandwidth parameter. With the help of these estimators, the authors define the statistic
\[ \Delta_{n,T} = \sum_{i=1}^n \int_0^1 \big(\hat{m}_i(u) - \hat{m}(u)\big)^2 du, \] 
which measures the $L_2$-distance between the estimators $\hat{m}_i$ and $\hat{m}$. In the theoretical part of their paper, they derive the limit distribution of $\Delta_{n,T}$. 
%In particular, they prove that $T h^{1/2} (n-1)^{-1/2} [ \Delta_{n,T} - \mathbb{E} \Delta_{n,T} ]$ is asymptotically normal. 
\cite{ChenWu2018} develop theory for test statistics closely related to those from \cite{DegrasWu2012}, however, under more general conditions on the error processes $\mathcal{E}_i = \{ \varepsilon_{it}: 1 \le t \le T \}$. 


<<<<<<< HEAD
\cite{Zhang2012} investigate the problem of testing the hypothesis $H_0$ in a slightly restricted version of model \eqref{model}, where $\beta_i = \beta$ for all $i$. The regression coefficients $\beta_i$ are thus assumed to be homogeneous in their setting. They construct a residual based-test of $H_0$: \textcolor{red}{\textbf{[Add details!]}}


The tests of \cite{Zhang2012}, \cite{DegrasWu2012} and \cite{ChenWu2018} are based on nonparametric estimators of the trend functions $m_i$. They thus depend on one or several bandwidth parameters required to estimate the curves $m_i$. It is however far from clear how to choose these bandwidths in an appropriate way. This is a quite general problem which concerns essentially all tests that are based on nonparametric curve estimators. There are of course many theoretical results on the optimal choice of bandwidth for estimation purposes. However, the optimal bandwidth for curve estimation is usually not optimal for testing. Optimal bandwidth choice for tests is indeed a quite open problem, and only little theory for simple cases is available, cp.\ \cite{GaoGijbels2008}. Since tests based on nonparametric curve estimators are commonly quite sensitive to the choice of bandwidth and theory for optimal bandwidth selection is not available, it appears preferable to work with bandwidth-free tests. 


A classical way to obtain a bandwidth-free test of the hypothesis $H_0$ is to use CUSUM-type statistics which are based on partial sum processes. This approach is taken in \cite{Hidalgo2014}. \textcolor{red}{\textbf{[Add details!]}}
=======
%For example in longitudinal clinical studies, evaluators are interested in comparing response curves for treatment and control groups. In agriculture, it may be relevant to compare at different spatial locations the relationship between yield per plant and plant density. In biology, assessing parallelism between sets of dose-response data allows to determine if the biological response to two substances is similar or if two different biological environments give similar dose-response curves to the same substance. Another interesting application of comparing trends in cell phone activity pertains to the allocation of bandwidth in phone networks.
\item \cite{Sun2011} considers the same model as in \eqref{model-vogelsang} but focuses on the estimation of the long run variance and its influence on the asymptotic behavior of the OLS esimator of the coefficient. The authors estimate the long run variance matrix by a series type estimator with $K$ basis functions. As in \cite*{Vogelsang2005}, the authors test the null hypothesis \eqref{hypothesis-vogelsang} but employ Wald statistic instead of $F$-test or $t$-test. They prove that the asymptotic distribution of the Wald statistic converges to a standard distribution when $K$ is fixed and when $K$ is growing. The authors also provide an algorithm to select $K$, which is developed such that it minimizes the type II error whilst controlling for the type I error.

%It is shown that critical values from the fixed-K asymptotics are second order correct under the large-K asymptotics. The new test therefore combines the advantages of the nonstandard test and the standard Wald test while avoiding their main disadvantages (power loss and size distortion, respectively).
\item \cite{Xu2012} expands the framework in \cite*{Vogelsang2005} by considering the same model \eqref{model-vogelsang} but allowing the errors $\varepsilon_{it}$ to follow a semi-parametric vector autoregressive (VAR) process with nonstationary volatility. In this setting the conventional $F$-tests and the $t$-test proposed in \cite*{Vogelsang2005} are generally non-pivotal in the limit, hence, the authors propose robust tests to overcome this issue. However, the performance of these test suffers from size distortions in small samples. As a result, the authors propose two residual-based bootstrap procedures as a solution to this problem.

%involving the unknown timevarying volatility function in the limit. This can be explained by either of the two facts. First, under nonstationary volatility the nonrobust standard errors incorrectly estimate the asymptotic variances of the trend coefficients. This is analogous to the failure of the traditional t-test in the presence of heteroskedasticity. Second, the maintained assumption in VF that a standard invariance principle holds for model innovations is violated under nonstationary volatility.
\item \cite*{Zhang2012} considers the model \eqref{model}.
<<<<<<< HEAD


%This paper proposes a non-parametric test for common trends in semi-parametric
%panel data models with fixed effects based on a measure of non-parametric goodness-of-fit
%(R2). We first estimate the model under the null hypothesis of common trends by the method
%of profile least squares, and obtain the augmented residual which consistently estimates the
%sum of the fixed effect and the disturbance under the null. Then we run a local linear regression
%of the augmented residuals on a time trend and calculate the non-parametric R2 for each
%cross-section unit. The proposed test statistic is obtained by averaging all cross-sectional nonparametric R2s, which is close to 0 under the null and deviates from 0 under the alternative.
%We show that after appropriate standardization the test statistic is asymptotically normally
%distributed under both the null hypothesis and a sequence of Pitman local alternatives. We
%prove test consistency and propose a bootstrap procedure to obtain P-values. Monte Carlo
%simulations indicate that the test performs well in finite samples. Empirical applications
%are conducted exploring the commonality of spatial trends in UK climate change data and
%idiosyncratic trends in OECD real GDP growth data. Both applications reveal the fragility of
%the widely adopted common trends assumption
=======
>>>>>>> vogt


%This paper proposes a non-parametric test for common trends in semi-parametric
%panel data models with fixed effects based on a measure of non-parametric goodness-of-fit
%(R2). We first estimate the model under the null hypothesis of common trends by the method
%of profile least squares, and obtain the augmented residual which consistently estimates the
%sum of the fixed effect and the disturbance under the null. Then we run a local linear regression
%of the augmented residuals on a time trend and calculate the non-parametric R2 for each
%cross-section unit. The proposed test statistic is obtained by averaging all cross-sectional nonparametric R2s, which is close to 0 under the null and deviates from 0 under the alternative.
%We show that after appropriate standardization the test statistic is asymptotically normally
%distributed under both the null hypothesis and a sequence of Pitman local alternatives. We
%prove test consistency and propose a bootstrap procedure to obtain P-values. Monte Carlo
%simulations indicate that the test performs well in finite samples. Empirical applications
%are conducted exploring the commonality of spatial trends in UK climate change data and
%idiosyncratic trends in OECD real GDP growth data. Both applications reveal the fragility of
%the widely adopted common trends assumption
>>>>>>> ca3deda860260ecca959ad7fd2239415fd4598b7


A more modern way to obtain a test statistic which is free of classical bandwidth parameters is to use multiscale methods. The general idea is as follows: Let $S_h$ be a test statistic for the null hypothesis of interest, which depends on the bandwidth $h$. Rather than considering only a single statistic $S_h$ for a specific bandwidth $h$, a multiscale approach simultaneously considers a whole family of statistics $\{S_h: h \in \mathcal{H} \}$, where $\mathcal{H}$ is a set of bandwidth values. The multiscale test then proceeds as follows: For each bandwidth or scale $h$, one checks whether $S_h > q_h(\alpha)$, where $q_h(\alpha)$ is a bandwidth-dependent critical value (for given significance level $\alpha$). The multiscale test rejects if $S_h > q_h(\alpha)$ for at least one scale $h$. The main theoretical difficult in this approach is of course to derive appropriate critical values $q_h(\alpha)$. 


%The general idea of multiscale tests is to simultaneously consider a whole family of test statistics which correspond to different bandwidths. A more modern way to obtain a test statistic which is free of classical bandwidth parameters is to use multiscale methods. To illustrate this idea, we consider the simple trend model $Y_{it} = m_i(t/T) + \varepsilon_{it}$ for $1 \le t \le T$ and $1 \le i \le n$. For simplicity of exposition, we suppose that $n=2$ and that the errors $\varepsilon_{it}$ are i.i.d.\ with zero mean and unit variance. We want to test the null hypothesis $H_0: m_1 = m_2$. Let $H_0(u,h)$ be the hypothesis that $m_1$ and $m_2$ are the same on the interval $[u-h,u+h]$. Obviously, $H_0$ holds true if and only if $H_0(u,h)$ is fulfilled for any $u \in [0,1]$ and $h > 0$.  We now attempt to construct a procedure which tests the hypothesis $H_0(u,h)$ simultaneously for all $u$ and $h$.\footnote{In practice, we can of course not consider all $u \in [0,1]$ and $h > 0$ but have to take a finite subset thereof.} To achieve this, we proceed in two steps: 
%\begin{enumerate}
%\item We construct a test statistic $\Delta_T(u,h)$ for the hypothesis $H_0(u,h)$ for given $u$ and $h$. In particular, we let $\Delta_T(u,h) = \sqrt{Th} |\hat{m}_{1,h}(u) - \hat{m}_{2,h}(u)|$, where $\hat{m}_{i,h}(u)$ is a standard (local constant or local linear) kernel estimator of $m_i$ at location $u$ with bandwidth $h$. 
%\item We aggregate the individual test statistics $\Delta_T(u,h)$ into one overall statistic $\Delta_T$. As an example, we may simple aggregate the statistics $\Delta_T(u,h)$ by taking the supremum $\sup_{u,h} \Delta_T(u,h)$. In recent years, more intriciate aggregation schemes have been developed which are superior to a simple supremum, cp.\ for example \cite{DuembgenSpokoiny2001}.  
%\end{enumerate}
%$\Delta_T$ is a so-called multiscale statistic as it simulteneously takes into accoiun multple locations $u$ and bandwidths or scales $h$. The  ain theoretical chalnnege is to appropriate critical values for the multiscale statistic. 


One of the first multiscale methods proposed in the literature is the SiZer approach of \cite{ChaudhuriMarron1999, ChaudhuriMarron2000}. In recent years, this approach has been extended in various directions; see \cite{HannigMarron2006} and ?? among others. \cite{Park2009} developed SiZer methods for the comparison of nonparametric trend curves in a simplified version of model \eqref{model}. Their analysis, however, is mainly methodological and only partly backed up by theory. Indeed, theory is only derived for the special case $n=2$, that is, for the case that only two time series are observed. Moreover, the theoretical results are only valid under very severe restrictions on the set of bandwidths $\mathcal{H}$ that is taken into account. In particular, the bandwidths in the set $\mathcal{H}$ are assumed to be bounded away from zero. Put differently, they are not allowed to converge to zero as the sample size grows, which is obviously a very severe limitation. 


A major aim of our project is to develop new multiscale tests of the hypothesis $H_0$ in the general model \eqref{model} which do not have the limitations of the SiZer methods discussed above. Importantly, we do not only intend to develop new test methodology but also to back up the methods by a general asymptotic distribution theory. To achieve this, we plan to build on a multiscale approach pioneered by \cite{DuembgenSpokoiny2001}. This general approach has been very influential in recent years and has been further developed in numerous directions; see for example \cite{Duembgen2002}, \cite{Rohde2008} and \cite{ProkschWernerMunk2018} for multiscale methods in the regression context and \cite{DuembgenWalther2008}, \cite{RufibachWalther2010}, \cite{SchmidtHieber2013} and \cite{EckleBissantzDette2017} for methods in the context of density estimation. Importantly, all of these studies are limited to the case of independent data. It turns out that it is highly non-trivial to extend the methods to the case of dependent data. To do so, markedly different technical tools are needed. A first step to provide such tools has recently been made in \cite{KhismatullinaVogt2018}. They developed multiscale methods for testing shape restrictions of the nonparametric trend function $m$ in the univariate time series model $Y_t = m(t/T) + \varepsilon_t$. In our project, we aim to make further progress in this direction. Please see Section ?? on objectives for the details. 
\vspace{15pt}


\noindent \textbf{(b) Clustering of nonparametric trend curves} 
\vspace{10pt} 


\noindent Consider the general panel data model \eqref{model} and suppose that the null hypothesis $H_0: m_1 = \ldots = m_n$ is violated in this model. Even though some of the trend functions $m_i$ are different in this case, there may still be groups of time series with the same time trend. Formally, a group stucture can be defined as follows within the framework of model \eqref{model}: There exist sets or groups of time series $G_1,\ldots,G_{K_0}$ with $\{1,\ldots,n\} = \dot\bigcup_{k=1}^{K_0} G_k$ such that for each $1 \le k \le K_0$, 
\begin{equation}\label{model-groups}
m_i = m_j \quad \text{for all } i,j \in G_k. 
\end{equation}
Hence, the time series of a given group $G_k$ all have the same time trend. %In many applications, it is quite natural to suppose that there is a group structure in the data. 
An interesting statistical problem is how to estimate the unknown groups $G_1,\ldots,G_{K_0}$ and their unknown number $K_0$ from the data. 


There are several approaches to this problem in the context of models closely related to \eqref{model}. \cite{DegrasWu2012} used a repeated testing procedure based on the methods described in part (a) of this section to estimate the unknown group structure in model \eqref{model-degras}. \cite{Zhang2013} developed a clustering method within the same model framework which makes use of information criteria. \textcolor{red}{\textbf{[Add details!]}} \cite{VogtLinton2017} constructed a thresholding method to estimate the unknown group structure in the panel model $Y_{it} = m_i(X_{it}) + \alpha_i + \varepsilon_{it}$, where $X_{it}$ are random regressors. Their approach can also be adapted to the case of fixed regressors $X_{it} = t/T$. 


The problem of estimating the unknown groups $G_1,\ldots,G_{K_0}$ and their unknown number $K_0$ in model \eqref{model} has close connections to functional data clustering. There, the aim is to cluster smooth random curves that are functions of (rescaled) time and that are observed with or without noise. A number of different clustering approaches have been proposed in the context of functional data models; see for example \cite{Abraham2003}, \cite{Tarpey2003} and \cite{Tarpey2007} for procedures based on $k$-means clustering, \cite{James2003} and \cite{Chiou2007} for model-based clustering approaches and \cite{Jacques2014} for a recent survey. 


The problem of finding the unknown group structure in model \eqref{model} is also closely related to a developing literature in econometrics which aims to identify unknown group structures in parametric panel regression models. In its simplest form, the panel regression model under consideration is given by the equation $Y_{it} = \beta_i^\top X_{it} + u_{it}$ for $1 \le t \le T$ and $1 \le i \le n$, where the coefficient vectors $\beta_i$ are allowed to vary across individuals $i$. Similarly as the trend functions in model \eqref{model}, the coefficients $\beta_i$ are assumed to belong to a number of groups: there are $K_0$ groups $G_1,\ldots,G_{K_0}$ such that $\beta_i = \beta_j$ for all $i,j \in G_k$ and all $1\le k \le K_0$. The problem of estimating the unknown groups and their unknown number has been studied in different versions of this modelling framework; cp.\ \cite{Su2016}, \cite{Su2018} and \cite{Wang2018} among others. \cite{Bonhomme2015} considered a related model where the group structure is not imposed on the regression coefficients but rather on the unobserved time-varying fixed effects. 


%Alternatively to a group structure, factor-type structures are often consiudered for the regression coefficients or regression functions in  panel models. Such factor-type structures have been studied in ??, ?? and ?? among others. 
%Whether it is meaningful to impose a group structure on the panel model under consideration depends of course on the application context. In some applications, a group structure is very natural to assume; in others, one may impose other structures on the panel. A prominent examples are factor-type structure. To be more specific, consider the $Y_{it} = m_i(X_{it}) + u_{it}$, where $X_{it}$ are random or deterministic design points and $u_{it}$ are generic error terms. As a special case, one may have $X_{it} = t/T$ in this model. Rather than imposing a group structure on the functions $m_i$, one may work with a factor-structure of the following kind: The functions $m_i(w)$ have the form $m_i(w) = \beta_i^\top g(w)$, where $\beta_i = (\beta_{i1},\ldots,\beta_{iK_0})^\top$ are coefficient vectors and $g = (g_1,\ldots,g_{K_0})^\top$ is a vector of functions. Here, the functions $g$ can be interpreted as common factors and $\beta_i$ play the role of the factor loadings. Such factor-type structures have been considered in ??, ?? and ?? in a panel context.  


Virtually all of the proposed procedures to cluster nonparametric curves in panel and functional data models related to \eqref{model} have the following drawback: they depend on a number of bandwidths or smoothing parameters required to estimate the nonparametric functions $m_i$. 
%A common approach is to approximate the functions $m_i$ by a series expansion $m_i(x) \approx \sum_{j=1}^{L} \beta_{ij} \phi_j(x)$, where $\{ \phi_j: j =1,2,\ldots \}$ is a function basis and $L$ is the number of basis elements taken into account for the estimation of $m_i$. Here, $L$ plays the role of the smoothing parameter and may vary across $i$, that is, $L = L_i$. To estimate the classes $G_1,\ldots,G_{K_0}$, estimators $\hat{\beta}_i$ of the coefficient vectors $\beta_i = (\beta_{i1},\ldots,\beta_{iL})^\top$ are clustered into groups by a standard clustering algorithm. Variants of this approach have for example been investigated in \cite{Abraham2003}, \cite{Chiou2007} and \cite{Tarpey2007}. Another approach is to compute nonparametric estimators $\hat{m}_{i} = \hat{m}_{i,h}$ of the functions $m_i$ for some smoothing parameter $h$ (which may differ across $i$) and to calculate distances $\hat{\rho}_{ij} = \rho(\hat{m}_{i},\hat{m}_{j})$ between the estimates $\hat{m}_{i}$ and $\hat{m}_{j}$, where $\rho(\cdot,\cdot)$ is a distance measure such as a supremum or an $L_2$-distance. A distance-based clustering algorithm is then applied to the distances $\hat{\rho}_{ij}$. This strategy has for example been used in \cite{VogtLinton2017}. 
In general, nonparametric curve estimators strongly depend on the chosen bandwidth parameters. A clustering procedure which is based on such estimators can be expected to be strongly influenced by the choice of bandwidths as well. Moreover, as in the context of statistical testing, there is no theory available on how to pick the bandwidths optimally for the clustering problem. Hence, as in the context of testing, it is desirable to construct a clustering procedure which is free of classical bandwidth parameters. 


There are different ways to move into the direction of a bandwidth-free clustering algorithm. One possibility is to employ Wavelet methods. A Bayesian Wavelet-based method to cluster nonparametric curves has been developed in \cite{Ray2006}. There, the simple model $Y_{it} = m_i(t/T) + u_{it}$ is considered, where $m_i$ are smooth functions of rescaled time $t/T$ and the error terms $u_{it}$ are restricted to be i.i.d.\ Gaussian noise. 


Another possibility is to use multiscale methods. This approach has recently been taken in \cite{VogtLinton2018}. They develop a clustering approach within the framework of the panel regression model $Y_{it} = m_i(X_{it}) + u_{it}$, where $X_{it}$ are random regressors and $u_{it}$ are general error terms that may include fixed effects. Imposing the same group structure as in \eqref{model-groups} on their  model, they construct estimators of the unknown groups and their unknown number as follows: In a first step, they develop a multiscale statistic $\hat{d}_{ij}$ which measures the distance between any two functions $m_i$ and $m_j$. In a second step, the distance measures $\hat{d}_{ij}$ are used as the basis of a hierarchical clustering algorithm. In the theoretical part of their paper, they derive some consistency results for their estimators. Letting $\hat{K}_0$ be the estimator of $K_0$ and $\{ \hat{G}_1,\ldots,\hat{G}_{\hat{K}_0} \}$ the estimator of the group structure $\{ G_1,\ldots,G_{K_0} \}$, they in particular show that under appropriate regularity conditions, 
\begin{equation}\label{consistency-res-VogtLinton2018}
\pr \big( \hat{K}_0 = K_0 \big) \rightarrow 1 \quad \text{and} \quad \pr \Big( \{ \hat{G}_1,\ldots,\hat{G}_{\hat{K}_0} \} = \{ G_1,\ldots,G_{K_0} \} \Big) \rightarrow 1 
\end{equation}
as the sample size goes to infinity. %According to \eqref{consistency-res-VogtLinton2018}, the estimators are identical to their true counterparts with probability tending to $1$. 
Even though promising, the consistency result \eqref{consistency-res-VogtLinton2018} is only a first step into the direction of a complete asymptotic theory. A more refined theory would comprise results on convergence rates and confidence statements about the estimators. 


Building on the work of \cite{VogtLinton2018}, we aim to develop multiscale clustering methods in model \eqref{model}. We in particular aim to go beyond the basic theory developed in \cite{VogtLinton2018} and to provide results on convergence rates and confidence statements. We give more details on these objectives in Section ??.  


\subsection{Project-related publications}


\subsubsection{Articles published by outlets with scientific quality assurance, book publications, and works accepted for publication but not yet published}

\hangindent=0.4cm \textsc{Vogt}, M. and \textsc{Linton}, O. (2017). Classification of non-parametric regression functions in longitudinal data models. \textit{Journal of the Royal Statistical Society: Series B}, \textbf{79} 5-27.


\subsubsection{Other publications}

\hangindent=0.4cm \textsc{Khismatullina}, M. and \textsc{Vogt}, M. (2018). Multiscale inference and long-run variance estimation in nonparametric regression with time series errors. \textit{Preprint}.

\vspace{5pt}

\noindent \hangindent=0.4cm \textsc{Vogt}, M. and \textsc{Linton}, O. (2018). Multiscale clustering of nonparametric regression curves. \textit{Preprint}. 



\section{Objectives and work programme}


\subsection{Anticipated total duration of the project}


2 years from 01.10.2019 to 30.09.2021


\subsection{Objectives}


The main aim of the project is to develop new methods and theory for the comparison and clustering of nonparametric trend curves. As a modelling framework, we will consider the general panel setting \eqref{model} which was briefly introduced in Section ??:  Suppose we observe a panel of $n$ time series $\mathcal{Z}_i = \{(Y_{it},X_{it}): 1 \le t \le T\}$ for $ 1 \le i \le n$. Each time series $\mathcal{Z}_i$ is modelled by the equation 
\begin{equation}\label{model-objectives}
Y_{it} = m_i \Big( \frac{t}{T} \Big) + \beta_i^\top X_{it} + \alpha_i + \varepsilon_{it} 
\end{equation}
for $ 1 \le t \le T$, where $m_i$ is a nonparametric time trend curve, $X_{it}$ is a vector of regressor or control variables, $\alpha_i$ are unobserved fixed effects and $\varepsilon_{it}$ are idiosyncratic error terms with $\mathbb{E}[\varepsilon_{it}|X_{it} ] = 0$. For each $i$, $\mathcal{P}_i = \{(X_{it},\varepsilon_{it})\}$ is assumed to be a general time series process which fulfills some weak dependence conditions (e.g.\ conditions formulated in terms of strong mixing coefficients or in terms of the physical dependence measure introduced by Wu ??). We will not only allow for time series dependence in the data, but also for some forms of cross-sectional dependence. Put differently, we will allow the time series $\mathcal{P}_i$ to be dependent across $i$. To derive our theoretical results, we will assume that the time series length $T$ tends to infinity. The number of time series $n$, in contrast, may either be bounded or diverging. 
\vspace{15pt}


\noindent \textbf{(a) Contributions to statistical multiscale testing} 
\vspace{10pt} 


\noindent The first main contribution of the project is to develop a novel multiscale test for the comparison of the trend curves $m_i$ ($1 \le i \le n$). More specifically, we aim to develop multiscale tests forr the hypothesis $H_0: m_1 = \ldots = m_n$ and for the related hypotheses discussed in part (b) of Section ??. To keep the exposition focused, we restrict attention to $H_0$ in what follows. For any interval $[u-h,u+h] \subseteq [0,1]$, consider the hypothesis
\[ H_0^{[i,j]}(u,h): m_i(w) = m_j(w) \text{for all } w \in [u-h,u+h]. \] 
Obviously, the hypothesis $H_0$ can be reformulated as
\begin{align*}
H_0: \ & \text{The hypothesis } H_0^{[i,j]}(u,h) \text{ holds true for all intervals } [u-h,u+h] \subseteq [0,1] \\ & \text{ and for all } 1 \le i < j \le n. 
\end{align*} 
We construct a multiscale method to simultaneously test the hypothesis $H_0^{[i,j]}(u,h)$ for all possible points $(u,h)$ and all pairs $(i,j)$ with $i < j$.\footnote{Obviously, in practice, we can not consider all points $u \in [0,1]$ and all $h > 0$ but have to restrict attention to a finite subset of points. We ignore this in our presentation for simplicity.} We plan to construct such a method along the following lines: 
\begin{itemize}[leftmargin=1.5cm]
\item[Step 1:] Construct nonparametric estimators $\hat{m}_{i,h}$ of the trend functions $m_i$, where $h$ denotes the bandwidth parameter.   
\item[Step 2:] Construct a test statistic $\hat{S}_{ij}(u,h)$ for the hypothesis $H_0^{[i,j]}(u,h)$. A simple choice is a statistic of the form $\hat{S}_{ij}(u,h) = \sqrt{Th} (\hat{m}_{i,h}(u) - \hat{m}_{j,h}(u)) / \hat{\nu}_{ij,h}(u)$, where $\hat{\nu}_{ij,h}(u)$ is chosen such that the asymptotic variance of the statistic is normalized to $1$. 
\item[Step 3:] Aggregate the statistics $\hat{S}_{ij}(u,h)$ into a multiscale statistic. As already discussed in Section ??, we will use the aggregation scheme of \cite{DuembgenSpokoiny2001} to do so. The resulting multiscale statistic has the form 
\[ \hat{\Psi}_{n,T} = \max_{1 \le i < j \le n} \sup_{u,h} \big\{ |\hat{S}_{ij}(u,h)| - \lambda(h) \big\},  \]
where $\lambda(h)$ are (appropriately chosen) additive corrections terms. As one can see, the multiscale statistic is not obtained by simply aggregating the individual statistics $\hat{\Psi}_{n,T}$. We rather take the supremum of the additively corrected statistics $|\hat{S}_{ij}(u,h)| - \lambda(h)$. This idea of this correction scheme goes back to \cite{DuembgenSpokoiny2001}. 
 \end{itemize}
Given the statistic $\hat{\Psi}_{n,T}$ the multiscale test is carried out as follows: Suppose for a moment we could compute the $(1-\alpha)$-quantile $q_{n,T}^*(\alpha)$ of the statistic $\hat{\Psi}_{n,T}$ under the null $H_0$. Then we proceed as follows: 
\[ \text{Reject the overall null hypothesis } H_0 \text{ if } \hat{\Psi}_{n,T} > q_{n,T}^*(\alpha). \]
By construction, the decision rule in ?? is a rigorous level $\alpha$ test. However, the quantile $q_{n,T}^*(\alpha)$ is gihgly complicated and not know in practicve. The main theoretical challenge is to come up with an (asymptotic) apprioximation $q_{n,T}(\alpha)$ of this quantule which is computable in practice such that 
\[ \mathbb{P} (\hat{\Psi}_{n,T} > q_{n,T}(\alpha)) = 1-\alpha + o(1). \]
As already mention in Section ??, we try to build on the methods dervie in ?? to derive such an approximation. However, the panel setting differs in various respect from the univeratie time series setting in ??, which is why the technical details ??. / substantial amont of work to extend / adapt the techniques to the setting at hand. 

Compared to existing methods, the multiscale test ptroposed above has the following important advantages: 
\begin{enumerate}[label=(\roman*),leftmargin=0.75cm]
\item Unlike many other methods (cp. ?? and the discussion in Section ??), it does not depend on a specific bandwidth parameter. It rather takes into account multiple scales or bandwidths $h$ simulteneously. 
\item it is much more informative than non-multiscale tests: the method can be reagrded as a simutelenous test of the family of hypothesis $H_0^{[i,j]}(u,h)$ for all points $(u,h)$ and all $i,j$. Looking at the test in this way, one may define the following decision rule: 

For each interval $[u-h,u+h]$, reject the hypopthesis $H_0^{[i,j]}(u,h)$ if the corrected test statistic $|\hat{S}_{ij}(u,h)| - \lambda(h)$ is above the critical value $q_{n,T}(\alpha)$, that is, if $|\hat{S}_{ij}(u,h)| - \lambda(h) > q_{n,T}(\alpha)$. 

We aim to prove the following result: With asymptotci probability $1-\alpha$, the hypothesis $H_0^{[i,j]}(u,h)$ is violated for all intervals $[u-h,u+h]$ and for all $(i,j)$ for which $|\hat{S}_{ij}(u,h)| - \lambda(h) > 0$. hence, we can make the following simulteneous confidence statement: With statistical conficende $1-\alpha$, there is a violaten of the hypothesis $H_0^{[i,j]}(u,h)$ for all intervals and all time series $(i,j)$ where the test finds something. Hence, the multiscale test does not only tell us whether $H_0$ is vioated, it also allows us to make rigorous statistical confidence statement about (i) which trends are different and (ii) in which time regions $[u-h,u+h]$ they differ. This is valuable information in many applications. 
\end{enumerate}
%. The only exception is ?? who have developed theory for the case $n=2$. However, the theory is developed under severe restrictions: ??.   
%We do not only aim to develop methodology but also derive a complete asymptotic theory for the proposed multiscale test. In particular, we will derive the limit distribution and analyse the behaviour under (local) alternatives. 
\vspace{5pt}


\noindent \textbf{(b) Contributions to curve clustering} 
\vspace{10pt} 


\noindent The second main contribution is to develop a clustering approach which is based on the multiscale test from the first main part of the project. 
%The only multiscale clustering method available in the literature is \cite{VogtLinton2018}. 
Our ojectives are as follows:
\begin{enumerate}[label=(\roman*),leftmargin=0.75cm]
\item We adapt the multiscale clustering methods from \cite{VogtLinton2018} to the setting at hand. We in particular use the multiscale statistics constructed in part (a) as distance / dissimilarityx measures in a hierarchical clustering algoruthm. 
\item  The main challenge is to derive theory for this clustering approach which goes beyond the basic asymptotic results of \cite{VogtLinton2018}. Using the theoretical results on the multiscale statistic $\hat{\Psi}_{n,T}$ enables us to do so and to derive much more precise theroetical statements about the clustering method. We give an example to make this claim more precise: The estimator of the unknown number of groups $K_0$ depends aon athreshold parameter. There is only a hezuristic rule for choosing this paramater. However, we can construct threshols with the help of the quantiles $q_{n,T}(\alpha)$. This should allow us to make confidence statements about the estimator of $K_0$. Denoting this estimator by $\hat{K}_0$, we conjecture that we are able to derive a result of the following form (under appropriate regularity conditions):
\[ \mathbb{P}( \hat{K}_0 = K_0 ) = (1-\alpha) + o(1). \] 
Hence, by picking the significance level equal to $\alpha$ and constructing the estimator $\hat{K}_0$ on the basis of $q_{n,T}(\alpha)$, we get that $\hat{K}_0$ is eqaul to the true number of groups $K_0$ with (asymptotic) probability $1-\alpha$. We can thus tune the clusteering procedure such that the erro probability of misestimating the number of groups $K_0$ is (asymptotically) controlled. 
%Since we have a very precise distribution theory for these (we can determine approximate quantiles $q_{n,T}(\alpha)$, we have much more information than in \cite{VogtLinton2018} and can thus derive much more precise theoretical statements about the multiscale clustering approach. In particular, we can use the quantile $q_{n,T}(\alpha)$ as a threoshold in the construction of an estimator of the number of clusters $K_0$. 
\end{enumerate}
\vspace{5pt}


\noindent \textbf{(c) Empirical applications} 
\vspace{10pt}


We intend to explore some empirical applications with the help of the new mutlscale testing and clustering methods. 

Model \eqref{model-objectives} and the proposed testing/clustering method are useful in a number of application contexts which we aim to explore. We here give some examples: 

%In the finance literature, VF’s common trend tests were used by Carrieri et al. (2004) to explain the dynamics in the gains from sectoral versus cross-country diversification of equity markets, and by Eun and Lee (2006) to study risk-return convergence of several developed stock markets. Other applications of VF’s tests to environmental data and geodetic data can be found in Fomby and Vogelsang (2003), Vogelsang and Franses (2005b) and Bacigál (2005).
\begin{example}
Short-term risk-free interest rates are one of the main topics of interest in the financial markets. For example, it is a key component of the capital asset pricing model, which describes the relationship between risk and return. Furthermore, the risk-free rate is also a required input in financial calculations regarding the pricing of bonds. There is an evergrowing amount of literature on the dynamics of interest rate. US Treasury bills are the real-world investment that serve as the proxy for these rates. \cite{Park2009} analyze the yields of the 3-month, 6-month, and 12-month Treasury bills in the context of comparing nonparametric curves. The authors assume that the yields come from the following model:
\begin{equation}\label{model-park}
Y_{it} = m_i(t) + \sigma_i \varepsilon_{it}, \quad i=1,\ldots, n, \quad t=1,\ldots,T,
\end{equation}
which is a simplification of our model \eqref{model-objectives}. \cite{Park2009} apply Si{Z}er method to the data and come to the conclusion that the underlying structure for different time periods is almost identical. They could not find any significant difference between any pair of the time periods, which concides with the results from applying other methods, see, for example, \cite{Fan2008}.
\end{example}

\begin{example}
Another example of comparison of time series with nonparametric trend functions described in \cite{Park2009} involves the long-term rates for US, Canada, and Japan from January 1980 to December 2000. The data is assumed to follow the same model \eqref{model-park}. The authors perform pairwise comparison of the curves as well as comparison of the three time series at the same time using the proposed Si{Z}er method. In both cases their method was able to detect significant differences nd indicate ``suspicious'' regions. However, since Si{Z}er is a graphical device that is mainly designed for data exploration rather than for rigorous statistical inference, they do not make simultaneous confidence statements with a predetermined confidence level about the regions where these differences were most probable to occur. Our proposed multiscale method, in contrast, is a rigorous level-$\alpha$-test of the hypothesis $H_0$ which is aimed specifically at that.
%the long-term interest rates for the US and Canada moved quite closely together from approximately 1993 to 1995, despite different business cycle positions at those times. We also can confirm in the SiZer map the events of the fall of the Canadian rates to just below the US rates for the first time in over a decade around 1996.

%we can see that in the period from 1982 to mid 1984 the US rates rose as the Japanese rates were falling, believed by the authors to be caused in part by the effects of US fiscal expansion in raising the demand for domestic savings relative to its supply indicating this early 1980s time period. we can see significant divergences in the interest rates in the late 1980s as US rates begin to fall back, rates in Canada and Japan are increasing. the larger values of Canada and Japan cause a significant negative difference. We can see this short-term similarity between Canada and Japan, however, the graph is clearly dominated by the more rapid descent of the Canadian rates through the overall decrease of both countries.

%It would also be interesting to compare three yields of three countries at the same time, as in Examples 1 and 2, respectively. To save space we only report the result of Example 2 for multiple comparison. We can see that in Fig. 9, there are differences that occur within each SiZer map, denoting that there are present. We have seen in Fig. 8 that there existed pairwise differences between all of the countries. The presence of these differences are also correctly detected when we compare each set of residuals from each country's individual estimated function to the residuals from the overall estimation.
\end{example}

\begin{example}
Economic growth has been a key topic in marcoeconomics over many decades. Economists are very much interested in the question whether gross domestic product (GDP) growth  has been faster in some countries than in others. One of the ways to model the source of economic growth is to incorporate a nonparametric deterministic time trend in the model. For example, \cite{Zhang2012} consider such a model for the OECD economic growth data. Specifically, they investigate the following model for growth rates:
\begin{equation}\label{model-zhang}
\Delta \ln GDP_{it} = \beta_1 \Delta \log L_{it} +\beta_2 \Delta \log K_{it} +\beta_3 \Delta \log H_{it} +f_i(t/T) +\alpha_i + \varepsilon_{it},
\end{equation}
where $i = 1,\ldots, n$, $t = 1, \ldots, T = 140$, $GDP$ is gross domestic product, $K$ is capital stock, $L$ is labour input, $H$ is human capital, $\alpha_i$ is a fixed effect, $f_i(\cdot)$ is an unknown smooth time trend furnction and $\varepsilon_{it}$ are idiosyncratic errors. The errors are allowed to be dependent cross-sectionally, but not serially over $t$. The data comes from $n = 16$ OECD countries.

\cite{Zhang2012} estimate the common component of time trends which appears to be significantly different from zero over a wide range its support. Moreover, they test the null hypothesis that there are no significant differences in the time trends for the 16 OECD countries. Based on the bootstrap $p$-values the authors are able to reject the null hypothesis of all the trends being equal at the 10\% confidence level. Hence, it can be interesting to be able to further cluster the OECD countries based on their economic growth rates.
\end{example}

\begin{example}
The issue of global warming has been a vital topic for many scientists over the last few decades. Since the late 1970, different models that describe the global temperature have been published. In the current literature it is common to assume that the temperate time series (global as well as local) follow a model that can be decomposed into a deterministic trend component and a noise component, see, for example, \cite{Ghil1991} and \cite{Mudelsee2018}. In order to estimate and attribute the trends in climate variables, a variety of econometric methods have been employed, starting from the simple linear (\cite{Yue2013}) and quadratic regression (??) to the empirical mode decompoistion (\cite{Wu2011}), spectrum analysis (\cite{Ghil1991}) and semi- and fully non-parametric methods (\cite{Gao2006}). Parametric and change points methods are mostly suited to quantify the magnitude of the warming trend or to determine the change points, whereas nonparametric methods are best designed to describe the trend over the full time interval without imposing any additional structure on it. However, most of these papers apply nonparametric methods to analyze only one time series or the authors assume that the trend function is common for different time series (\cite{Atak2011}). To our knowledge, only a few papers regarding the comparison of warming trends in different cities or countries have been published (\cite{Zhang2012}). 

%\cite{Atak2011} propose the following semiparametric panel model for unbalanced data to describe the trend in UK regional temperatures:
%\begin{equation}\label{model-atak}
%y_{it} = \alpha_i + \beta_i^\prime D_t + \gamma_i^\prime X_{it} + g(t/T) + \varepsilon_{it},
%\end{equation}
%where $y_{it}$ are the monthly mean temperature at a station $i, i =1, \ldots, n$ in month $t, t=t_i, \ldots, T$, $D_t$ is a vector of seasonal dummy variables, $X_{it}$ are a vector of observed covariates, $\alpha_i$ is a fixed effect for station $i$, $g(\cdot)$ is an unknown single common trend and $\varepsilon_{it}$ are idiosyncratic errors.

\cite{Zhang2012} propose the following semiparametric panel model for unbalanced data to describe the trend in UK regional temperatures:
\begin{equation}\label{model-atak}
y_{it} =\beta_i^{T}D_t + m_i(t/T) + \alpha_i + \varepsilon_{it},\quad i =1, \ldots, n, \quad t=1, \ldots, T
\end{equation}
where $y_{it}$ are the monthly mean maximum temperature, monthly mean minimum temperature or total rainfall in millimeters at a station $i$ in month $t$, $D_t$ is a $11$-dimensional vector of monthly dummy variables, $\alpha_i$ is the fixed effect for station $i$, $m_i(\cdot)$ is an unknown trend function and $\varepsilon_{it}$ are idiosyncratic errors. The dataset used is the balanced panel data set for $n=26$ stations in UK for $T=382$ months from October 1978 to July 2010. This model is a special case of our proposed model \eqref{model-objectives} with dummy variables as covariates.

\cite{Zhang2012} are interested in testing the null hypothesis $m_i = m$ for all $i =1, 2, \ldots, n$. In order to do this, they apply a non-parametric $R^2$-based test for common trends that was developed in their paper. Based on the obtained $p$-values, they reject the null hypothesis of common trend at $5\%$ level for the monthly mean maximum temperature and the monthly mean minimum temperature. However,  they do not reject the null hypothesis for the total rainfall eve at the significance level of $10\%$. As before, it would be interesting to further cluster the UK stations based on the common trend in order to be able to detect the causes of this warming trend. Moreover, it can also be of particular interest to see in which time regions the trends are significantly different from each other.

%Over the past decade, substantial efforts have gone into establishing reliable and accurate records of surface air temperatures for periods of a century or more. The Intergovernmental Panel on Climate Change (IPCC) Report 10 provides an excellent review of remaining problems. We have chosen for the present analysis the time series of annually-averaged temperatures from 1854 to 1988 produced by the Climate Research Unit (CRU) of the University of East Anglia, and verified the results against the IPCC consensus time series (1856-1989). Only the annual means for the Northern Hemisphere (NH), Southern Hemisphere (SH) and the entire globe were used here.
%An ever-growing body of evidence regarding observed changes in the climate system has been gathered over the last three decades, and large modeling efforts have been carried to explore how climate may evolve during the present century. The impacts from both observed weather and climate endured during the twentieth century and the magnitude of the potential future impacts of climate change have made this phenomenon of high interest for the policy-makers and the society at large. Two fundamental questions arise for understanding the nature of this problem and the appropriate strategies to address it: Is there a long-term warming signal in the observed climate, or is it the product of natural variability alone? If so, how much of this warming signal can be attributed to anthropogenic activities? As discussed in this review, these questions are intrinsically related to the study of the time-series properties of climate and radiative forcing variables and of the existence of common features such as secular co-movements. This paper presents a brief summary of how detection and attribution studies have evolved in the climate change literature and an overview of the time-series and econometric methods that have been applied for these purposes.
%Significant advances have been made in documenting how global and hemispheric temperatures have evolved and in learning about the causes of these changes. On the one hand, large efforts have been devoted to investigate the time series properties of temperature and radiative forcing variables \cite{Gay-Garcia2009}; \cite{Kaufmann2006}; \cite{Mills2013}; \cite{Tol1993}.
%In addition,, including features such as breaks and nonlinearities \cite{Estrada2013}; \cite{Gallagher2013}; \cite{Harvey2002}; \cite{Karl2000}; \cite{Pretis2015}; \cite{Reeves2007}; \cite{Seidel2004}; \cite{Stocker2013}; Tom´e and Miranda, 2004). Multivariate models of temperature and radiative forcing series provide strong evidence for a common secular trend between these variables, and help to evaluate the relative importance of its natural and anthropogenic drivers (Estrada, Perron and Mart´ınez-L´opez, 2013; Estrada, Perron, Gay-Garc´ıa and Mart´ınez-L´opez, 2013; Kaufmann et al., 2006; Tol and Vos, 1998). The methodological contributions of the econometrics literature to this field have been notable; e.g., Dickey and Fuller (1979), Engle and Granger (1987), Johansen (1991), Perron (1989, 1997), Bierens (2000), Ng and Perron (2001), Kim and Perron (2009), Perron and Yabu (2009), among many others, see Estrada and Perron (2014) for a review. Regardless of the differences in assumptions and methods (statisticalor physical), there is a general consensus about the existence of a common secular trend between temperatures and radiative forcing variables.
\end{example}


%The main purpose of the research project is to propose a new multiscale testing and inference approach for the model which consists of multiple time series with time series error structure and in the presence of generated regressors. 

%When several time series $\mathcal{Y}_i = \{ Y_{it}: 1 \le t \le T \}$ are observed for $1 \le i \le n$, we model each time series $\mathcal{Y}_i$ by the equation
%\begin{equation}\label{model2-intro}
%Y_{it} = m_i \Big( \frac{t}{T} \Big) + \alpha_i + \varepsilon_{it}
%\end{equation}
%for $1 \le t \le T$, where $m_i$ is a nonparametric time trend, $\alpha_i$ is a (random or deterministic) intercept and $\varepsilon_{it}$ are time series errors with $\ex[\varepsilon_{it}] = 0$ for all $t$.

%An important question in many applications is whether the time trends $m_i$ are the same for all $i$. When some of the trends are different, there may still be groups of time series with the same trend. In this case, it is often of interest to estimate the unknown groups from the data. In addition, when two trends $m_i$ and $m_j$ are not the same, it may also be relevant to know in which time regions they differ from each other. In Section \ref{sec-test-equality}, we construct statistical methods to approach these questions. In particular, we develop a test of the hypothesis that all time trends in model \eqref{model2-intro} are the same, that is, $m_1 = m_2 = \ldots = m_n$. Similar as before, our method does not only allow to test whether the null hypothesis is violated. It also allows to detect, with a given statistical confidence, which time trends are different and in which time regions they differ from each other. Based on our test method, we further construct an algorithm which clusters the observed time series into groups with the same trend. 


\subsection{Work programme incl. proposed research methods}


%All phases of the research will be conducted in close collaboration with the partners in Bonn.
The first part of the research period will be devoted to derive the multiscale test methods described in part (a) of Section ??. The second part will focus on the multiscale clustering methods described in part (b) of Section ??. 

\begin{center}
\begin{tabular}{r c c c}
{\bf Milestone} & {\bf 2019} & {\bf 2020} & {\bf 2021} \\
Multiscale testing & Oct--Dec & Jan--Dec & \\
Multiscale clustering & & & Jan--Oct
\end{tabular}
\end{center}


%\subsection{Data handling}

%[Text]

%\subsection{Other information}
%Please use this section for any additional information you feel is relevant which has not been provided elsewhere.

%[Text]

%\subsection{Descriptions of proposed investigations involving experiments on humans, human materials or animals as well as dual use research of concern}

%[Text]

%\subsection{Information on scientific and financial involvement of international cooperation partners}

%[Text]


\section{Bibliography}

\vspace{-1cm}

\renewcommand\refname{}
\bibliographystyle{ims}
{\small
\setlength{\bibsep}{0.55em}
\bibliography{bibliography}}


\section{Requested modules/funds}
Explain each item for each applicant (stating last name, first name).

\subsection{Basic Module}

\subsubsection{Funding for Staff}
\begin{center}
\begin{tabular}{ c c c c c}
Nr. &Position & 2019 &2020&2021\\
1 &Research staff U. Bonn (EGr. 13 TV-L 75 \%) &11.869 \texteuro &47.475 \texteuro &35.606 \texteuro \\
2 &Student Assistant Bonn &2.700 \texteuro& 10.800 \texteuro& 8.100\texteuro\\
&Required Amount & 14.569\texteuro & 58.275\texteuro& 43.706\texteuro
\end{tabular}
\end{center}

{\bf Job description of staff payed from auxiliary support for the funding period requested }

\begin{enumerate}
	\item Marina Khismatullina already possesses considerable experience in the study of nonparametric models with time series error. Moreover, she is a co-author of the paper ``Multiscale Inference and Long-Run Variance Estimtor in Nonparametric Regression with Time Series Friends'' by Khismatullina and Vogt, which is currently submitted to JRSSB. She will be capable to develop computational software taylored to assess the empirical performance of the proposed multiscale test.
	\item At the onset of the project a student assistent position should be available in order to
support stuff with exploratory data analysis, data mining and organisational issues. The
prerequisities are strong analytical and programming skills.
\end{enumerate}

\subsubsection{Direct Project Costs}

[Text]

\paragraph{Equipment up to Euro 10,000, Software and Consumables}

[Text]

\paragraph{Travel Expenses}

[Text]

\paragraph{Visiting Researchers (excluding Mercator Fellows)}

[Text]

\paragraph{Expenses for Laboratory Animals}

[Text]

\paragraph{Other Costs}

[Text]

\paragraph{Project-related publication expenses}

[Text]


%\subsubsection{Instrumentation}

%\paragraph{Equipment exceeding Euro 10,000}

%[Text]

%\paragraph{Major Instrumentation exceeding Euro 50,000}

%[Text]

%\subsection{Module Temporary Position for Principal Investigator}

%[Text]

%\subsection{Module Replacement Funding}

%[Text] 

%\subsection{Module Temporary Clinician Substitute}

%[Text]

%\subsection{Module Mercator Fellows}

%[Text]

%\subsection{Module Workshop Funding}

%[Text]

%\subsection{Module Public Relations Funding}

%[Text]



\section{Project requirements}

\subsection{Employment status information}
 For each applicant, state the last name, first name, and employment status (including duration of contract and funding body, if on a fixed-term contract).

[Text]

\subsection{First-time proposal data}
Only if applicable: Last name, first name of first-time applicant

[Text]
\subsection{Composition of the project group}
List only those individuals who will work on the project but will not be paid out of the project funds. State each person’s name, academic title, employment status, and type of funding.

[Text]

\subsection{Cooperation with other researchers}

\subsubsection{Researchers with whom you have agreed to cooperate on this project}

[Text]

\subsubsection{Researchers with whom you have collaborated scientifically within the past three years}

[Text]

\subsection{Scientific equipment}
The University of Bonn has a sufficient infrastructure in hard- and software. Personal computers are available and can be used within the project. Equipment like printer, fax and copier can be used as well.

%\subsection{Project-relevant cooperation with commercial enterprises}
%If applicable, please note the EU guidelines on state aid or contact your research institution in this regard.

%[Text]

%\subsection{Project-relevant participation in commercial enterprises}
%Information on connections between the project and the production branch of the enterprise

%[Text]



\section{Additional information}
If applicable, please list proposals requesting major instrumentation and/or those previously submitted to a third party here.

[Text]



\end{document}







\newpage

\begin{itemize}[label=--,leftmargin=0.5cm]
\item \cite{Stock1988} is one of the first papers to compare trend curves in a multiple time series. However, the focus of the author's attention is rather on stochastic trends than deterministic ones. The authors develop two tests for detecting common stochastic trends in a number of time series. They apply these tests to the economic time series, in particularly, the postwar U.S. data on the federal funds rate and the three- and twelve-month treasury bill rates. All three time series on inerest rates appear to share a common stochastic trend.

%Both tests involve the roots of the ordinary least squares coefficient matrix obtained by regressing the series onto its first lag. 

\item \cite{Vogelsang2005} consider a simple linear model
\begin{equation}\label{model-vogelsang}
Y_{it} =\alpha_i + \beta_i t + \varepsilon_{it}, \quad i=1, \ldots, n, \quad t=1, \ldots, T,
\end{equation}
where $\beta_i t$ is a linear trend function, $\alpha_i$ are so-called fixed effect error terms and $\varepsilon_{it}$ are standard regression errors such that a functional central limit theorem is applicable to $\{\varepsilon_{it}\}$. This model can be considered as a special case for our model \eqref{model} with nonparametric time curve $m_i(t)$ being linear in $t$. The authors propose two $F-$tests and a $t$-test to test the null hypothesis $H_0: R\beta = r$, where $R$ is $q \times n$ known deterministic matrix and $r$ is $q\times 1$ known deterministic vector. They derive an asymptotic theory for these tests and provide relevant critical values. As an empirical application, the authors compare the postwar European time series of gross domestic product (GDP) to the time series of GDP in Italy. They reject the null hypothesis that the rates of growth between Italy and 6 other European countries in the years 1950 to 1992 are the same.

%A key issue is the estimation of the asymptotic covariance matrix, for which we aim to compare three different approaches, amongst which is the familiar heteroskedasticity autocorrelation consistent (HAC) estimator. The other two approaches are new andare basedon extensions of the approach proposedby Kiefer and Vogelsang (2002a). 
\item \cite*{Park2008}, \cite*{ParkHannigKang2009} and \cite*{Park2009} extend the well-known SiZer method for analyzing one time series, which was originally proposed by \cite{ChaudhuriMarron1999}, and advanced as a procedure to analyze time series \cite{Rondonotti2007}. In \cite*{Park2008}, only the model with independent idiosyncratic errors $\{\varepsilon_{it}\}$ is considered, whereas in \cite*{ParkHannigKang2009} and \cite*{Park2009} the errors are allowed to be dependent across $t$. The authors use a regression function estimation to fit a local linear function to obtain a kernel etimate. This kernel estimate depends on a location $x$ and a bandwidth $h$. The proposed SiZer method then uses the color map to display the significance of differences between two regression functions for a range of locations $x$ and locations $h$. This is a useful graphical tool that can indicate the regions where the differences between trend curves should be investigated further. However, this method has its limitations since the comparison should be done only pairwise

%for example seismic recordings of earthquakes and nuclear explosions, gait analysis, temperature precipitation patterns, brain potentials evoked by flashes of light, packet/byte counts in Internet traffic, and so on.
\item \cite*{DegrasWu2012} is a seminal paper on the parallelism between the deterministic trends in multiple time series. The model considered is 
\begin{equation}\label{model-degras}
Y_{it} = m_i \Big( \frac{t}{T} \Big) + \varepsilon_{it}
\end{equation}
for $1 \le t \le T$ and $1 \le n \le N$, where $m_i$ is a nonparametric trend curve and $\varepsilon_{it}$ are standard regression errors with zero mean for all $t$. Furthermore, $\varepsilon_{it}$ are allowed to be weakly dependent in the terms of \cite{Wu2005}. This dependence is non-stationary and it generalizes the stationary assumptions on the error process used previously in the literature. In our proposed project we also intend to follow this model (and this non-stationary error process), further including the fixed effect $\alpha_i$ and the covariates $\{X_{it}\}$.

The authors test the parrallel hypothesis, specifically, $H_0: m_i(u) = c_i +m(u)$ for all $i=1, \ldots, N$ and $u\in[0,1]$. In this setting $c_i$ are considered to be vertical shifts between the reference curve $m(u)$ and the $i$-specific curve $m_i(u)$. In order to do so, the authors first estimate the individual trends $m_i(u) - c_i$ and the common trend $m(u)$ by the means of local linear smoothing procedure and then they develop a test based on the $L_2$-distances between the estimators. They derive the asymptotic theory for the proposed test and decise a  clustering algorithm based on the test statistic. As an illustration of the use of the proposed method, the authors analyze download trends (up to a scale) in the time series that consist of cell phone download activity in different areas in the United States. This is an interesting question for developing region-specific advertising strategies for cell phone companies. 

%For example in longitudinal clinical studies, evaluators are interested in comparing response curves for treatment and control groups. In agriculture, it may be relevant to compare at different spatial locations the relationship between yield per plant and plant density. In biology, assessing parallelism between sets of dose-response data allows to determine if the biological response to two substances is similar or if two different biological environments give similar dose-response curves to the same substance. Another interesting application of comparing trends in cell phone activity pertains to the allocation of bandwidth in phone networks.
\item \cite{Sun2011} add covariates to a simple linear trend model regression.

%The paper develops a novel testing procedure for hypotheses on deterministic trends in a multivariate trend stationary model. The trends are estimated by the OLS estimator and the long run variance (LRV) matrix is estimated by a series type estimator with carefully selected basis functions. Regardless of whether the number of basis functions K is Öxed or grows with the sample size, the Wald statistic converges to a standard distribution. It is shown that critical values from the Öxed-K asymptotics are second order correct under the large-K asymptotics. A new practical approach is proposed to select K that addresses the central concern of hypothesis testing: the selected smoothing parameter is testing-optimal in that it minimizes the type II error while controlling for the type I error. The new test therefore combines the advantages of the nonstandard test and the standard Wald test while avoiding their main disadvantages (power loss and size distortion, respectively).

\item \cite{Xu2012}.

%The current paper addresses this issue by modeling the time series under investigation as a linear deterministic trend with innovations that follow a semi-parametric vector autoregressive (VAR)
%process with nonstationary volatility. The volatility specification
%is quite general allowing for a broad range of patterns of nonstationary behaviors in volatility such as jumps or trending variances mentioned above. We derive the limit distributions of the
%standard trend tests in VF under nonstationary volatility and show
%that they are generally non-pivotal, involving the unknown timevarying volatility function in the limit. This can be explained by
%either of the two facts. First, under nonstationary volatility the nonrobust standard errors incorrectly estimate the asymptotic variances of the trend coefficients. This is analogous to the failure of
%the traditional t-test in the presence of heteroskedasticity. Second,
%the maintained assumption in VF that a standard invariance principle holds for model innovations is violated under nonstationary
%volatility. A class of robust tests is then suggested. Although the
%robust tests proposed are asymptotically pivotal under a quite general type of nonstationary volatility, they may suffer from large size
%distortions in small samples. We then propose for practical use two
%types of two-step residual-based bootstrap procedures, i.i.d. bootstrap and wild bootstrap, that can be applied to the robust tests.
%Robust inference under nonstationary volatility has attracted
%substantial attention recently in the econometrics literature; see
%Phillips and Xu (2006), Cavaliere and Taylor (2007), Cavaliere et al.
%(2010), Beare (2008) and Xu and Phillips (2008). They mainly focus
%on the coefficients in the stable or unstable autoregressive models.
%The current work contributes to the literature in two aspects. First,
%it considers the effects of nonstationary volatility on the tests of
%multivariate trend coefficients. Second, to our best knowledge,
%it is the first paper to formally show the non-robustness of the
%heteroskedasticity and autocorrelation (HAC) ‘‘robust’’ tests with
%nonstandard fixed-bandwidth (fixed-b) limit distribution theory
%(Kiefer et al., 2000) to nonstationary variances.

\item \cite*{Zhang2012}.

%This paper develops a test for common trends in a semi-parametric panel data model of the
%form that can be correlated with Xit, and its are idiosyncratic errors. The trend functions fi(t/T )
%that appear in (1.1) provide for idiosyncratic trends for each individual i. For simplicity, we will
%assume that (i)  satisfies certain martingale difference conditions along the time dimension
%but may be correlated across individuals, and (ii)  are independent of {Xit}. Note that fi and
%i are not identified in (1.1) without further restrictions.
%Model (1.1) covers and extends some existing models. First, when fi 0 for all i, (1.1)
%becomes the traditional panel data model with fixed effects. Second, if n = 1, then model (1.1)
%reduces to the model discussed in Gao and Hawthorne (2006). Third, when fi = f for some
%unknown smooth function f and all i, (1.1) becomes the semi-parametric trending panel data
%model of CGL (2011).
%The main objective of this paper is to construct a non-parametric test for common trends.
%Under the null hypothesis of common trends: fi = f for all i in (1.1), we can pool the
%observations from both cross-section and time dimensions to estimate both the finite dimensional
%parameter () and the infinite dimensional parameter (f ) under the single identification
%restriction the estimate of uit based on the pooled regression. The residuals {uit} should not contain any
%useful trending information in the data. This motivates us to construct a residual-based test for the
%null hypothesis of common trends. To be concrete, we will propose a test for common trends by
%averaging the n measures of non-parametric goodness-of-fit (R2) from the non-parametric time
%series regressions of uit on the time trend for each cross-sectional unit i. Such non-parametric
%R2 should tend to zero under the null hypothesis of common trends and diverge from zero
%otherwise. We show that after being properly centred and scaled, the average non-parametric
%R2 is asymptotically normally distributed under the null hypothesis of common trends and a
%sequence of Pitman local alternatives. We also establish the consistency of the test and propose
%a bootstrap method to obtain the bootstrap P-values.1
%To proceed, it is worth mentioning that (1.1) complements the model of Atak et al. (2011)
%who allow for heterogeneous slopes but a single non-parametric common trend across crosssections. As mentioned in the concluding remarks, it is also possible to allow the slope
%coefficients in (1.1) to vary across individuals and consider a joint test for the homogeneity of
%the slope coefficients and trend components. But this is beyond the scope of this paper

\item \cite{Hidalgo2014}.

\end{itemize}


Most tests of the hypothesis $H_0: m_1 = \ldots = m_n$ existing in the literature proceed in two steps: They first estimate the curves of interest by nonparametric methods and then construct a distance measure between the estimated curves which serves as a test statistic. By construction, these tests depend on one or several smoothing parameters which are needed to estimate the curves $m_i$. However, there is no theory available for a proper choice of the bandwidth/smoothing parameter. In particular, the optimal (MSE minimizing) bandwidth used for curve fitting is usually not optimal for testing. A classical way to get a bandwidth-free test statistic is to use empirical process theory and partial sum processes [cp.\ \cite{Hidalgo2014}]. A more modern way which is related to these partial sum processes are so-called multiscale tests. The idea is as follows: ?? 


Multiscale tests for the comparison of nonparametric curves under general conditions are not available to the best of our knowledge. One aim of the project is to develop such a test for nonparametric regression curves. Multiscale tests do not only have the advantage of being bandwidth-free. They also are much more informative compared to other tests. They do not only allow to test whether the curves $m_i$ are all the same or not; they also allow to say, with a pre-specified statistical confidence, which curves are different and in which regions they differ.  
\vspace{15pt}


\newpage

%Testing the equality of nonparametric curves is a classical theme in econometrics and statistics. In the simplest setting, two data sets $\mathcal{D}_i = \{ (Y_{it},X_{it}): 1 \le t \le T \}$ for $i=1,2$ are observed, where $T$ denotes the sample size of each data set. The data are supposed to come from the model 
%\begin{equation}\label{model-1}
%Y_{it} = m_i(X_{it}) + \varepsilon_{it}, 
%\end{equation}
%where $m_i$ denotes the unknown (nonparametric) regression function for each data set and $\varepsilon_{it}$ is the error term. The aim is to construct a statistical test for the hypothesis $H_0: m_1 = m_2$ that the two regression curves are equal. A number of tests have been proposed for this problem. ??

%(1) A classical theme in econometrics and statistics is the comparison of nonparametric curves. In many applications, the curves of interest can be interpreted as (determinstic or stochastic) trends. Starting with Stock and Watson (1988), there has been a growing literature in econometrics on testing for common trends [good summary in the introduction of Zhang, Su, Phillips (2012)): 
%\begin{itemize}[label=--,leftmargin=0.5cm]
%\item Stock \& Watson (1988). Testing for common trends.
%\item Vogelsang \& Franses (2005). Testing for common deterministic trend slopes.
%\item Park, Vaughan, Hannig \& Kang (2009). SiZer analysis for the comparison of time series.
%\item Sun (2011). Robust trend inference with series variance estimator and testing-optimal smoothing parameter.
%\item Xu (2011). Robustifying multivariate trend tests to nonstationary volatility.
%\item Zhang, Su \& Phillips (2012). Testing for common trends in semi-parametric panel data models with fixed effects.
%\end{itemize}

\noindent When the number of curves is large, classical tests for the comparison of nonparametric curves are not fully appropriate as a statistical tool. The issue is the following: In most applications where the number of curves is large, one can expect that not all curves are exactly the same. Hence, a test of the null that all curves are the same is quite uninformative. Most frequently, the hypothesis will be rejected. A more interesting question is the following: Are there groups of curves that are the same? This question leads to the problem of curve clustering. Clustering of coefficient or functions in panel data models is a relative young emerging field in econometrics: 
\begin{itemize}[label=--,leftmargin=0.5cm]
\item \cite{Bonhomme2015}. Grouped patterns of heterogeneity in panel data.
\item \cite*{Su2016}. Identifying latent structures in panel data.
\item \cite{Su2018}. Identifying latent grouped patterns in panel data models with
interactive fixed effects.
\item \cite{Wang2018}. Homogeneity pursuit in panel data models: theory and application.
\end{itemize}
In the statistics literature, there is also a literature on curve clustering (functional and longitudinal data): 
\begin{itemize}[label=--,leftmargin=0.5cm]
\item \cite*{Abraham2003}. Unsupervised curve clustering using B-splines.
\item \cite{James2003}. Clustering for sparsely sampled functional data.
\item \cite*{Tarpey2003}. Clustering functional data.
\item \cite*{Ray2006}. Functional clustering by Bayesian wavelet methods.
\item \cite{Chiou2007}. Functional clustering and identifying substructures of longitudinal data.
\item \cite{DegrasWu2012}. Testing for parallelism among trends in multiple time series.
\item \cite{Zhang2013}.
\end{itemize}
Most of the clustering procedures in the literature depend on a number of smoothing parameters. Multiscale approaches do not. 




